,URL,Content,Polarity,Subjectivity
0,https://www.yahoo.com/entertainment/artificial-intelligence-replace-human-musicians-163411480.html,"The post Will Artificial Intelligence Replace Human Musicians? appeared first on Consequence of Sound. Two months before she dropped the news that she’s expecting her first child with Elon Musk, Grimes (aka Claire Boucher, who now legally goes by c, the symbol for the speed of light), lit up music Twitter. The one-time neuroscience major let loose with a provocative forecast on theoretical physicist Sean Carroll’s Mindscape podcast when she shared, “I think live music is going to be obsolete soon,” predicting we might be seeing the last generation of human artists and also inspiring dozens of variations on the same joke suggesting Grimes’ own imminent obsolescence. In Grimes’ defense, she was referring to Artificial General Intelligence (AGI), an AI that’s as powerful as human intelligence in every way and is expected to arrive any time between the next decade and never and is currently unmatched by any AI tool on the market or in the works. However, even looking into the music landscape’s hazy future, “obsolete” is a strong word.  A few musicians publicly responded, including Holly Herndon, who made her own AI songwriting tool for last May’s PROTO , which was released as part of her dissertation for Stanford’s Ph.D. program in Computer-Based Music Theory and Acoustics, plus a series of since-deleted tweets from Zola Jesus and another thread from Grimes’ ex-boyfriend, Majical Cloudz’s Devon Welsh. Grimes may be correct about AGI being technically better at making art and music, much like Google DeepMind’s AlphaGo executed creative, never-before-seen strategies to beat the world’s best Go player. But, that won’t stop people from making music about their lived experiences, a feat AGI won’t be capable of credibly undertaking on its own. Editors' Picks  Setting aside AGI and focusing on the current AI songwriting tools aimed at the music industry, to a large degree these tools open up new opportunities for musicians rather than threatening to replace them. One notable exception, a point Cherie Hu made in an article for SongTrust last year, is production music. These songs are intended to create a vibe or provide an emotional backdrop geared toward licensing for film, TV, and commercials. Creators behind this particular medium are primed to struggle under the weight of the new entrants targeting easily reproducible styles of mood-heavy music, like JukeDeck, a UK-based AI music startup that was recently acquired by TikTok and can “interpret video and automatically set music to it.” Emerging music technologies of the recent past have been unable to live up to the threat-level hype. Drum machines, samplers, sequencers, synthesizers — these have been field-levelers, not death knells, opening doors to allow more people to participate in writing, recording, and performing music rather than shutting creators out. Backing bands aren’t losing gigs to loop stations; instead, we get visionary creativity like tUnE-yArDs looping the hell out of “Bizness”, and bad singers aren’t displacing any talented vocalists in the wake of Cher pioneering Auto-Tune as the vehicle for her transcendently pitch-corrected cyborgian break-up anthem, “Believe”. AI tools are unlikely to be markedly different, at least that’s how it looks from this side of the singularity. Rather than replacing biological creators or pushing anyone out, AI might just expand the field for new entrants for the foreseeable future, creating opportunities for those for whom making music was to some degree inaccessible (for instance, hits can now be written and produced on iPhones), and in some cases, pushing artists and music into completely new creative territory. David Bowie paved the way for AI-powered songwriting in the early 1990s, teaming up with Apple for Verbasizer, a proto AI lyric assistant that used Bowie’s own lyrics in a kind of digital cut-out technique that randomized sentences, which he used on his 1995 album, Outside. Today, we have Lyric AI, “a collaboration between Reimagine.AI and Google Brain building an artificial intelligence assistant designed to help musicians create original lyrics.” Google’s DeepMind has an AI project that generates “speech which mimics any human voice” called WaveNet, so we’re approaching an AI that can actually handle vocal duties. Facebook’s Artificial Intelligence Research (FAIR) team is building their own AI system on top of WaveNet, attempting to develop a music translation AI system that can transform a hummed or whistled tune into a complete song. Google isn’t only building tools for lyrics and vocals; the tech behemoth also has a variety of AI songwriting tools under the umbrella of open-source research project Google Magenta, including NSynth, a neural synthesizer that uses a deep neural network to learn the characteristics of sounds and uses the original sounds’ acoustic qualities to synthesize new sounds. In fact, Grimes herself used NSynth on her forthcoming album Miss_Anthropocene. Another Google Magenta project is Piano Genie, an intelligent controller that maps 8-buttons of input to a full 88-key piano in real time, which The Flaming Lips tweaked to make Fruit Genie, with its physical interface consisting of fruit; yet another is MusicVAE, which blends musical loops and scores. YACHT’s August release, Chain Tripping, was composed using MusicVAE on every track. French songwriter Benoît Carré wrote “Daddy’s Car”, the first song written with the help of artificial intelligence using a songwriting tool made by Sony Flow Machines. The tool emulates a specific style of genre, in this case, mimicking The Beatles. Together, they created the first AI album, Hello World (2016), under the moniker SKYGGE (Danish for “shadow”), actually landing a track on Spotify’s New Music Friday playlist. Spotify later poached the head of Sony Flow Machines, François Pachet, for Spotify’s Creator Technology Research Lab in July of 2017, so not only can Spotify use AI for music discovery and distribution, but they can now also utilize AI to write their own songs (which might make record label execs unhappy but will probably be used to make the rest of us as happy as possible). Another album written in collaboration with AI is I Am AI, the 2018 debut album by former American Idol contestant Taryn Southern using a combination of Amper, which lets you create and rework stems, and IBM Watson Beat, which uses a combination of reinforcement learning and neural networks to create tracks; and Alex da Kid used IBM Watson Beat on “Not Easy”, which was a Top 40 hit in 2016. Both IBM Watson Beat and Google Magenta are open source on GitHub, making it easy for developers to access, and while Amper seems available to musicians, judging from its use on Southern’s I Am AI, it’s actually aimed at “enterprise teams” creating “stock music” en masse with sliding prices dependent on “team size and specific music needs.”  AI-powered songwriting assistant Amadeus Code is available as an iPhone app (free, $1.99 pay as you go, or $9.99/mo), while ambitious Aussie start-up Popgun (one-time presumptive slayers of the Top 40 hit, currently providing an app for “teenagers using AI tools to make music for one another”) is focused on creating a space for “pop stars on training wheels.” Popgun’s Splash is newly available as a free iPhone/Android app, just coming out of beta in mid-December. Also recently out of beta, where it was most popular with gamers creating diss tracks to send to their vanquished enemies over Discord, is an extremely simple AI “songwriting” tool, Boomy, available via a web-based interface (free or $8.99/mo versions available). Amazon Web Services (AWS) just announced their entrant into the field, AWS DeepComposer, last month at their AWS re:Invent conference in Vegas, which offers “the world’s first machine learning-enabled keyboard for developers.” Arca just announced her AI collaboration with Bronze AI (which Jai Paul also demoed), and AI composer tool JAM is a new entrant set for release in March of 2020. Tons of music-related start-ups are popping up globally (Music Ally published a report last November detailing some of the most interesting), including WaveAI, creators of Alysia, a “lyric assistant” and “melody partner” complete with AI-generated vocals; HumTap, which converts a hummed tune into full instrumentation that you can add beats to and set to a video; UK start-up Vochlea, which takes vocalizations (even beatboxing) and turns it into MIDI accessible in various digital audio workstations (DAW) that can be used to create a full song; and Greek creators of an AI “beat assistant” at Accusonus. Who knows what will be announced next, but for now, most musicians’ jobs don’t seem to be going anywhere. If we’re “nearing the end of human-only art,” as Grimes later clarified on Twitter, then it’s a period ripe for hybrid creativity. It’s already happening — the tools exist right now and enterprising musicians are currently using them in creative ways. This will only increase as more tools become available and more musicians experiment with them, ushering in a new phase of creative expression that incorporates an ever-evolving AI tool set that enables musicians to more fully express our humanity. Will Artificial Intelligence Replace Human Musicians? Matt Melis Popular Posts Subscribe to Consequence of Sound’s email digest and get the latest breaking news in music, film, and television, tour updates, access to exclusive giveaways, and more straight to your inbox. 明悟之士: Be wary of opinions about China from people speaking from their experience as an invited guest there.  These opinions tend to be biased by a sense of reciprocity--a feeling of wanting to return the favor to China, as by complimenting the country. Also, guests are likely to be shown only those aspects of the country that reflect well on its leaders. 3.8k",0.15893462598008054,0.48254472799927345
1,https://news.yahoo.com/music-numbers-robot-conducts-human-131757921.html,"By Tarek Fahmy SHARJAH, UAE (Reuters) - The conductor on the podium has no baton, no tailcoat and no musical score, but Android Alter 3 is kicking up a storm as it guides a symphony orchestra's players through their paces. The robot has a humanoid face, hands and lower arms, which gesticulate with what could pass for passion as it bounces up and down and rotates during the live performance of Keiichiro Shibuya's opera ""Scary Beauty"" in the Emirate of Sharjah. For Shibuya, a composer from Japan, the role of robots in our everyday lives may well be increasing, but it is up to us to decide how artificial intelligence might add to the human experience, and humans and androids create art together. ""This work is a metaphor of the relations between humans and technology. Sometimes the android will get crazy, human orchestras have to follow. But sometimes humans can cooperate very comfortably,"" he said. Shibuya wrote the music, but the android controls the tempo and volume of the live show, and even sings at times. ""The premise is that the android itself is moving according to its own will,"" said its technician Kotobuki Hikaru. The work's lyrics are based on literary texts from American ""Beat Generation"" writer William Burroughs and French author Michael Houellebecq. ""The robots and AI that exist now are not at all complete. The focus of my interest... is what happens when this incomplete technology comes together with art,"" said Shibuya. From those who witnessed it, the performance drew a mixed response. ""I think this is a very exciting idea...we came to see how it looks like and how much is ...possible,"" said Anna Kovacevic. A second audience member, who gave his name only as Billum, said after the show: ""You know, a human conductor is so much better."" Although he is interested in AI and anticipates big breakthroughs, he concluded on the project: ""the human touch is lost.""  (Reporting by; Writing by Alexandra Hudson and John Stonestreet)",0.03373737373737373,0.4322649572649572
2,https://www.engadget.com/2020-01-23-bjork-microsoft-ai-lobby-score-soundtrack.html,"I sat in the lobby of the Sister City hotel in downtown New York City, laying back on a comfy couch, breathing as deeply as I could. Bjork stayed here for a month last year. I thought that maybe if I breathed hard enough, I could absorb some of her essence. It's a surprisingly unassuming setting for Kórsafn, the ""generative audio experience"" that the Icelandic musician created in partnership with Microsoft and Sister City. There are no displays, no laser beams, no smoke machines. If you didn't know about the installation, you might miss it. At the right time of day, this lobby could be a quiet respite from New York's unceasing hustle. But the sound installation -- or in Sister City's words, its new ""lobby score"" -- takes it a step further and transports you (not literally) to a transcendent realm. The relative peace and stillness in the lobby was punctuated at random moments by haunting choral voices from overhead speakers. Every few seconds, heavenly high notes rang out, followed by deep, booming chants, altogether sounding like a horde of ghosts playing a taunting game of choral peekaboo. Of course, it's not supernatural. It's Microsoft's AI using feedback from a camera installed on the hotel's roof to determine which of Bjork's selected works to play. ""Kórsafn"" translates to ""choral archives,"" and fittingly it features snippets drawn from the musician's favorite arrangements of her career, performed by Iceland's Hamrahlid Choir. Even as disjointed chunks, the vocal clusters sounded otherworldly, lending an ethereal air to the space.  Microsoft used what it calls an automated, long-term AI-training algorithm to understand weather trends via the streaming camera and detect things like snow, rain, clouds, clear skies and even bird flocks. The AI is even smart enough to distinguish among types of clouds, like cumulus or cirrus, according to Microsoft Director of Strategic Partnerships Amy Sorokas. It decides which vocal clusters to play depending on what's overhead. When the sun is shining, for example, the AI plays higher-pitched clusters. Since I was there in the evening, I heard more low tones than someone there during an afternoon listening session would have. In the lobby, you can't tell or even imagine what might be crossing the rooftop camera's line of sight. All you get are the clusters of voices that ring out of the speakers overhead, and the effect is almost unsettling. It's not something you'd expect to hear in a hotel lobby, but this is by design. Those in the know, like the roomful of media on opening night, might find it fun to try to guess what triggered the sound. Hotel guests who haven't been briefed, however, might be confused. ""For me, the interesting thing is exciting this notion of curiosity,"" Sorokas said. ""Like, wondering what made that sound happen but not explaining it so totally that people feel like, 'I know exactly what's happening.'"" It is this sense of wonderment that she believes makes a difference. ""It kind of makes you relax a little bit if you let yourself, just let your mind wander.""  Ryan Bukstein agreed. He's the Vice President of Brand for Atelier Ace, which runs Sister City, and he believes that the score offers an unexpected environment that jolts visitors' awareness. ""Every time you walk into a space now, you hear playlists,"" he said. ""We wanted to score, because it really helps take people out of their normal of what they're used to hearing."" This isn't the first time a custom score has been designed for a hotel or a restaurant, of course. Composer Ryuichi Sakamoto has made original music for museums and buildings before, even using light and wind sensors to change the music during the day. The hospitality industry has also opted for unique soundtracks over generic playlists, though those are still composed of individual tracks. It's not even Sister City's first time doing something like this: Its inaugural score launched with its opening last year, featuring work from experimental electronic musician Julianna Barwick. Yet Kórsafn is still an unconventional experience that manages to be enjoyable. I only sat in the lobby for about half an hour, which is far longer than most guests might, and though some of the motifs eventually felt slightly repetitive, the voices of the choir left me feeling relaxed. Each time a different vocal cluster played, I wondered if a bird had flown by -- or, since this is rat-infested New York City after all, whether a rodent had skittered past the camera. Though that's a chilling thought, the otherworldly sounds helped make the city's grit feel far away. I could listen to this soundtrack all day, as it's exactly the unobtrusive type of background noise I need to work. It's mostly due to the style of music, but the randomness of the AI generation also keeps things from getting boring and becoming a lullaby. The lobby is open to the public, and you can hear the sounds from the adjacent hotel bar, so if you're a big Bjork fan looking for a nice space to work out of, this could be worth checking out. Kórsafn will run through the end of the year, so you have plenty of time to listen to what a teamup between AI and Bjork sounds like. If you're not likely to be in New York City before then, you can still check out the soundtrack and watch the rooftop camera's feed on Sister City's website.",0.09510339885339883,0.4841166491166491
3,https://www.yahoo.com/entertainment/sundance-2020-ultimate-party-guide-160747289.html,"Click here to read the full article.  Heading to Park City? From intimate dinners and cocktail parties to late night bashes (that end just in time to head to brunch), there’s plenty to keep this year’s film festival attendees out of the cold between screenings. More from Variety Here is Variety’s ultimate party guide for Sundance 2020:  “Summertime” Premiere Party Lyft Lounge, 8-11 p.m. “Bad Hair” Premiere Party Writer-director Justin Simien Chase Sapphire on Main, 11 p.m. – 2 a.m. TAO Park City & Chase Sound Check Concert Mark Ronson performs at the private event for Chase cardholders. TAO’s three-night pop-up will feature DJ sets by Brody Jenner & Devin Lucien, Vice and DJ NVM with cocktails from Casamigos The Yard, 10:30 p.m. A Conversation with Quibi’s Founder Jeffrey Katzenberg Chase Sapphire on Main, 9:30-10:15 a.m. The Jewish Film Institute’s Light Lunch and Not-So-Light Conversation “Some Kind of Heaven” director Lance Oppenheim Kimball Arts Center, 12-2 p.m. Sundance TV’s Women on the Front Lines: Changing the Game Panel Ekwa Msangi, Haifaa al-Mansour, Monica Levinson, Hanelle Culpepper and Jackie Cruz. IFC Films’ Arianna Bocco will moderate the panel, presented by AMC Networks, New York Women in Film & Television, Women in Film Los Angeles and ReFrame Sundance TV HQ, 1-2 p.m #MeToo Voter: Centering Survivors’ Political Power Ai-jen Poo, Tarana Burke, Fatima Goss Graves and Mónica Ramírez in conversation with Rosanna Arquette The Latinx House, 1-2 p.m. MACRO “Leaving the Door Open Behind You” Panel Lena Waithe, Sundial Brand’s CEO Cara Sabin MACRO Lodge, 1:30-2:30 p.m. HBO’s Après Ski Happy Hour Shangela Laquifa Wadley  306 Main St, 2-4 p.m. “The Fight” Premiere Party Executive producers Kerry Washington and Pilar Savone; ACLU lawyers Lee Gelernt, Chase Stangio, Dale Ho and Brigitte Amirie; directors Elyse Steinberg, Josh Kriegman and Eli Despres MACRO Lodge, 3-4:30 p.m. Sundance TV’s Growing the Story Panel Sundance TV HQ, 3-4 p.m. HBO & TBS’s Unfolding Narratives Panel Suzy Nakamura, Alexander Hodge, Geraldine Viswanathan, Karan Soni, Minji Chan, moderated by Kimmy Yam.  306 Main St, 4-6 p.m.  “Black Bear” Premiere Party Aubrey Plaza, Sarah Gadon and Christopher Abbot, presented by Oakhurst Entertainment and Tandem Pictures with Audible The Audible Speakeasy, 4-6 p.m. “Wake Up” Premiere Screening & Fireside Chat Olivia Wilde, Margaret Qualley Chefdance, 3:30 p.m. Sundance TV’s New York Women in Film & Television and IFC Films Cocktail Celebration Sundance TV HQ, 5-7 p.m. The Cut’s “How I Get It Done” Panel Sarah Aubrey, Amy Entelis, Dee Rees and Zazie Beetz, hosted by New York Magazine’s “The Cut” and Amazon Fire TV O.P. Rockwell Cocktail Lounge and Musical Hall, 5-7p.m. The Latinx House & Netflix Kick-Off Party The Latinx House, 5-8 p.m. “The Go-Go’s” Dinner Rand’s Luxury Escape at St. Regis Deer Valley, 5:30 p.m. Variety and AT&T’s Indie Impact Cocktail Party Celebrating Justin Simien Variety Studio Lounge, 6-7:15 p.m. HBO’s HERstory Dinner Issa Rae, Yvonne Orji, Robin Thede, Lena Waithe, music by Olivia Dope 306 Main St, 7:30-10 p.m. WarnerMedia & AT&T Sundance Film Festival Kickoff Party Performance by Matt Berninger from The National Lateral at WarnerMedia Lodge, 9 p.m. – 2 a.m. “The Night House” Premiere Party Rebecca Hall, Evan Jonigskeit, Stacy Martin and director David Bruckner Wellhaus Lounge at Old Town Cellars, 9-11 p.m. Midnight MACRO Party Hosted by MACRO’s Stacey Walker King & Charles D. King MACRO Lodge, 10 p.m. Masters in Craft: A Workshop on the Power of Visibility HBO’s POV workshop, including Andrij Parekh, Thembi Banks, Akilah Green, Syreeta Singleton, Prentice Penny, Mathan Erhardt 306 Main St., 10 a.m. – 12 p.m.  It Gets Better House Brunch It Gets Better House, 10 a.m. – 12 p.m. A Fireside Chat with America Ferrera Supported by Netflix’s upcoming series “Gentefied” The Latinx House, 11 a.m. – 12 p.m. “The Photograph” Fireside Chat Issa Rae and director Stella Meghie MACRO Lodge, 11 a.m. – 12 p.m. Inclusion in Film Festivals: A Call to Action to Change the Narrative Panel (Time’s Up and USC Annenberg Inclusion Initiative) Eva Longoria, Angela Robinson, the Sundance Institute’s Dilcia Barrera and USC Annenberg Inclusion Initiative’s Dr. Stacy Smith The Latinx House, 11 a.m. – 12 p.m.  Color Creative’s Cocktails and Conversation Issa Rae and Deniese Davis, featuring Casamigos cocktails UTA Sundance House, 12-3 p.m. “Snowpiercer” Panel and Reception Daveed Diggs, Lena Hall and showrunner Graeme Manson 306 Main St., 12:30-2:30 p.m.  Reproductive Justice Through Entertainment Sophia Bush, Mishel Prada, Planned Parenthood’s Rachel Moreno The Latinx House, 3-4 p.m.  “Miracle Workers: Dark Ages” Screening and Panel Steve Buscemi, Geraldine Viswanathan, Karan Soni and Simon Rich  Lateral at WarnerMedia Lodge, 3:30-4:30 p.m. “Is This Real? Sustaining Gender Equality in Entertainment” Panel Presented by Women in Film and The Atlantic UTA Sundance House, 4-6 p.m. New Audiences: The Power of Storytelling in Education Panel Presented by Strayer Studios and the Blackhouse Foundation Kimball Art Center, 4-6 p.m.  HFPA Panel: “Women Breaking Barriers Year 3: Where Are We Now?” Kerry Washington, Frankie Shaw, Julie Taymor, Lisa Jackson, Sundance Institute Executive Director Keri Putnam and HFPA’s Elisabeth Sereda Sundance TV HQ, 4:15-5:15 p.m. Audible Speakeasy VIP Party Audible Speakeasy, 5-7 p.m. “Insecure” Dinner Issa Rae, Yvonne Orji, Jay Ellis, Alexander Hodge, Prentice Penny, moderated by Elaine Welteroth 306 Main St., 6:30-9 p.m.  ChefDance Celebration of Remarkable Women in Food, Film, Business and Philanthropy Honoring Martha Stewart with 2020 ChefDance Legend Award Memorial Building, 7 p.m. The Creative Coalition Spotlight Initiative Awards Dinner Rachel Brosnahan, Jim Gaffigan, Julie Taymor, Tim Daly, Olivia Munn, Brad Paisley, Kimberly Williams-Paisley, Clark Gregg Kia Telluride Supper Suite at The Mustang, 7-9 p.m. Celebration of Music in Film Rufus Wainwright, Sharon Van Etten, Jorge Aragón Brito The Shop, 7-10 p.m. “On the Record” Documentary Party Rebecca Hall, Evan Jonigskeit, Stacy Martin and director David Bruckner Wellhaus Lounge at Old Town Cellars, 9-11 p.m. “Siempre, Luis” After Party Lin-Manuel Miranda, Luis Miranda, John James  The Latinx House, 9 p.m.  HBO x Blackhouse: A Lowkey Convo with Issa Rae and Prentice Penny Blackhouse, 7-9 p.m. “Possessor” Cocktail Party Sean Bean, Andrea Riseborough, Christopher Abbott, Jennifer Jason Leigh Kia Telluride Supper Suite at The Mustang, 7-9 p.m. UTA Sundance Party UTA Sundance House, 10 p.m. “Promising Young Woman” Premiere Party Carey Mulligan, Bo Burnham, Alison Brie, director/writer Emerald Fennell, live performance by Cyn and DJ set by Kito Lyft Lounge, 10 p.m. – 2 a.m. “Blast Beat” Premiere Party Moises Arias, Mateo Arias, Kali Uchis, Daniel Dae Kim, Wilmer Valderrama, music by DJ Mel DeBarge, with a performance by Teo Arias MACRO Lodge, 10 p.m. Bootsy Bellows Pop Up Hosted by David Arquette, music by Zack Bia. Lateral at WarnerMedia Lodge, 10 p.m – 3 a.m. “Save Yourselves” Premiere Party Wellhaus Lounge at Old Town Cellars, 11 p.m. – 1 a.m. Outfest Queer Brunch Special remarks by Alan Ball Kimball Terrace, 10 a.m. – 12:30 p.m. “Hair Love” Screening and Conversation Director Matthew A. Cherry, producer Karen Toliver, Issa Rae The Latinx House, 11 a.m. – 12 p.m. NALIP Brunch: Latinx in Action – Directors Upfront Panel  Angel Manuel Sto, Cristina Costantini, Michael Arcos, presented by WarnerMedia 150 Spur Bar & Grill, 11 a.m. – 12 p.m. NALIP Signature Conversation with Mishel Prada Presented by Starz Spur Bar & Grill, 12-1 p.m. “Be Water” and “Lance” Premiere Party Presented by ESPN Films Tupelo, 1-4 p.m. HBO’s “Our Stories to Tell” Brunch 306 Main St., 1-4 p.m. “Music & Film: The Creative Process” BMI’s Composer/Director Roundtable  Kimball Arts Center, 3-5 p.m.  Sex, Politics, Film & Television Reception Planned Parenthood’s 9th annual reception, presented with Refinery 29 Old Town Cellars, 4-6 p.m.  “Damage Control” Interactive Screening and Q&A Presented by Variety and Eko, Q&A with Emily Pendergast, Rekha Shankar, Alon Benari, director Johnny Milord WarnerMedia Lounge, 4-6 p.m.  “I Carry You With Me” Cocktail Party Michelle Rodriguez, Armando Espitia, Christian Vasquez, Mirreyes Contra Godinez Kia Telluride Supper Suite at The Mustang, 4:30-6:30 p.m. “Omniboat: A Fast Boat Fantasia” Reception Starring Adam Devine, Robert Redford, Jessica Williams and Finn Wolfhard Rand’s Luxury Escape at St. Regis Deer Valley, 5:30 p.m. “Falling” Premiere Party Director-star Viggo Mortensen, music by DJ Ana Calderon Warner Media Lodge, 7-9 p.m. “The Glorias” Premiere Party Celebrating the Gloria Steinem film, starring Julianne Moore and Alicia Vikander. Sundance TV HQ, 5-7 p.m.  “Palm Springs” Premiere Party Andy Samberg, Cristin Milioti, presented by Limelight and Party Over Here with Audible Audible Speakeasy, 5:30-8:30 p.m. “The Climb” Cocktail Party Michael Angelo Covino, Gayle Rankin, Kyle Marvin Kia Telluride Supper Suite at The Mustang, 9-11 p.m. “Sylvie’s Love” Premiere Party Tessa Thompson, Nnamdi Asomugha and Eva Longoria Acura Festival Village, 4-6 p.m. The New York Times’ “Some Kind of Heaven” and “Time” Party The Shop, 4-6 p.m. Variety and Free the Work Panel Variety Studio Lounge, 5-6:30 p.m. “Tesla” Pre-Premiere Party Wellhaus Lounge at Old Town Cellars, 6-8 p.m. BET’s “Twenties” Screening and Party Lena Waithe, DJ D-Nice + DJ Domo Park City Live, 7-11:30 p.m. “The Last Shift” Cocktail Party Richard Jenkins, Ed O’Neill, Da’vine Joy Randolph Kia Telluride Supper Suite at The Mustang, 8-10 p.m. “The Evening Hour” Reception Starring Kerry Bishe’, Lili Taylor, Marc Menchaca, Stacy Martin, directed by Braden King Rand’s Luxury Escape at St. Regis Deer Valley, 5:30 p.m. “On the Record” Documentary Party Rebecca Hall, Evan Jonigskeit, Stacy Martin and director David Bruckner Wellhaus Lounge at Old Town Cellars, 9-11 p.m. The Art of Editing Reception Sponsored by Adobe The Shop, 1-3 p.m. GLAAD’s “Black, Queer & Unapologetic: The Shifting Lens of Storytelling in Hollywood” Panel Color of Change President Rashad Robinson, Jonica T. Gibbs, Justin Simien, Alexandra Grey, Marquise Vilson. Moderated by Danielle Moodie-Mills.   Filmmaker Lodge, 550 Main St., 4:30-5:30 p.m. BMI’s Snowball Lisa Loeb headlines the all-female lineup, including Chloé Caroline and Georgia Ku The Shop, 7:30 p.m.  Sundance Ignite Private Screening “Some Kind of Heaven” directed by Ignite Fellow Lance Oppenheim, presented by Adobe   Holiday Village Cinemas, 8:30-10 p.m. Best of Variety Sign up for Variety’s Newsletter. For the latest news, follow us on Facebook, Twitter, and Instagram. 明悟之士: Be wary of opinions about China from people speaking from their experience as an invited guest there.  These opinions tend to be biased by a sense of reciprocity--a feeling of wanting to return the favor to China, as by complimenting the country. Also, guests are likely to be shown only those aspects of the country that reflect well on its leaders. 3.8k",0.1441466054270932,0.4603962094815754
4,https://news.yahoo.com/beethoven-never-finished-last-symphony-170400811.html,"From Popular Mechanics Ludwig von Beethoven’s unfinished last symphony is being completed by artificial intelligence, Yahoo! reports. Beethoven died at age 56 in 1827. The work on his 10th and final symphony is part of a yearlong celebration of the composer’s 250th birthday on December 17, 2020. The decision to use AI is polarizing in the classical musical community, but Beethoven’s work in particular has been the subject of a lot of technological attention for decades. In 1824, Beethoven finished his legendary ninth symphony in D minor, the one that includes “Ode to Joy.” After that, he began work on a 10th symphony that wasn't just never completed—it’s so incomplete that scholars aren’t certain when partial drafts or sketches of musical ideas are even related to it. And efforts to complete it have also gone on for decades, causing uncomfortable arguments between prominent scholars. They compare it to Gustav Mahler’s similar unfinished 10th symphony, which has a fully orchestrated first movement and a complete entire draft in a simplified score form. Mahler’s work was completed by a musical scholar named Deryck Cooke, who studied Mahler all his career and felt able to extrapolate, with caveats, based on the extensive outline Mahler left. Far from composing or even orchestrating, he felt he was just building to a blueprint. “‘[O]rchestrate’ is not quite the right word here; Mahler conceived this music orchestrally, and his short scores are blueprints for instrumentation,” Cooke wrote. When this work is performed, Cooke’s involvement is prominently noted both as a credit and an explanation. In contrast, Beethoven’s final work is, in the literal and figurative senses, much more sketchy. Many scholars argue there just isn’t enough of Beethoven’s final symphony to complete it in a responsible way, with surviving sketches of only the first movement and no example of a fully orchestrated version. Cooke felt able to do justice to Mahler’s final work because he had examples of what he believed were Mahler’s whole vision for three of the five sections. For Beethoven’s 10th, there’s no such example. Germany’s national 250th birthday celebration for Beethoven has drawn fresh attention to the unfinished symphony, and the team’s project to “complete” the symphony is more of an experiment in AI composition with a Beethoven focus. Like the machine learning that scholars are using to sniff out the real William Shakespeare, these scholars have given their algorithm all of Beethoven’s works and used that to build a profile. From this, they ask the algorithm to analyze what little Beethoven left of the 10th symphony and extrapolate from there. It’s more of an abstract science experiment than, say, a cohesive yet still hotly debated Tupac Shakur hologram. Even previous human scholarly efforts to complete a draft of only the first movement of the symphony have been classified strictly as experiments, compared with the performable and reasonably respected Mahler 10th symphony synthesized by Cooke. So far, Beethoven scholars aren’t impressed. Barry Cooper, a world-renowned Beethoven expert who has sketched a complete first movement in the past, told Yahoo!, ""I listened to a short excerpt that has been created. It did not sound remotely like a convincing reconstruction of what Beethoven intended.” You Might Also Like",0.13914141414141415,0.5090728715728716
5,https://news.google.com/articles/CBMifWh0dHBzOi8vZ2VuZGVyLnN0YW5mb3JkLmVkdS9uZXdzLXB1YmxpY2F0aW9ucy9nZW5kZXItbmV3cy9jb21wb3Nlci1leHBsb3Jlcy1sZWdhY3ktY29tcHV0ZXItcGlvbmVlci1hZGEtbG92ZWxhY2UtYW5kLXVzaW5nLWFp0gEA?hl=en-US&gl=US&ceid=US%3Aen,"Alessandrini An exploration of artificial intelligence and musical composition may seem like a modern question, but Dr. Patricia Alessandrini found the beginnings of the idea in the 19th century. “Ada Lovelace is credited with the first published imaginings of AI-assisted composition,” Alessandrini said. She quoted Lovelace: “Numerous fundamental relations of music can be expressed by those of the abstract science of operations, such that a machine could compose elaborate and scientific pieces of music of any degree of complexity or extent.” At a Feb. 13 Clayman Institute Artist’s Salon, Alessandrini presented a project developed as a tribute to the 19th century mathematician. Called Ada’s Song: A Tribute to Ada Lovelace, the work was performed in November 2019 by the Britten Sinfonia, which included a small instrument ensemble, a soprano vocalist, and a device of Alessandrini’s design – the piano machine. At the salon, Alessandrini explained how her interest in human musical expressivity, interactivity and the innovations of Lovelace led to this exploration of computer-assisted composition and performance. Alessandrini is an assistant professor in the Department of Music as well as the Stanford Center for Computer Research in Music and Acoustics. She is a composer, sound artist and researcher on embodied interaction and immersive experience. Her work has been presented in festivals worldwide, and she also has toured extensively as a performer of live electronics. She previously has taught at the University of Bangalore and Goldsmith’s, University of London; here at Stanford she teaches composition, sonic arts and computer music. The Clayman Institute’s annual Artist’s Salon, under the leadership of Artist-in-Residence Valerie Miner, shows how the arts contribute to the larger mission of gender equality and research. Miner invites a diverse group of artists from the Stanford community to appear in the Artist’s Salon series. Before playing a video excerpt of the November performance, Alessandrini explained the project’s development. “For this project, what was most relevant was the piano machine,” she said. “It’s an artificial way of causing the piano to sound through computing.” Created with a former colleague at Goldsmiths, the machine uses MIDI messaging, controlled by a computer or an electronic piano keyboard, to connect with microprocessors, which control a physical device. Each device includes a small vibrating motor – the same ones found in many cell phones – in a clear acrylic casing, connected to a piece inside a piano that touches the strings and causes them to sound. “This idea of physically making this connection between the symbolic and the sound really interested me,” she said.  ""... I always go back to this question of human expression. Where does that lie? Where is that exact aspect?"" Regarding the role of AI in Ada’s Song, Alessandrini said: “The basis is similarity matching.” She compared the process with a smart phone selfie app that matches a user’s photograph with an image from a large bank of stored images. “I can take that same principle and do that in music,” she says, when a computer tries to match a note or passage or sound to generate notes. “The trajectory of the piano machine in that piece is that maybe in the beginning you hear it’s rather elementary, and there’s not too much, and then… it’s learning a bit from the musicians, and then by the end” it was taking a more leading role in the performance. Not only the piano machine is assisted by the computer, but the other performers as well, who play or sing from a musical score generated in real time. The soprano wears an earpiece that provides her next notes. “Part of the material she sings is generated in real time, so she’s part of the machine in a way.” Regarding her inspiration, Ada Lovelace, Alessandrini noted that she “was a strong artificial intelligence skeptic.” Often referred to as the first computer programmer, for the algorithm she wrote to accompany the design of Charles Babbage for an analytical machine, Lovelace saw limits to the capabilities of such calculating devices. “She said, computers can’t create anything. For creation requires, minimally, originating something. But, computers originate nothing. They merely do that which we order them, via programs, to do.” Alessandrini said, “You can imagine with AI theory, these statements are fascinating to reflect upon, and timely right now.” Lovelace was cited extensively by Alan Turing, a computer scientist in the early 20th century. His Turing test of artificial intelligence – can computers have consciousness? can they fool humans into thinking they are other humans? – “is past now,” Alessandrini said, “too low a bar.” She said, “We can talk about a Lovelace Test, which some theorists put as, ‘Only if computers originate things, should they be believed to have minds.’”     Before presenting Ada’s Song, Alessandrini shared her work on other interactive projects. In one, she took a domestic scene – a woman ironing in a home – and made a few changes. “I was thinking about how especially when it was a very traditional image of the women at home, doing the housework – my thing was a push toward innovation in these devices, and at the same time, innovations in hi-fi.” She notes that marketing in the 1950s for stereo equipment often targeted women. “This was my idea of women as pioneers in ways that maybe we haven’t thought about.” She turned an iron into an instrument. Where steam usually comes out, a microphone was added. With the iron in a relaxed position, the person operating it – a soprano – could sing into its microphone, providing material for it to process. A vintage radio also was emptied out and replaced with a microprocessor.  In a video game project, Alessandrini networked players from different sites who were producing both electronics and a musical score in real time. She also entered – and won – a team hackathon to design prototypes for sex toys. Their challenge: “Getting out of the kind of stereotypes and very strict functions of sex toys, very heteronormative kinds of things.” The Love Pad allowed users to create an experience that could be pre-programmed via a sensuous interface, including her sound design, that could guide the experience of someone else remotely. She says, “I won a very expensive vibrator I now use for musical purposes.” In questions at the end of her presentation, Alessandrini returned to some of the ideas she wanted to examine with Ada’s Song. “The means of producing sound – people know this who are working in film – getting really expressive performance from virtual instruments is not so easy. That’s why I always go back to this question of human expression. Where does that lie? Where is that exact aspect?” Her work is not about replacing human performance, but better understanding how it differs from computer-generated music. “This question of expressivity to me has not been solved, and it’s still an area to work on. That’s a positive thing, because there is an individuality of human expression.” Photos by Cynthia Newberry, Clayman Institute   Subscribe Donate For general inquiries: gender-email@stanford.edu For press inquiries: clayman-press@stanford.edu © Stanford University, Stanford, California 94305.",0.07928640508427745,0.41407778391820954
6,https://news.google.com/articles/CAIiEERWkNGhGayNxMTQ2y_Gx2wqFQgEKg0IACoGCAowrqkBMKBFMMGBAg?hl=en-US&gl=US&ceid=US%3Aen,"Sign in to your Forbes account or register For instructions on how to disable your ad blocker, click here. If this is your first time registering, please check your inbox for more information about the benefits of your Forbes account and what you can do next! We know machines and artificial intelligence (AI) can be many things, but can they ever really be creative? When I interviewed Professor Marcus du Sautoy, the author of The Creativity Code, he shared that the role of AI is a “kind of catalyst to push our human creativity.” It’s the machine and human collaboration that produces exciting results—novel approaches and combinations that likely wouldn’t develop if either were working alone. Can Machines And Artificial Intelligence Be Creative? Instead of thinking about AI as replacing human creativity, it's beneficial to examine ways that AI can be used as a tool to augment human creativity. Here are several examples of how AI boosts the creativity of humans in art, music, dance, design, recipe building, and publishing. Art In the world of visual art, AI is making an impact in many ways. It can alter existing art such as the case when it made the Mona Lisa a living portrait a la Harry Potter, create likenesses that appear to be real humans that can be found on the website ThisPersonDoesNotExist.com and even create original works of art. When Christie’s auctioned off a piece of AI artwork titled the Portrait of Edmond de Belamy for $432,500, it became the first auction house to do so. The AI algorithm, a generative adversarial network (GAN) developed by a Paris-based collective, that created the art, was fed a data set of 15,000 portraits covering six centuries to inform its creativity.  Another development that blurs the boundaries of what it means to be an artist is Ai-Da, the world’s first robot artist, who recently held her first solo exhibition. She is equipped with facial recognition technology and a robotic arm system that’s powered by artificial intelligence. More eccentric art is also a capability of artificial intelligence. Algorithms can read recipes and create images of what the final dish will look like. Dreamscope by Google uses traditional images of people, places and things and runs them through a series of filters. The output is truly original, albeit sometimes the stuff of nightmares.  Music If AI can enhance creativity in visual art, can it do the same for musicians? David Cope has spent the last 30 years working on Experiments in Musical Intelligence or EMI. Cope is a traditional musician and composer but turned to computers to help get past composer’s block back in 1982. Since that time, his algorithms have produced numerous original compositions in a variety of genres as well as created Emily Howell, an AI that can compose music based on her own style rather than just replicate the styles of yesterday’s composers. In many cases, AI is a new collaborator for today’s popular musicians. Sony's Flow Machine and IBM's Watson are just two of the tools music producers, YouTubers, and other artists are relying on to churn out today's hits. Alex Da Kid, a Grammy-nominated producer, used IBM’s Watson to inform his creative process. The AI analyzed the ""emotional temperature"" of the time by scraping conversations, newspapers, and headlines over a five-year period. Then Alex used the analytics to determine the theme for his next single.  Another tool that embraces human and machine collaboration, AIVA bills itself as a “creative assistant for creative people” and uses AI and deep learning algorithms to help compose music. In addition to composing music, artificial intelligence is transforming the music industry in a variety of ways from distribution to audio mastering and even creating virtual pop stars. An auxuman singer called Yona, developed by Iranian electronica composer Ash Koosha, creates and performs music such as the song Oblivious through AI algorithms. Dance and Choreography A powerful way dance choreographers have been able to break out of their regular patterns is to use artificial intelligence as a collaborator. Wayne McGregor, the award-winning British choreographer and director, is known for using technology in his work and is particularly fascinated by how AI could enhance what is done with the choreography in a project with Google Arts & Culture Lab. Hundreds of hours of video footage of dancers representing individual styles were fed into the algorithm. The AI then went to work and ""learned how to dance.” The goal is not to replace the choreographer but to efficiently iterate and develop different choreography options. AI Augmented Design Another creative endeavor AI is proving to be adept at is commercial design. In a collaboration between French designer Philippe Starck, Kartell, and Autodesk, a 3D software company, the first “chair designed using artificial intelligence” and put into production was presented at Milan Design Week. The Chair Project is another collaboration that explores co-creativity between people and machines.   Recipes The creativity of AI is also transforming the kitchen not only by altering longstanding recipes but also creating entirely new food combinations in collaborations with some of the biggest names in the food industry. Our favorite libations might also get an AI makeover. You can now pre-order AI-developed whiskey. Brewmasters’ decisions are also being informed by artificial intelligence. MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) is making use of all those photos of the food that we post on social media. By using computer vision, these food photos are being analyzed to better understand people’s eating habits as well as to suggest recipes with the food that is pictured. Write Novels and Articles  Even though the amount of written material to inform artificial intelligence algorithms is voluminous, writing has been a challenging skill for AI to acquire. Although AI has been most successful in generating short-form formulaic content such as journalism ""who, what, where, and when stories,"" its skills continue to grow. AI has now written a novel, and although neural networks created what many might find a weird read, it was still able to do it. And, with the announcement a Japanese AI program’s short-form novel almost won a national literary prize, it’s easy to see how it won’t be long before AI can compete with humans to write compelling pieces of content. Kopan Page published Superhuman Innovation, a book not only about artificial intelligence but was co-written by AI. PoemPortraits is another example of AI and human collaboration where you can provide the algorithm with a single word that it will use to generate a short poem. As the world of AI and human creativity continue to expand, it’s time to stop worrying about if AI can be creative, but how the human and machine world can intersect for creative collaborations that have never been dreamt of before.    You can watch the full interview with Marcus du Sautoy here: Bernard Marr is an internationally best-selling author, popular keynote speaker, futurist, and a strategic business & technology advisor to governments and companies. He… Bernard Marr is an internationally best-selling author, popular keynote speaker, futurist, and a strategic business & technology advisor to governments and companies. He helps organisations improve their business performance, use data more intelligently, and understand the implications of new technologies such as artificial intelligence, big data, blockchains, and the Internet of Things. Why don’t you connect with Bernard on Twitter (@bernardmarr), LinkedIn (https://uk.linkedin.com/in/bernardmarr) or instagram (bernard.marr)?",0.10646178534109571,0.5222913580672202
7,https://news.google.com/articles/CAIiEEkf1ph8FqqB-s-GMDAEm3AqFwgEKg8IACoHCAowlPHSATCqjTUwxOdy?hl=en-US&gl=US&ceid=US%3Aen,"Two months before she dropped the news that she’s expecting her first child with Elon Musk, Grimes (aka Claire Boucher, who now legally goes by c, the symbol for the speed of light), lit up music Twitter. The one-time neuroscience major let loose with a provocative forecast on theoretical physicist Sean Carroll’s Mindscape podcast when she shared, “I think live music is going to be obsolete soon,” predicting we might be seeing the last generation of human artists and also inspiring dozens of variations on the same joke suggesting Grimes’ own imminent obsolescence. In Grimes’ defense, she was referring to Artificial General Intelligence (AGI), an AI that’s as powerful as human intelligence in every way and is expected to arrive any time between the next decade and never and is currently unmatched by any AI tool on the market or in the works. However, even looking into the music landscape’s hazy future, “obsolete” is a strong word.   A few musicians publicly responded, including Holly Herndon, who made her own AI songwriting tool for last May’s PROTO , which was released as part of her dissertation for Stanford’s Ph.D. program in Computer-Based Music Theory and Acoustics, plus a series of since-deleted tweets from Zola Jesus and another thread from Grimes’ ex-boyfriend, Majical Cloudz’s Devon Welsh. Grimes may be correct about AGI being technically better at making art and music, much like Google DeepMind’s AlphaGo executed creative, never-before-seen strategies to beat the world’s best Go player. But, that won’t stop people from making music about their lived experiences, a feat AGI won’t be capable of credibly undertaking on its own. Setting aside AGI and focusing on the current AI songwriting tools aimed at the music industry, to a large degree these tools open up new opportunities for musicians rather than threatening to replace them. One notable exception, a point Cherie Hu made in an article for SongTrust last year, is production music. These songs are intended to create a vibe or provide an emotional backdrop geared toward licensing for film, TV, and commercials. Creators behind this particular medium are primed to struggle under the weight of the new entrants targeting easily reproducible styles of mood-heavy music, like JukeDeck, a UK-based AI music startup that was recently acquired by TikTok and can “interpret video and automatically set music to it.”  Emerging music technologies of the recent past have been unable to live up to the threat-level hype. Drum machines, samplers, sequencers, synthesizers — these have been field-levelers, not death knells, opening doors to allow more people to participate in writing, recording, and performing music rather than shutting creators out. Backing bands aren’t losing gigs to loop stations; instead, we get visionary creativity like tUnE-yArDs looping the hell out of “Bizness”, and bad singers aren’t displacing any talented vocalists in the wake of Cher pioneering Auto-Tune as the vehicle for her transcendently pitch-corrected cyborgian break-up anthem, “Believe”. AI tools are unlikely to be markedly different, at least that’s how it looks from this side of the singularity. Rather than replacing biological creators or pushing anyone out, AI might just expand the field for new entrants for the foreseeable future, creating opportunities for those for whom making music was to some degree inaccessible (for instance, hits can now be written and produced on iPhones), and in some cases, pushing artists and music into completely new creative territory. David Bowie paved the way for AI-powered songwriting in the early 1990s, teaming up with Apple for Verbasizer, a proto AI lyric assistant that used Bowie’s own lyrics in a kind of digital cut-out technique that randomized sentences, which he used on his 1995 album, Outside. Today, we have Lyric AI, “a collaboration between Reimagine.AI and Google Brain building an artificial intelligence assistant designed to help musicians create original lyrics.”  Google’s DeepMind has an AI project that generates “speech which mimics any human voice” called WaveNet, so we’re approaching an AI that can actually handle vocal duties. Facebook’s Artificial Intelligence Research (FAIR) team is building their own AI system on top of WaveNet, attempting to develop a music translation AI system that can transform a hummed or whistled tune into a complete song. Google isn’t only building tools for lyrics and vocals; the tech behemoth also has a variety of AI songwriting tools under the umbrella of open-source research project Google Magenta, including NSynth, a neural synthesizer that uses a deep neural network to learn the characteristics of sounds and uses the original sounds’ acoustic qualities to synthesize new sounds. In fact, Grimes herself used NSynth on her forthcoming album Miss_Anthropocene. Another Google Magenta project is Piano Genie, an intelligent controller that maps 8-buttons of input to a full 88-key piano in real time, which The Flaming Lips tweaked to make Fruit Genie, with its physical interface consisting of fruit; yet another is MusicVAE, which blends musical loops and scores. YACHT’s August release, Chain Tripping, was composed using MusicVAE on every track.  French songwriter Benoît Carré wrote “Daddy’s Car”, the first song written with the help of artificial intelligence using a songwriting tool made by Sony Flow Machines. The tool emulates a specific style of genre, in this case, mimicking The Beatles. Together, they created the first AI album, Hello World (2016), under the moniker SKYGGE (Danish for “shadow”), actually landing a track on Spotify’s New Music Friday playlist. Spotify later poached the head of Sony Flow Machines, François Pachet, for Spotify’s Creator Technology Research Lab in July of 2017, so not only can Spotify use AI for music discovery and distribution, but they can now also utilize AI to write their own songs (which might make record label execs unhappy but will probably be used to make the rest of us as happy as possible). Another album written in collaboration with AI is I Am AI, the 2018 debut album by former American Idol contestant Taryn Southern using a combination of Amper, which lets you create and rework stems, and IBM Watson Beat, which uses a combination of reinforcement learning and neural networks to create tracks; and Alex da Kid used IBM Watson Beat on “Not Easy”, which was a Top 40 hit in 2016.  Both IBM Watson Beat and Google Magenta are open source on GitHub, making it easy for developers to access, and while Amper seems available to musicians, judging from its use on Southern’s I Am AI, it’s actually aimed at “enterprise teams” creating “stock music” en masse with sliding prices dependent on “team size and specific music needs.”  AI-powered songwriting assistant Amadeus Code is available as an iPhone app (free, $1.99 pay as you go, or $9.99/mo), while ambitious Aussie start-up Popgun (one-time presumptive slayers of the Top 40 hit, currently providing an app for “teenagers using AI tools to make music for one another”) is focused on creating a space for “pop stars on training wheels.” Popgun’s Splash is newly available as a free iPhone/Android app, just coming out of beta in mid-December. Also recently out of beta, where it was most popular with gamers creating diss tracks to send to their vanquished enemies over Discord, is an extremely simple AI “songwriting” tool, Boomy, available via a web-based interface (free or $8.99/mo versions available).  Amazon Web Services (AWS) just announced their entrant into the field, AWS DeepComposer, last month at their AWS re:Invent conference in Vegas, which offers “the world’s first machine learning-enabled keyboard for developers.” Arca just announced her AI collaboration with Bronze AI (which Jai Paul also demoed), and AI composer tool JAM is a new entrant set for release in March of 2020. Tons of music-related start-ups are popping up globally (Music Ally published a report last November detailing some of the most interesting), including WaveAI, creators of Alysia, a “lyric assistant” and “melody partner” complete with AI-generated vocals; HumTap, which converts a hummed tune into full instrumentation that you can add beats to and set to a video; UK start-up Vochlea, which takes vocalizations (even beatboxing) and turns it into MIDI accessible in various digital audio workstations (DAW) that can be used to create a full song; and Greek creators of an AI “beat assistant” at Accusonus. Who knows what will be announced next, but for now, most musicians’ jobs don’t seem to be going anywhere. If we’re “nearing the end of human-only art,” as Grimes later clarified on Twitter, then it’s a period ripe for hybrid creativity. It’s already happening — the tools exist right now and enterprising musicians are currently using them in creative ways. This will only increase as more tools become available and more musicians experiment with them, ushering in a new phase of creative expression that incorporates an ever-evolving AI tool set that enables musicians to more fully express our humanity.",0.16605439262724034,0.4694473297122303
8,https://news.google.com/articles/CBMid2h0dHBzOi8vd3d3LmZhc3Rjb21wYW55LmNvbS85MDQ2NDA2Mi9tZWV0LXRyYXZpcy1ib3R0LXRoZS10cmF2aXMtc2NvdHQtdHdpbi13aG9zZS1tdXNpYy1hbmQtbHlyaWNzLXdlcmUtY3JlYXRlZC13aXRoLWFp0gEA?hl=en-US&gl=US&ceid=US%3Aen,"An award-winning team of journalists, designers, and videographers who tell brand stories through Fast Company's distinctive lens What’s next for hardware, software, and services Our annual guide to the businesses that matter the most Leaders who are shaping the future of business in creative ways New workplaces, new food sources, new medicine--even an entirely new economic system Celebrating the best ideas in business Who: Travis Bott What: Travis Scott’s AI twin Why we care: The robots are coming for your playlists again, and they’re mimicking your favorite artists in music videos too. Take Travis Bott. Yes, Travis Bott. He’s an AI musician inspired by Travis Scott, the rapper, producer, and songwriter responsible for “Sicko Mode” and more. Travis Bott has a new song⁠⁠ and accompanying video out that actually features Bott⁠⁠⁠—who is, not surprisingly, a Scott look-alike. The song is called “Jack Park Canny Dope Man,” and it’s eerily close in sound to the original source of inspiration. space150 is the tech-driven creative agency responsible for this project, and the AI they used to compose the song learned the lyrics and melodies of some of Scott’s compositions before creating its own take. “We were sort of fascinated with, like, ‘What if we tried to make a song⁠—like, an actual good song⁠—by using AI and basically creative-directing AI?’ And so we chose Travis Scott just because he is just such a unique artist and he has an aesthetic to it, both audibly and visually,” space150 executive creative director Ned Lampert told Adweek. The sound is on point, but the lyrics, much like the title, don’t make sense and could be a result of one of Travis Bott’s quirks: an obsession with sustenance. “The bot kept talking about food,” Lampert said. “There was one line, like, ‘I don’t want to f–k your party food,’ and we’re just like, ‘What?!&apos” The official video features Bott dancing around a Lamborghini with the rest of his AI crew.   As for Travis Scott, he need not worry about his career just yet. Among his many advantages, he’s a real, flesh-and-blood being who can compose memorable songs that actually make sense. Plus, science shows that too much AI can put people on edge. Entertainment Newsletter",0.2737662337662337,0.4927922077922077
9,https://news.google.com/articles/CBMizAFodHRwczovL21hcnRlY2hzZXJpZXMuY29tL3ByZWRpY3RpdmUtYWkvYWktcGxhdGZvcm1zLW1hY2hpbmUtbGVhcm5pbmcvYXVzdHJhbGlhbi1pbnN0aXR1dGUtbWFjaGluZS1sZWFybmluZy1zaWEtZnVybGVyLWluc3RpdHV0ZS11bml2ZXJzaXR5LWFkZWxhaWRlLXNoYXBlLWZ1dHVyZS1haS1hcnQtbXVybXVyLXdvcmxkcy1maXJzdC1haS1hcnQtZXhoaWJpdC_SAdABaHR0cHM6Ly9tYXJ0ZWNoc2VyaWVzLmNvbS9hbXAvcHJlZGljdGl2ZS1haS9haS1wbGF0Zm9ybXMtbWFjaGluZS1sZWFybmluZy9hdXN0cmFsaWFuLWluc3RpdHV0ZS1tYWNoaW5lLWxlYXJuaW5nLXNpYS1mdXJsZXItaW5zdGl0dXRlLXVuaXZlcnNpdHktYWRlbGFpZGUtc2hhcGUtZnV0dXJlLWFpLWFydC1tdXJtdXItd29ybGRzLWZpcnN0LWFpLWFydC1leGhpYml0Lw?hl=en-US&gl=US&ceid=US%3Aen,"


Publisher - Marketing Technology Insights

 Centerpiece of the new initiative includes the Art Intelligence Research Program, pairing world-class AI and machine learning engineers with leading artists, such as Laurie Anderson, who will be the first artist-in-residence in 2020 The Australian Institute for Machine Learning (AIML) and the Sia Furler Institute at the University of Adelaide announce new initiatives to gather the best artists and AI scientists in the world to change the future of art through AI. New initiatives include MURMUR, the world’s-first AI Art Exhibit Space, and Art Intelligence, an all-encompassing artistic research platform and gallery environment that pairs world-class AI and machine learning engineers with leading artists. Art Intelligence will support their collaboration in mediums from VR to robotics, from music to architecture, with a goal to produce groundbreaking explorations at the intersection of art and artificial intelligence (AI). Marketing Technology News: Zix Corporation Adds Marcy Campbell to Board of Directors For the first time ever, Art Intelligence will host an artist-in-residence program with American avant-garde artist, Laurie Anderson, as the first resident artist in 2020. Anderson is well-known as a performance artist, composer, musician, and film director whose work has spanned the media spectrum, most recently in virtual reality. Additionally, she was the first Artist-in-Residence at NASA in 2003, making her the perfect visionary artist to kick off this exciting new platform. “I’m really excited to be the first artist-in-residence within the Art Intelligence program,” said Laurie Anderson. “One of my favorite quotes about technology is from one of my meditation teachers who said, ‘If you think technology will solve your problems, you don’t understand technology – and you don’t understand your problems.’ When people say the purpose of art is to make the world a better place I always think: better for who? Art is not medicine or science. It’s not about creative problem solving. If I had to use one word to describe art, it would be freedom. I’m curious about whether this freedom can be translated or facilitated by AI in a meaningful way. I can’t wait to try.” Marketing Technology News: Freshworks Taps Oracle Cloud Vetaran Prakash Ramamurthy as New CPO Both branches of the project will be located at Lot Fourteen, a 7-hectare neighborhood in Adelaide, South Australia designed explicitly for the cross-pollination of ideas and research across innovation, arts and culture. Additional “ neighbors” at Lot Fourteen include the Australian Space Agency and the Aboriginal Art and Cultures Centre. “This project will help shape the future of AI and of art,” says Anton van den Hengel, the Director of AIML. “In the world, we know there’s a lot of art ‘about AI’ but very little that is AI.  We’re aiming to change that!” Also linked to these new initiatives are several international partnerships. These include a relationship with the Computer Music Center at Columbia University to leverage opportunities at the intersection of music and AI. “The CMC has been at the forefront of emerging creativity since the 1950’s” said Brad Garton, Director of the Computer Music Center. “The possibilities of this new relationship with the Art Intelligence program ensures that we stay in that leading position.” Informing these various partners and projects is an international working group of influential curators, creative thinkers and practitioners. Members of the working group are invited to explore diverse philosophical and ethical facets of AI, as well as to nominate artist-residents and help forge unexpected partnerships within contemporary art fields. The working group is convened by Slow Research Lab, a multidisciplinary research and curatorial platform based in the Netherlands. The platform’s director Carolyn F. Strauss explains: “It is imperative that we expand the development of AI to include a broader spectrum of identities and creative perspectives. We are honored to have this chance to help chart critical and truly pluralistic trajectories of artistic expression within a field that will so greatly impact our shared planetary future.” “The duty of the artist is to respond to and reflect upon the human experience,” says Tom Hajdu, Director of the Sia Furler Institute. “What’s at the edge of that experience is now clearly being challenged by AI and the machines that we are rapidly welcoming into our lives. At this crucial moment in human history, deep and sustained dialogue between artists and technologists feels essential.” Marketing Technology News: Kount Named to Fast Company’s Annual List of the World’s Most Innovative Companies MarTech Series (MTS) is a business publication dedicated to helping marketers get more from marketing technology through in-depth journalism, expert author blogs and research reports.  We publish high quality, relevant Marketing Technology Insights to help the business community advance martech knowledge and develop new martech skills. Our focus is on bringing marketers the latest business trends, products and practices affecting their marketing strategy.  We help our readers make sense of the rapidly evolving martech landscape, and cover the incredible impact of marketing technologies adoption on the way we do business.  Prev Post				 Bobcat Selects Commercial Web Services as Preferred Dealer Website Provider Next Post  LiveXLive Recognized As “Most Complete Streaming Music Service On The Market” By PC Mag  

				Coronavirus Robocalls Prey on Consumer Fears			

  

				ProVantageX and PremiumMedia360 to usher in a new era in Local TV Activation			

  

				LiveXLive Recognized As “Most Complete Streaming Music Service On The…			

  

				Asia-Pacific’s Leading Marketing Technology Platform Flaunter Set to Shake up…			

 Leave A Reply Cancel Reply Your email address will not be published.     Save my name, email, and website in this browser for the next time I comment.  

 

						 Popular Posts					
  
			Shoppers Demand New Standards to Combat Fake Reviews as…		
  
			Three Ways Brands Can Build an Engaged Social Audience in…		
  
			WorkWave Agency’s Lead Accelerator Program Empowers Small…		
  
			Aprimo and Templafy Form Strategic Partnership to Deliver…		
  
			R2integrated Elevates Digital Stack by Appointing Michael…		
  Copyright © 2020 MarTech Series. All Rights Reserved. Privacy Policy Welcome, Login to your account. Recover your password. A password will be e-mailed to you.",0.19513269412878792,0.4755473484848484
10,https://news.google.com/articles/CAIiEP-kklYmbXEtYKZtEy81o1AqGQgEKhAIACoHCAowy4j7CjCUjPMCMP_zxwU?hl=en-US&gl=US&ceid=US%3Aen,"
LIVE CURIOUSLY
 Japanese virtual singer Hatsune Miku at a concert in Paris.  Cutting-edge trends, rising stars and big ideas. 
								Read more							 Copy link to share with friends Because software programmers are replacing musicians. 
		By  Joshua Eferighe  
The Daily Dose

February 21, 2020

 The game console is singing the baby to sleep; in the other room, you’re listening as Alexa counts down the weekly top 10 — when it suddenly dawns on you that your favorite tunes are no longer made by musicians. Instead, hit-making algorithms are filling your playlist. Yep, your favorite artists are robots. This isn’t a scene from an episode of Black Mirror. Generative music is already a reality, and it’s enabling devices in ways that are changing how music is both consumed and composed. Hatsune Miku, a singing voice synthesizer, has released more than 100,000 released songs and performed in sold-out 3D concerts worldwide, has 900,000 fans on Facebook and corporate collaborations with Sega, Toyota USA, Google and more. Last year, Endel, a Berlin-based startup, partnered with Warner Music Group, which signed the startup’s algorithm to a major music label deal, the first of its kind. And just this month, three-time platinum recording artist Travis Scott, whose 2018 LP Astroworld gave him his first No. 1 debut on the Billboard 200 and a Grammy nomination, had his sound duplicated — the track is Jack Park Canny Dope Man — by an artificial intelligence machine trained on the rapper’s music.  Last August, Sony CSL Paris unveiled an AI tool that adds kick-drum beats to preexisting songs. Lil Miquela, a computer-generated 19-year-old Brazilian American model, musical artist and influencer created in 2016 that has more than 1 million Instagram followers, closed a $125 million investment round led by Spark Capital in January. And Mubert, founded in 2015 as the world’s first generative streaming service, already has 200,000 users. I think there will be a lot of implications. Alexander Lerch, Music Informatics Group  Alexander Lerch, head of the Music Informatics Group at the Center for Music Technology at Georgia Tech, believes this trend will only grow.  “From a technological point of view, we have more and more powerful tools that support composers or even try to replace composers,” Lerch says. “I think there will be a lot of implications, especially if you think about things like producers’ animated music. Those I would go as far as to say would get replaced by machines.”  Initially, generative music was thought of as a composition not specified by a score but by an algorithm. This concept was first applied back in the 1950s by gradually changing the simplest of music motifs over time. Composers like John Cage, Steve Reich and Terry Riley would take recordings with the same phrase and play it back at different speeds to yield different mathematical results, thus creating a very basic community-based generative system: absent of deliberate sound.  It was Brian Eno, with the release of his album Generative Music 1, in 1996,  who popularized the concept. Unlike his predecessors, Eno removed all human elements from the creative process, while still achieving a unique system-made sound. Now when people refer to generative music, they mean music that is ever-different and ever-changing, created by a system.  “He [Eno] certainly gets credit for coming up with the name and what I consider to be the rules of generative music in that it does not end,” says Alex Bainter, a part-time generative music programmer who runs the site Generative.fm.  Today’s advancements are Eno’s vision amplified: complex data-fed systems that can emulate and even fool listeners into thinking they’re hearing the spark of human creativity — something that has confounded computer experts for years. Technological advancements have made it possible, says Briana Brownell, a technology data scientist and CEO of Pure Strategy. “For a long time, a lot of these computer-generated music pieces and the algorithms needed a super powerful computer to really understand the science behind how it actually works,” she says. “But now there are so many tools out there that anyone can use them.”  Producer and musician Brian Eno in 1996. Source Martyn Goodacre/Getty Just as we witnessed the digitization of music take us from CDs to MP3s, a similar evolution is taking place in the streaming era. There are new opportunities for algorithms and computer-based programming to occupy the spaces artists once did. Algorithmic technology also allows streaming platforms to personalize music and playlists and even to predict what listeners want to hear.  Alfred Darlington, also known as Daedelus, an electronic production and engineering professor at Berklee College of Music in Boston, says that society’s willingness to give up more information is a big part of it as well.  “Ultimately, I think it comes down to a willingness by producers, composers, etc., to give up themselves more,” Darlington says. “We have a rich dataset out there that these AI systems can mine to find similarities, links, hidden agendas that we don’t perceive.”  It’s no wonder that Endel has already received funding from former Major Lazer member Christopher Leacock, Amazon’s Alexa Fund and Techstars Music, among others. If you’re able to discern what people like and play what they want to hear, why wouldn’t you?  When it comes to movie and television scores, gaming or licensing for a video online, generative music will continue to grow in sectors where a composer is not essential.  But the advent of computer-composed scores does not ring a death knell for musicians. Though artists are leaning more on previous hits to help predict future No. 1 hits, a generative composed song has never charted — yet.  “I’m less likely to believe that this type of technology will replace artists,” says Apostolos Zervos, CEO of Akazoo, a global, on-demand music streaming subscription company. “My position is that over time what this will probably do is enhance the tools that artists have to complement their artistic endeavor rather than [to] replace.”    As streaming continues to dominate the music industry’s revenue, with artists making less and subscriptions steadily climbing, there is no question that generative music and AI will increasingly play a bigger role in how artists approach their music in the future. That may not thrill everyone.  But, hey, if a robot can get a baby to sleep …   
DON'T SETTLE FOR BORING NEWS. SIGN UP FOR OUR NEWSLETTERS.
  Cutting-edge trends, rising stars and big ideas. Amazon, JPMorgan, Berkshire Hathaway, Walmart and others are promising an affordable health care option that's a third alternative to what Democratic moderates and progressives are offering.  Marijuana, it turns out, has a whole other use. A deep dive into the many facets, and faces, of homelessness in America. Financial firms are trying to get traders to work from home. But what if they need to compete for the same broadband network a Netflix?  Adolescent suicide has been increasing at a staggering rate, but it's hitting American Indian and Alaska Native teens much harder. These programs are offering culturally inspired ways to help. 50M People and growing © OZY 2020 - Terms & Conditions",0.09557116425928308,0.4106807109282357
11,https://news.google.com/articles/CBMiYmh0dHA6Ly93d3cubmV0aW1wZXJhdGl2ZS5jb20vMjAyMC8wMi93aHktdGhlLXJveWFsdHktZnJlZS1tdXNpYy1zcGFjZS1pcy10aGUtcGVyZmVjdC1uaWNoZS1mb3ItYWkv0gEA?hl=en-US&gl=US&ceid=US%3Aen,"Shutterstock is just the latest brand to jump into the royalty-free music world for brands and content creators – hot on the heels of YouTube and Apple Music. Taishi Fukuyama, Co-founder COO at Evoke Music, looks at the growing trend of big brands offering ‘free music for businesses’, and how machine-learning platforms are inventing a whole new category of music within a growing industry. A solid brand audio identity can be a powerful tool for any organisation, and music is the most powerful element of this. The importance of having a distinctive ‘sound’ is fast becoming as important as having a recognisable appearance, particularly considering the growing popularity of the “audio first” strategy. This is true for brands of every size, from the smallest YouTubers to the biggest multinationals. So it’s no surprise that the market providing music to business is booming, but largely to the detriment of the artists creating it. Content needs a soundtrack to truly bring it to life (for an excellent example of the power of the right soundtrack, I’d recommend this video here) but content creators across platforms like YouTube and TikTok don’t have enough time—or money—to license or commission backing soundtracks for every video they post. YouTube hasn’t succeeded in creating a fully ‘royalty-free’ musiclibrary, though they’ve tried. So creators and brands have largely resorted to using stock music. This is set to be a long term trend, with market researcher Technavio predicting a CAGR of 6% for the stock music market for 2019-2023, with several audio industry leaders moving into the space. Apple has partnered with the PlayNetwork to launch Apple Music for Business – a service geared towards providing a fully licensed and customisable soundtrack for businesses. Shutterstock has also launched a music subscription service aimed at YouTubers, social media managers, and podcast producers. To stock this growing number of libraries, a huge number of artists are using their time and talents to create music that is just good enough to provide an inoffensive backing track to accompany the vast amount of audio-visual content produced every day. This is a huge waste of creative talent, and to see so much of musician’s work be watered down to be synced to visual content is immensely disappointing. Although stock music can be high quality, and the next Yesterday or Sympathy For The Devil could conceivably begin as a backing track, it is extremely unlikely, and that is a shame. This is where AI generated music comes into its own. AI music will never achieve the majesty of Bohemian Rhapsody, but for the purposes of stock music it doesn’t have to, which makes it perfect for businesses looking for something specific but not necessarily revolutionary. This places the value of human compositions back on its story-telling ability, something which AI cannot do, rather than its ability to provide an adequate backdrop for someone else’s work. Rather than expending their energy and talent creating music for royalty-free music libraries, musicians can reclaim the freedom, compose, distribute, and perform in a way that is meaningful, and brands can rely on AI powered alternatives to create the soundtracks that they need at scale. This is not some far off prediction, Amazon’s AWS Deepcomposer already leverages AI to turn melodies into original songs, without forcing composers through the trials of understanding the AI and its underlying coding. As with human produced music though, any AIgenerated music’s success is still predicated on its ability to elicit an emotional response. While input parameters such as tempo, timbre, key and choice of instruments are all mechanically useful, the real potential of AI generated music will be unlocked when we can quickly and easily create compositions around specific feelings. AI’s role in the wider creative music industry should be seen as an invaluable tool rather than something that threatens to replace humans and their creativity. Like the best tools it can dramatically reduce the time and effort needed to achieve a specific goal, without making existing expertise and talent redundant. Make no mistake, no musician lives for the pleasure of creating non-descript stockmusic, and the time saved can be used on things that AI can never do, like telling stories that no-one else can, and creating the truemusical masterpieces of tomorrow. By Taishi Fukuyama Co-founder COO Evoke Music    Biggest new digital breakthrough of the decade?  View Results Copyright © 2020 Netimperative - latest digital marketing news. Magazine WordPress Theme by themehall.com",0.20171260928613877,0.5081740726593666
12,https://news.google.com/articles/CAIiELd-akoJdQYYa6Ipg_xatFYqGQgEKhAIACoHCAow4uzwCjCF3bsCMNGAygM?hl=en-US&gl=US&ceid=US%3Aen,"To continue, please click the box below to let us know you're not a robot. Please make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy. For inquiries related to this message please contact our support team and provide the reference ID below.",0.3333333333333333,0.5962962962962962
13,https://news.google.com/articles/CBMiV2h0dHBzOi8vbXVzdGFuZ25ld3MubmV0L21lZXQtdGhlLXN0dWRlbnRzLWNvLWNyZWF0aW5nLWFydC13aXRoLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlL9IBAA?hl=en-US&gl=US&ceid=US%3Aen,"Copyright 2019 Mustang Media Group 



    googletag.cmd.push(function() { googletag.display('div-gpt-ad-1578679267156-0'); });
  





    googletag.cmd.push(function() { googletag.display('div-gpt-ad-1582736597863-0'); });
  

 Computer science junior Kathir Gounder spends much of his time in the Engineering East building (Bldg. 20), completing schoolwork and pondering the technicalities of artificial intelligence. Gounder was enrolled in a graduate level course called Intelligent Agents (CSC 580). With his new skills, he creates art using machines called artificial neuron networks, which loosely resemble the computational powers of the human brain’s neurons. Gounder gave an example of seeing a cat on the street — the brain’s neurons process the visual image of the cat and give an instinctual understanding of what people are seeing. But, people cannot explain how the neurons achieved that recognition on the most basic microscopic level. This is where artificial intelligence comes into the picture. It possesses the capabilities to give art lovers more insight into how exactly brains abstract an image from visual stimuli. There is also the potential for artificial intelligence to produce original images of its own — some machines going so far as to mimic the styles of famous artists. Using artificial neuron networks to carry out tasks that would otherwise be too dangerous for humans to perform is a central goal of the artificial intelligence community, Gounder said. There is also an opportunity to create new art through artificial intelligence analyzing other artists’ styles and then using it to create new original images of its own. Gounder used artificial intelligence in Spring 2019 to generate original landscape pictures, a result of pitting two types of artificial neuron networks together — the generator and the classifier. The generator took in millions of inputs from data collected over time and made outputs that shared the same characteristics of that data. In Gounder’s case, this data was hundreds of landscape pictures found on the internet. Gounder’s generator made images meant to resemble landscapes, and the classifier inspected them to make sure they look like the real thing. “[The generator] takes in like a random probability vector and outputs an image, and its job is to generate, say, faces of cats that are so good that it tricks the [classifier] into thinking those are real images,” Gounder said. “You basically put these two networks into like a fight.” And just like human-made art, artificial intelligence art can take on several forms. Computer science graduate student Megan Washburn said she is interested in using artificial intelligence to generate music in video games, but even then, this  can also be used to come up with new melodies for composers who are stuck on a song. “A.I. in music can definitely boost creators’ work,” Washburn said. “For example, we can create an algorithm – like, say I wanted to stay in this key and in this time signature – we can create an algorithm to search that space and find something we might not have, as a composer, thought of previously.” Washburn said she likes to think of artificial intelligence in music as a co-writer, and indeed the same can be said of artificial intelligence in other situations where humans are standing right beside it, assisting with its functionality. Computer science professor Franz Kurfess said that while artificial intelligence machines are highly capable learners, their processes are still limited when compared to the learning capabilities of humans. “The basic principle here is that you give those neural networks a set of examples where you have inputs and the expected outputs,” Kurfess said. “Based on these examples, they learn how to behave in situations that are covered by the range of inputs that you give it.” Gounder initially became interested in creating art with artificial intelligence after reading several research papers on the subject. He saw his project as an opportunity to bridge the gap between STEM and liberal arts subjects by using code to create something artistic and, in doing so, step out of his comfort zone. After completing the class in Spring 2019, he said it has increased his confidence in his abilities and expanded his understanding of the broad applications of artificial intelligence. “It basically had a huge impact because now I feel a lot more confident in the sense that I can take on different subjects, and it obviously enhanced my STEM education,” he said. His favorite part of the class was taking his artwork to Laguna Lake Park and selling it at the Shabang music festival. His work, he said, inspired him to branch out into other areas such as languages. Now he works on teaching computers how to understand English. “There is no reason an art student can’t walk up the street to the computer science building and contribute something, in the same way a computer science student can go to the art or biology department,” Gounder said. Copyright 2020 Mustang Media Group. All rights reserved.  Copyright 2019 Mustang Media Group",0.014062437562437568,0.5853427498427499
14,https://news.google.com/articles/CBMigAFodHRwczovL3d3dy5laW5uZXdzLmNvbS9wcl9uZXdzLzUxMTc3MjQ2OC9rZW5hLWFpLWxhdW5jaGVzLWJlc3QtaW4tY2xhc3MtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtYmFzZWQtcGlhbm8tbXVzaWMtdHJhbnNjcmlwdGlvbtIBhAFodHRwczovL3d3dy5laW5uZXdzLmNvbS9hbXAvcHJfbmV3cy81MTE3NzI0Njgva2VuYS1haS1sYXVuY2hlcy1iZXN0LWluLWNsYXNzLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWJhc2VkLXBpYW5vLW11c2ljLXRyYW5zY3JpcHRpb24?hl=en-US&gl=US&ceid=US%3Aen," Preetham Vishwanatha
                    Kena.AI
                    +1 925-487-1773
email us here
                    Visit us on social media:
Facebook
Twitter
LinkedIn Kena Opus Demo  

                    EIN Presswire does not exercise editorial control over third-party content provided, uploaded, published, or distributed by users of EIN Presswire. We are a distributor, not a publisher, of 3rd party content. Such content may contain the views, opinions, statements, offers, and other material of the respective users, suppliers, participants, or authors.
                   
EIN Newsdesk
        &
        EIN Presswire
        (a press release distribution service)
       
Follow us on Facebook
        &
        Twitter
        and
        connect with us on LinkedIn
 
IPD Group, Inc., 1025 Connecticut Avenue NW, Suite 1000, Washington, DC 20036
        · Contact
        · About
 
        © 1995-2020 IPD Group, Inc., a publisher of EIN News ·
        All Rights Reserved
        ·
        Privacy Policy
        ·
        User Agreement
",0.0013888888888888885,0.17361111111111113
15,https://news.google.com/articles/CBMiZGh0dHBzOi8vd3d3Lm5iY25ld3MuY29tL21hY2gvc2NpZW5jZS9haS1jYW4tbm93LWNvbXBvc2UtcG9wLW11c2ljLWV2ZW4tc3ltcGhvbmllcy1oZXJlLXMtbmNuYTEwMTA5MzHSASxodHRwczovL3d3dy5uYmNuZXdzLmNvbS9tYWNoL2FtcC9uY25hMTAxMDkzMQ?hl=en-US&gl=US&ceid=US%3Aen," © 2020 NBCNEWS.COM For the past two centuries, composers, academics and fans of classical music have puzzled over Franz Schubert’s Symphony No. 8, which he abandoned in 1822 after writing the first two movements. Why did the Austrian composer walk away from his “unfinished symphony” six years before his death? Did he associate it with illness? Was he distracted by other work? And if he had completed the final two movements, what would they sound like? Now Lucas Cantor, a Los Angeles-based composer for film and television, has finished the work Schubert started — with help from artificial intelligence that runs on a smartphone. ""I jumped at the opportunity to use AI to complete the symphony; I think I responded yes before I even finished reading the email when Huawei reached out,"" Cantor told NBC News MACH. ""I've always been excited about the intersection of technology and music."" Cantor is not alone. Composers he has worked with are eager to use new technology as a tool for pushing their work into new areas. And it's not just in classical music. With several new musical programs emerging that encourage musicians to compose with AI, tech enthusiast and former American Idol contestant Taryn Southern released the first pop album to be co-written and co-produced with artificial intelligence. AI is essentially a pattern-recognition system. Feed it enough data, and it will find patterns within that information that it can use to make decisions. In the case of the Schubert symphony, the decisions were about which musical notes should be placed where. And there are other examples of AI's creativity, from other disciplines. Artists like the Munich-based Mario Klingemann have used AI to recognize patterns across thousands of 14th to 17th century European oil portraits to create an ever-changing single work, highlighting a “greatest hits” medley of images. (The work sold at auction in March for more than $50,000.) Similarly, software from Open AI, a San Francisco-based artificial intelligence company, can write a portion of a news story and then have AI write the rest. (The results are so convincing that Open AI, fearful of building a fake-news machine that any propaganda outlet or hostile nation could use to spread disinformation, released only a limited version of the software to the public.) This site is protected by recaptcha Privacy Policy | Terms of Service For Cantor and Huawei, the Chinese telecommunications giant that built the software used to complete the Schubert symphony, the question was which dataset to use to train the software. All of Western music? All classical music? In the end, Cantor and engineers from Huawei fed as much of Schubert’s catalog as they could find — roughly 2,000 pieces of piano music — into the software inside the company's new Mate 20 phone. The goal was to teach the AI to think like Schubert and to compose new passages, including what Cantor calls the “heart and soul of any piece of music”: the melody. “I was impressed that a computer could generate musical ideas, so everything that it sent me back I found to be kind of incredible,"" Cantor says. ""It’s like having a collaborator that has an endless stream of ideas, that never gets tired, that never runs out of ideas, that never has a bad attitude and never needs to take a break.” Once the AI suggested a series of new melodies, Cantor used his professional expertise to choose one. Then he elaborated on the software's notes, adding instruments and harmonies to flesh out the AI's contribution into a full movement. Economists, behavioral scientists and other experts have long argued that AI could one day automate entire industries, from delivery driving to dry cleaning. Critics of AI have concluded, however, that even the most sophisticated AI software can’t replace our most quintessentially human capacity: the ability to find inspiration in the world around us and use our innate creativity to make art. But even the most beautiful music is, ultimately, just code. Algorithms can be taught to write code, and when an algorithm has seen enough of the musical code it's being asked to emulate, it can compose music that sounds, well, human. Does this mean artificial intelligence can now take the place of a skilled composer? Cantor demurs, calling AI “just another tool in the toolbox” when he’s composing and saying he exploits all sorts of technology during the creative process. “The guitar is a piece of technology that you would collaborate with,"" he says. ""None of these things can make music on their own. When put in the hands of a capable human, they can really augment the human potential. And AI is not different.” But what happens when AI is used to create entire compositions? Southern, the Los Angeles-based pop artist, says the technology — which she says makes the process of composing more like shopping for a melody — is perfect for someone like her: a music lover who grew up writing code but who lacks formal training in either discipline. “Music was always a fun hobby for me, but I always felt like I didn’t actually have the skill set to make it as a musician,” Southern said. “But understanding the language of data and how music can translate into data made so much sense to me.” For her new album, ""I Am AI,"" Southern composed songs using the artificial intelligence platform Amper. But it's just one of several similar programs musicians can use to compose songs, including Google’s Magenta, Sony's Flow Machines and Jukedeck. Each of these programs makes it possible for a musician to pick the key of music, specific instruments, the beats per minute of the rhythm as well as the point where the music builds to a climax. Then the artificial intelligence offers up a range of options that the musician can choose from — picking one, combining several or altering their initial input for an alternative result. The creators of Amper, who imagined that it would be used to replace musicians typically hired to compose and record promotional music, fed it a variety of genres. Choose “documentary,” “futuristic” and “hopeful,” and up comes a spare, glitchy track with an upbeat bass line. Choose “trap bombastic,” and you can imagine the result rattling the windows of your car. The music created by Amper and similar programs is essentially indistinguishable from what you can hear on any radio station, and it raises difficult questions. For example, if we use pattern-recognition software to create only music that is similar to what we’ve enjoyed in the past, will we wind up trapped in a feedback loop? How will musicians find new inspiration if they are only sampling what’s already been done? Will music lovers of future generations wind up listening only to variations on compositions from the past? Southern says that, for better or for worse, the need to produce nearly endless amounts of new work will drive the market for music-composition AI software. “There is a tremendous amount of pressure for any content creator to be constantly producing faster, better and stronger,” she says. And if the result sounds as good as anything composed by a human, is that necessarily a bad thing? “These tools will actually just fortify creativity,” she says. “And ultimately, it’s not really about, ‘Is the computer being creative?’ It’s about, ‘Does the person who’s listening to the music or looking at the art think it’s being creative?’” SIGN UP FOR THE MACH NEWSLETTER AND FOLLOW NBC NEWS MACH ON TWITTER, FACEBOOK, AND INSTAGRAM. © 2020 NBC UNIVERSAL",0.06063908517952637,0.45724670550038204
16,https://news.google.com/articles/CBMiZ2h0dHBzOi8vbGVhcm5pbmdlbmdsaXNoLnZvYW5ld3MuY29tL2Evcm9ib3QtbGVhZHMtaHVtYW4tbXVzaWNpYW5zLWluLW9yY2hlc3RyYS1wZXJmb3JtYW5jZS81MjgxNTU0Lmh0bWzSAWlodHRwczovL2xlYXJuaW5nZW5nbGlzaC52b2FuZXdzLmNvbS9hbXAvcm9ib3QtbGVhZHMtaHVtYW4tbXVzaWNpYW5zLWluLW9yY2hlc3RyYS1wZXJmb3JtYW5jZS81MjgxNTU0Lmh0bWw?hl=en-US&gl=US&ceid=US%3Aen,"

See comments

 


Print

 No media source currently available A robot has led human musicians during a live performance in the United Arab Emirates. The robot, called Alter 3, has a human-like face and two long arms. Video from the recent performance in the Emirate of Sharjah showed the machine turning to face orchestra members and waving its arms. Alter 3 even sang at times. The performance was an opera called “Scary Beauty,” created by Japanese composer Keiichiro Shibuya. He told Reuters news agency that the robot acted as the conductor by setting the speed and sound level of the performance. Shibuya said the involvement of robots in the everyday lives of humans is continually increasing. But, he said he thinks people will need to decide in the future how artificial intelligence, AI, can best improve the human experience. Shibuya added that he believes humans and robots can learn to work together to create beautiful art. “This work is a metaphor of the relations between humans and technology,” he said. Shibuya noted that sometimes the music-leading robot can “get crazy,” making it difficult for the musicians to keep up. But other times, the humans and machines cooperate very well. Shibuya said the robots and AI that exist today are “far from complete.” He is interested in studying how such incomplete technology can be combined with art. From those who witnessed it, the performance drew mixed reactions. “I think this is a very exciting idea...we came to see how it looks like and how much is possible,” said Anna Kovacevic. Another attendee, who gave his name only as Billum, said after the show: “You know, a human conductor is so much better.” Although he said he is interested in AI and looks forward to big developments, he noted of the project: “the human touch is lost.” I’m Bryan Lynn. Reuters reported on this story. Bryan Lynn adapted the report for VOA Learning English, with additional information from Tokyo’s New National Theater. Mario Ritter, Jr. was the editor. We want to hear from you. Write to us in the Comments section, and visit our Facebook page. _____________________________________________________________   orchestra – n. a large group of musicians who play different instruments together composer – n. someone who writes music conductor – n. person who leads a group of musicians or singers artificial intelligence – n. the development of computer systems with the ability to perform work that normally requires human intelligence metaphor – n. a way of describing something by comparing it with something else that has some of the same qualities crazy – adj. extremely enthusiastic or excited   


Load more comments


 ",0.06458169737239504,0.5141422530957414
17,https://news.google.com/articles/CAIiEC1CMC1SspSShLWtJcKEu_UqFwgEKg4IACoGCAow3O8nMMqOBjDe2aYG?hl=en-US&gl=US&ceid=US%3Aen,"Filed under: Lady Gaga’s Poker Face in the style of Mozart? Sure, why not OpenAI’s MuseNet is a new online tool that uses AI to generate songs with as many as 10 different instruments. Not only that, but it can create music in as many as 15 different styles, imitating classical composers like Mozart, contemporary artists like Lady Gaga, or genres like bluegrass or even video game music. You can give it a short segment of music to get it started or have it start from scratch. MuseNet works by using a deep neural network that’s been trained on a dataset of MIDI files gathered from a range of online sources that cover jazz, pop, African, Indian, and Arabic styles of music. The researchers behind the project say that the system is able to pay attention to music over long periods of time, meaning it’s able to understand the broad context of a song’s melodies, rather than just how they flow together in a short section. With this data, the system is tasked with predicting the next note in a sequence.  When you try it out for yourself, you can hear this approach at work. Give MuseNet a short bit of music to get it started, and it will initially follow the style quite closely. However, as the music goes on, the AI’s predictions gradually veer more and more from the original. When I had it write the ending of the Harry Potter theme in the style of a video game soundtrack, it eventually descended into the stuff of nightmares.  The software is just the latest project from OpenAI, which recently made headlines for producing an AI that was capable of beating a world champion e-sports team at Dota 2. The same AI was then let loose on the Dota-playing public, with devastating results. OpenAI’s other projects have been able to write convincingly by ingesting huge numbers of articles, blogs, and websites.  OpenAI isn’t the first company to experiment with AI-generated music. Taryn Southern released an album composed entirely using AI back in 2017, and other musicians are increasingly experimenting with using artificial intelligence in their composing process. But this new wave of AI composing poses some complicated legal problems about who owns the rights to the music, and the problem is only going to get worse as tools like MuseNet lower the barrier to entry of AI composing.  MuseNet is available to try out now on OpenAI’s site, where you can also listen to a selection of songs the team has already generated and read about how the system works. TechCrunch reports that the generator will stay online until around mid-May, after which point, it will eventually be partially open-sourced.  A newsletter about computers",0.06299941724941724,0.46842754467754466
18,https://news.google.com/articles/CBMiZ2h0dHBzOi8vd3d3LmZhc3Rjb21wYW55LmNvbS85MDQzNzYyNS9hbWF6b25zLWtvb2t5LW5ldy1rZXlib2FyZC1sZXRzLWh1bWFucy1hbmQtYWktd3JpdGUtbXVzaWMtdG9nZXRoZXLSAQA?hl=en-US&gl=US&ceid=US%3Aen,"An award-winning team of journalists, designers, and videographers who tell brand stories through Fast Company's distinctive lens What’s next for hardware, software, and services Our annual guide to the businesses that matter the most Leaders who are shaping the future of business in creative ways New workplaces, new food sources, new medicine--even an entirely new economic system Celebrating the best ideas in business Amazon has come up with an entertaining but unlikely way to get developers excited about using its AI services: music. At its AWS re:Invent event in Las Vegas on Sunday night, the company unveiled a new music keyboard called DeepComposer, which developers can use to compose music in collaboration with Amazon generative AI models running in the cloud. The keyboard plugs into a PC, where the human can use a control panel to communicate and collaborate with the AI models. Amazon calls DeepComposer “the world’s first machine learning-enabled musical keyboard.” Generative AI creates art, text, music, or other content based on examples fed to it via machine-learning models. In this case, the AWS models can create a four-part accompaniment around a simple melody or chord progression a developer might play on the keyboard. The accompaniment can be in one of four genre styles–rock, pop, jazz, or classical. Mike Miller [Photo: Mark Sullivan]But the point is not making great music (it doesn’t exactly do that) but rather to get familiar with AI models that create the tunes. “This whole system, DeepComposer, is designed for giving developers a hands-on opportunity to learn about this new technology while at the same time having fun with music,” said AWS engineer Mike Miller, who moved from Amazon’s consumer-electronics design group at Lab126 to AWS in order to create the generative AI models for music creation. “It walks developers through the process of using generative AI by starting with just an understanding of how the creative process works, understanding how generative models are trained,” Miller says. “And then taking that knowledge and allowing them to apply it to other domains and train their own models.” The keyboard itself resembles the sort of cheap mini keyboards some people use to play virtual instruments within a digital audio workstation (DAW) app like GarageBand or Pro Tools. It’s plasticky and the keys are too small for any serious playing. But, again, it’s meant for developers, not musicians. How does the model know how to create the original music and conform it to a specific style? By ingesting lots and lots of training data. Miller and his team used thousands of public domain pieces of music from various genres as training data and fed it into the model via MIDI files containing eight-measure chunks of the music. The neural network then processed the data and learned from patterns in the various types of music. DeepComposer uses a generational adversarial neural network (GANN) to create original accompaniment based on the user’s input on the keyboard. Hardware-wise, DeepComposer looks like a pretty typical music keyboard. [Photo: courtesy of Amazon]Miller told me that the GANN actually contains two models. One acts as a “generator” that creates the musical notes and phrases. The other acts as a “discriminator” that continually gives feedback to the generator to push it toward creating music that’s more consistent with the genre. Miller said he thinks of the “generator” as an orchestra that makes sounds and of the “discriminator” as a conductor that’s constantly guiding and correcting the players. I visited Miller at Amazon’s offices in Palo Alto so that I could try out DeepComposer myself. When I sat down at the keyboard and gave the AI a melody to improvise around, I won’t say we made beautiful music together. But we did make interesting music, and it was fascinating to watch the AI making real musical and stylistic choices, almost in real time. A few examples: Jazz. After I fed a simple chord progression into the model—right now it’s only possible to create eight-measure improvisations—I asked it to create an accompaniment in the jazz genre. Even though the progression I’d played was uncomplicated, the resulting accompaniment I heard bore little relation to it. The chords I heard the piano playing were, well, bizarre. It sounded like someone pressing down as many keys as possible with both hands, then letting up, then repeating. Pop. Miller told me the models had advanced farther in learning to play in the pop and rock genres. I played a flowery pop melody on the keyboard. This time the accompaniments generated by the model were clearly related to what I had played on the keyboard. The drummer decided to dispense with the snare drum backbeat altogether and instead play fast sequences on a cymbal. The keyboard player hit a few clams. Rock. On one song the AI surprised me by playing a half-time rhythm behind the rapid succession of notes I’d played on the keyboard. I heard a guitar play a little improvised lick that sounded original. On the other hand, I heard that same AI guitar suddenly squawk out a couple of completely random and discordant notes. With more training, the discriminator in the model might recognize those notes as errors and correct the generator. After playing with DeepComposer for a half hour I could hear how the generator and the discriminator were working together. Miller played me the generator’s performance after progressively more rounds of feedback and correction, and I could hear the music improve, both musically and stylistically. I also got the sense that neural networks find their path to creating pleasing, genre-faithful music very differently than humans do. The model’s mistakes are jarring and out of context, but it’s also capable of strikingly innovative flourishes. Most human musicians, in my experience, hide in the safety of derivativeness until they’ve gained enough experience to take a fledgling leap at originality. Miller told me that DeepComposer is a way to get developers to see the possibilities using AWS AI cloud services in general, not to promote a specific generative AI service. AI can feel daunting to some developers because of the complexity of the models and the idea that designing them is as much art as science. AWS is trying to use fun to break down the barriers. AWS’s DeepComposer, DeepLens, and DeepRacer. [Photo: courtesy of Amazon]AWS has used that approach before. At last year’s re:Invent AWS offered developers a mini race car called DeepRacer and challenged them to create reinforcement learning models that guide the car safely around a track. The previous year, it announced a DeepLens camera, on which developers could run their own computer vision AI models to recognize objects. Amazon will be selling the DeepComposer keyboard (with the companion cloud AI service) on Amazon.com. It’s meant for developers, but there’s nothing stopping curious non-techies from buying one. Amazon had not yet set the price at the time of this writing. Technology Newsletter",0.13678537956888473,0.4496208774043826
19,https://news.google.com/articles/CBMi6QFodHRwczovL3d3dy5tc24uY29tL2VuLXVzL25ld3MvdGVjaG5vbG9neS9ib3QtZHlsYW4tc2luZ2VyLXNvbmd3cml0ZXItcm9ib3QtY2FsbGVkLXNjaGltb24tY2FuLXdyaXRlLWl0cy1vd24tbHlyaWNzLWFmdGVyLXN0dWR5aW5nLXRlbnMtb2YtdGhvdXNhbmRzLW9mLXdvcmRzLXBlbm5lZC1ieS10aGUtbXVzaWNhbC1ncmVhdHMtYW5kLWhhcy1hbi1hbGJ1bS1vdXQtaW4tdGhlLXNwcmluZy9hci1CQjEwUXhGY9IBAA?hl=en-US&gl=US&ceid=US%3Aen,"



 

< PREVIOUS SLIDE

SLIDE 1 of 5

NEXT SLIDE >




Schimon and his human band are releasing an album on Spotify in the spring featuring songs written by the robot with help from Weinberg  
© Provided by Daily Mail


Shimon the singing songwriting robot has been taught to write his own lyrics by studying tens of thousands of songs written by the musical greats. Developed by researchers from the Georgia Tech Center for Music Technology, the robot collaborates with human musicians and even has an album out in the spring. The robot was given a dataset of 50,000 lyrics covering all genres including rock, hip-hop, jazz and progressive as part of its song writing education. As well as writing the lyrics the robot can sing them and dance while performing with 'his band' made up of Georgia Tech students and researchers. Professor Gil Weinberg, creator of Schimon said he works with humans to create music, they are a mixture of songs made by human and robot together. 'I suggest a theme, he writes lyrics and gives me a melody that I compose into a song - we then perform it together with other humans while he sings,' he said. The process starts when Weinberg gives Schimon a theme - such as space - and the robot will then go away and come up with a set of 'coherent lyrics' that flow together. 'It learns the correlation between the thousands of lyrics it has studied and can then go into the dataset to generate new lyrics that fit melodies,' said Weinberg. 'I always wanted to write songs, but I just can't write lyrics. I'm a jazz player,' Weinberg said. 'This is the first time that I actually wrote a song, because I had inspiration: I had Shimon writing lyrics for me.'  The lyrics created by Schimon include the lines 'there may be music on a star into your mind' written for the song Into Your Mind, the first release from the album.  PhD student Richard Savery who worked on the robot plays base in the band with Schimon, says it is an unusual but rewarding experience playing with it. 'It is great interacting with him, I play base and it feels like another musician on stage, there is lots of dialogue between me and him,' he said. 'There are ups and downs in the relationship, sometimes it is 'it's really annoying with that guy' and other days it is the greatest collaboration.' As well as having him study the lyrical datasets they use semantic knowledge - that is where it can identify one word and link it to other matching words.  'Shimon will knows the word storm is linked to rain as will generate lyrics linked to rain such as sunshine,' said Savery. 'The whole lyric system is finding words that are interconnected.' When Shimon sings the songs he really does sing, said Weinberg. He has a unique voice created by collaborators at Pompeu Fabra University in Barcelona and involved creating an AI voice trained on hundreds of other songs. Along with his new musical skills Shimon has some new hardware that improves its movement and ability to play the marimba. He is still mostly stationary, but he has a mouth, new eyebrows, and new head movements designed to help convey emotion and interact with his bandmates.  'Shimon plays much faster — about 25 to 30 hertz at the maximum — and also much more expressively, playing from a soft dynamic range to a strong dynamic range,' said PhD. student Ning Yang, who designed the all-new motors and hardware. 'That also allows him to do choreography during the music being played,' he said. Yang worked closely with fellow PhD. student Lisa Zahray, who created a new suite of gestures for the robot — including how he uses those new eyebrows. 'We have to think about his role at each time during the song and what he should be doing,' Zahray said.  'We also want to make sure he's interacting with the other musicians around him to give that feel that he's performing with people.' Shimon can count in at the beginning of songs to cue the band, and sometimes he’ll wave his mallets around in time to the music.  New brushless DC motors mean he has a much greater range of motion and control of that motion.  Yang accomplished that by bringing his engineering knowledge and musical background together to create human-inspired gestures. 'A big part of what we are trying to do is to take the research and bring it to the people,' Weinberg said. 'I want the whole experience, it is just like a human, it is interesting but I think we want the audience to just sit back and say 'I like the music'. 'This is music a human themselves might not have written, it is unique to the robot.' Th partnership with people is key for Weinberg, who said teaching Shimon new skills isn’t about replacing musicians but expanding music. “We will need musicians, and there will be more musicians that will be able to do more and new music because robots will help them, will generate ideas, will help them broaden the way they think about music and play music,” Weinberg said. Shimon, Weinberg, and the band are building a touring schedule now with the goal of taking their unique blend of robot and human-created music to more people.  Weinberg said he hopes those shows will prove to be more than a novelty act",0.1558845497225779,0.465800611751316
20,https://news.google.com/articles/CBMiUGh0dHBzOi8vd3d3LnRoZW5hdGlvbmFsaGVyYWxkLmNvbS8yODY0MDUvc3VtbWVyLW5vc3Rvcy1mZXN0aXZhbC1qdW5lLTIxLTI4LTIwMjAv0gEA?hl=en-US&gl=US&ceid=US%3Aen,"ATHENS – A week for us all to look forward to! Eight captivating, creative, and carefree days in June to feed our minds, bodies, and souls. The SNF Conference on “Humanity and Artificial Intelligence,” featuring speakers such as Stuart Russel and Nikolas Christakis, will stimulate our minds and expand our horizons. The now-classic SNF RUN: Running to the Future will test our endurance and our dedication. Visual events and installations will engage our senses and our imagination. Concerts with renowned international artists, such as Burna Boy, will invigorate us with their power and vibrancy. These magical June days are, as always, free and open to everybody. The Summer Nostos Festival welcomes the new decade enthusiastically, full of optimism and energy. From June 21 to 28, 2020, at the Stavros Niarchos Foundation Cultural Center (SNFCC) in Athens, pioneering artists, researchers, and thinkers invite us to participate in a world of ideas, opinions, collaborations, activities, performances, and film screenings. The Summer Nostos Festival takes place through the exclusive support of the Stavros Niarchos Foundation (SNF), in collaboration with the Stavros Niarchos Foundation Cultural Center (SNFCC). Every new technology we develop brings not only scientific, but also ethical discovery. From the alphabet to the telescope, from the internet to the gene sequencer, each new invention recasts our understanding of ourselves and of our place in the world, provoking new answers to the eternal question: how, then, shall we live? As longstanding dreams of Artificial Intelligence (AI) become reality, they will touch every part of the human experience. And so, we ask, how will AI reshape what it means to be human in the decades to come? Can we put AI’s power to work for everyone? Will AI liberate us from the drudgery of needless labor? Or will it further enrich a few at the expense of the many? How will AI augment humanity’s thinking and creativity? How can we keep AI from learning the wrong lessons about us? And how might AI and humanity ultimately co-evolve together? The SNF Conference will bring together a thoughtfully curated lineup of AI innovators, leaders, critics, iconoclasts, artists, and experts in law, medicine, science, and the humanities drawn from around the world, for an unconventional dialogue about AI—one that puts humanity at the center. We will ask questions from many perspectives and disciplines to expand the usual dialogue on what may yet be humanity’s most powerful creation. Invited speakers include computer scientist and AI pioneer Stuart Russell, author of Human Compatible: AI and the Problem of Control; leading journalist, author, and entrepreneur Krista Tippett; Google Senior Scientist Blaise Agüera y Arcas; former Prime Minister of Australia Kevin Rudd; celebrated science fiction author Ted Chiang; Director of the Dalai Lama Center for Ethics and Transformative Values, The Venerable Tenzin Priyadarshi; Greek-American sociologist and physician Nicholas Christakis; AI anthropologist Madeleine Elish; philosopher Sean Kelly; MIT AI researcher Karthik Dinakar; humanitarian futurist Aarathi Krishnan; accelerating futures researcher Azeem Azhar; chess legend and political activist Garry Kasparov; Founder and Lab Director of the Interactive Robots and Media Lab Nikolaos Mavridis; and many others. The third annual SNF Agora Institute Workshop will take place as part of SNF’s DIALOGUES series on June 24, 2020, under the title “Technology and Democracy:  Reimagining Connections and Borders.” SNF RUN: Running into the Future  The Irrepressibles: As we await the release of their latest album, Superheroes, coming out in mid-March, Jamie Irrepressible and his art-pop ensemble will treat the SNFestival audience to their new work, as part of their international tour. (21/6)    Your email address will not be published. Required fields are marked * Comment  Name *  Email *   

",0.08136363636363635,0.4594545454545455
21,https://news.google.com/articles/CBMiUWh0dHBzOi8vd3d3LmNsYXNzaWNmbS5jb20vbXVzaWMtbmV3cy9haS1jb25jZXJ0LWhvc3RlZC13Y2l0LWZpcnN0LXRpbWUtb3JjaGVzdHJhL9IBUWh0dHBzOi8vYW1wLmNsYXNzaWNmbS5jb20vbXVzaWMtbmV3cy9haS1jb25jZXJ0LWhvc3RlZC13Y2l0LWZpcnN0LXRpbWUtb3JjaGVzdHJhLw?hl=en-US&gl=US&ceid=US%3Aen,"
Classic FM
 


                
                    Saturday Night at the Movies with Andrew Collins
                
                

5pm - 7pm
 
Peter Pan - Fairy Dance

                        

    
    James Newton Howard
    

                    
 
                        
                            News
                            


 30 September 2019, 16:57 
        By Helena Asprou
     A visionary performance in Armenia will feature a live, AI-composed orchestral score, performed by musicians from around the world. The World Congress of Information Technology (WCIT) has announced that it will be hosting its first concert to be composed entirely by artificial intelligence. The innovative outdoor concert takes place in Armenia as part of the organisation’s opening ceremony, and will be performed by the ‘World Orchestra’. Specially formed for the event, the orchestra has been made up of more than 100 professional musicians – including 75 from orchestras in the 14 countries that previously hosted WCIT, and 30 from the Armenia State Symphony Orchestra. The unique ensemble will be led by Sergey Smbatyan, a virtuoso violinist and Principal Conductor of the Armenian State Symphony Orchestra, who also came up with the idea for the show. The talented collective will be performing two works in total – but unlike many other classical concerts (where the pieces are learnt by musicians beforehand), these will both be written live by AI and then played in real-time. The first AI piece will be inspired by the national anthems of the 15 previous host countries, while the second will be composed from a database of traditional Armenian chants.  These chants are typically melismatic, monophonic melodies used in the liturgy of the Armenian Apostolic Church, which use a specific rhythmic pattern and consist mainly of hymns.  In order to perform the compositions, AI technology will transmit each musical part to electronic tablets on the players’ stands – as it’s being written. The bright initiative will be tested out during rehearsals, but if all goes well then this is surely another exciting step forward for music and technology.  Smbatyan said: “This ambitious creative endeavour will explore uncharted territory, creating symbiosis where art, culture, and technology meet.  “I like to call this type of AI ‘Artificial Talent’, as it brings together these two definitive fields. The whole world will witness this extraordinary fusion of music and technology, and gives us the opportunity to showcase the power of music.  “The creation of this orchestra underlines the importance of culture within the technology sphere, and once again proves that music transcends boundaries and does not recognise nationality.” A WCIT spokesperson added: “The world has witnessed AI make developments and improvements in spheres such as mechanical engineering and transportation. Now, this orchestra and this concert will convey a powerful message to the world – they’ll demonstrate new horizons for the future of art, science and engineering all working together.  “We’re thrilled to have the first ever musical project at WCIT, and we think the Armenian State Symphony Orchestra are the perfect musicians to execute such an ambitious venture.” The WCIT’s opening ceremony is free to attend and will be held between 6 and 9 October. To find out more, visit wcit2019.org. Puccini Discover Music Lifestyle Videos See more Latest news 1 day ago Discover Music 1 day ago Beethoven 1 day ago Videos 1 day ago New York Metropolitan Opera 1 day ago See more Latest videos 1 day ago Bach 4 days ago Videos 4 days ago Videos 4 days ago Yo-Yo Ma 4 days ago Discover Music See more Latest pictures Discover Music Bach Discover Music Simon Rattle",0.23017815517815518,0.5486104173604173
22,https://news.google.com/articles/CBMiQ2h0dHBzOi8vd3d3LnJlZGJ1bGwuY29tL2ludC1lbi9ob3ctYWktd2lsbC1tYWtlLWV2ZXJ5b25lLWEtbXVzaWNpYW7SAQA?hl=en-US&gl=US&ceid=US%3Aen,,0.0,0.0
23,https://news.google.com/articles/CBMiTmh0dHBzOi8vaGFwcHltYWcudHYvOC10b29scy10aGF0LXVzZS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS10by1wcm9kdWNlLW11c2ljL9IBAA?hl=en-US&gl=US&ceid=US%3Aen,,0.0,0.0
24,https://news.google.com/articles/CAIiEEa7JP18fALGiWzX_ZO7d2EqFwgEKg4IACoGCAow3O8nMMqOBjCzr7gD?hl=en-US&gl=US&ceid=US%3Aen,"Filed under: The Future of Music, episode 2 The idea that artificial intelligence can compose music is scary for a lot of people, including me. But music-making AI software has advanced so far in the past few years that it’s no longer a frightening novelty; it’s a viable tool that can and is being used by producers to help in the creative process. This raises the question: could artificial intelligence one day replace musicians? For the second episode of The Future of Music, I went to LA to visit the offices of AI platform Amper Music and the home of Taryn Southern, a pop artist who is working with Amper and other AI platforms to co-produce her debut album I AM AI. Using AI as a tool to make music or aid musicians has been in practice for quite some time. In the ‘90s, David Bowie helped develop an app called the Verbasizer, which took literary source material and randomly reordered the words to create new combinations that could be used as lyrics. In 2016, researchers at Sony used software called Flow Machines to create a melody in the style of The Beatles. This material was then turned over to human composer Benoît Carré and developed into a fully produced pop song called “Daddy’s Car.” (Flow Machines was also used to help create an entire album’s worth of music under the name SKYGGE, which is Danish for “shadow.”) On a consumer level, the technology is already integrated with popular music-making programs like Logic, a piece of software that is used by musicians around the world, and it can auto-populate unique drum patterns with the help of AI. Now, there’s an entire industry built around AI services for creating music, including the aforementioned Flow Machines, IBM Watson Beat, Google Magenta’s NSynth Super, Jukedeck, Melodrive, Spotify’s Creator Technology Research Lab, and Amper Music. Most of these systems work by using deep learning networks, a type of AI that’s reliant on analyzing large amounts of data. Basically, you feed the software tons of source material, from dance hits to disco classics, which it then analyzes to find patterns. It picks up on things like chords, tempo, length, and how notes relate to one another, learning from all the input so it can write its own melodies. There are differences between platforms: some deliver MIDI while others deliver audio. Some learn purely by examining data, while others rely on hard-coded rules based on musical theory to guide their output.  However, they all have one thing in common: on a micro scale, the music is convincing, but the longer you listen, the less sense it makes. None of them are good enough to craft a Grammy Award-winning song on their own... yet. Of all the music-making AI platforms I’ve tried out, Amper is hands down the easiest to use. IBM and Google’s projects require some coding knowledge and unpacking of developer language on GitHub. They also give you MIDI output, not audio, so you also have to have a bit more knowledge about music production to shape the output into an actual song. Amper, on the other hand, has an interface that is ridiculously simple. All you have to do is go to the website and pick a genre of music and a mood. That’s it. You don’t have to know code or composition or even music theory in order to make a song with it. It builds tracks from prerecorded samples and spits out actual audio, not MIDI. From there, you can change the tempo, the key; mute individual instruments, or switch out entire instrument kits to shift the mood of the song its made. This audio can then be exported as a whole or as individual layers of instruments (known as “stems”). Stems can then be further manipulated in DAWs like Ableton or Logic. I had Amper generate the clip of music below while cruising around LA in the back seat of my friend’s car. Using my phone, I picked rock as the genre, and then, appropriately, “driving” as the mood. It spent about a minute churning away before delivering 30 seconds of audio. The result isn’t radio-ready, but it has chords, a little structure, and it sounds... pleasant. It could easily sit in the back of a YouTube video or an advertisement and no one would guess it was coded, not written. As someone who makes music, the idea that code can do what I do is freaky. It’s unnerving to think that an algorithm can make a not-terrible song in minutes and that AI is getting in on creative turf we categorize as distinctly human. If AI is currently good enough to make jingly elevator music like the clip above, how long until it can create a number one hit? And if it gets to that point, what does it mean for human musicians? These aren’t questions that Taryn Southern is concerned with. Southern is an online personality who you might know from her YouTube channel or when she was a contestant on American Idol. These days, Southern is interested in emerging tech, which has led to her current project: recording a pop album. Those two things don’t sound like they could be related, but her album has a twist: instead of writing all the songs herself, Southern used artificial intelligence to help generate percussion, melodies, and chords. This makes it one of the first albums of its kind, a collaboration of sorts between AI and human. Amper was the first AI platform Southern used when beginning her album, and now she also works with IBM Watson Beat and Google Magenta. She views AI as a powerful tool and partner, not a replacement for musicians.  “Using AI, I’m writing my lyrics and my vocal melodies to the actual music and using that as a source of inspiration,” Southern tells me. “I find that really fun, and because I’m able to iterate with the music and give it feedback and parameters and edit as many times as I need, it still feels like it’s mine in a sense.” To get an idea of how a human can work with AI, look at Southern’s 2017 single, “Break Free.” The SoundCloud audio below is an early export of material from Amper. Compare that to the YouTube video that has the final, released version of the song. Bits of the AI-composed original peek through here and there, but it’s more like seasoning, not the main dish. To transform it into a pop song, Southern made a lot of creative decisions, including switching instruments, changing the key, and, of course, writing and performing the vocals. Southern originally turned to AI because even though she was a songwriter, she knew “very, very little about music theory.” It was a roadblock that frustrated her to no end. “I’d find a beautiful chord on the piano,” Southern says, “and I’d write an entire song around that, but then I couldn’t get to the next few chords because I just didn’t know how to play what I was hearing in my head. Now I’m able to iterate with the music and give it feedback and parameters and edit as many times as I need. It still feels like it’s mine in a sense.” This feeling of empowerment is exactly what Amper Music is trying to deliver. “I don’t look at it like artificial intelligence,” Amper co-founder Michael Hobe says. “It’s more of intelligence augmentation. We can facilitate your creative process to cut a lot of the bullshit elements of it. For me, it’s allowing more people to be creative and then allowing the people who already have some of these creative aspects to really further themselves.” When Hobe says “bullshit elements,” he’s talking about a guitarist not knowing how to orchestrate an instrument they’ve never worked with before, the time spent crafting the velocity of individual drum hits, or simply being faced with writer’s block. Amper isn’t meant to create the next AI superstar; it’s meant to enable musicians. Of course, using AI also has the added benefit of allowing Southern and others with no formal music background to participate in making music. It democratizes the creative playing field so anyone can play what they hear in their head, just like Southern. I ask Southern what she would say to people who think using AI is cheating. “Great,” she says. “Yes, we are totally cheating. If music is concretely defined as this one process that everyone must adhere to in order to get to some sort of end goal, then, yes, I’m cheating. I am leading the way for all the cheaters.” She laughs, and then pointedly says, “The music creation process can’t be so narrowly defined.” It’s something to think about. Every time a new technology is introduced and that tectonically shifts the way we create music, there are naysayers. Things like AutoTune, the use of samples and loops, and Digital Audio Workstations were all “disruptors” that we adapted to and are now commonplace tools and methods. AI will probably be next. The technology’s impact on the music industry as a whole remains to be seen. Will it destroy jobs? How will it affect musical copyright? Will it ever be able to work without a human? But people like Hobe and Southern believe it will ultimately reap positive benefits. Sure, an algorithm making music sounds scary because it mirrors human capabilities that we already find mysterious, but it’s also a compelling tool that can enhance said human capabilities. AI as a collaborator increases access to music-making, it can streamline workflows, and it provides the spark of inspiration needed to craft your next hit single. “You’re collaborating and working with the AI to achieve your goal,” Hobe says. “It’s not that the AI is just doing its own little thing. It’s all about the process between it and you to achieve that final artistic vision.”",0.10200811806203966,0.45326526233388964
25,https://news.google.com/articles/CBMiXGh0dHBzOi8vd3d3Lm5lb3dpbi5uZXQvbmV3cy9jb21tYW5kLS1jb25xdWVyLXJlbWFzdGVyZWQtbGF1bmNoZXMtanVuZS01LW9uLW9yaWdpbi1hbmQtc3RlYW0v0gFbaHR0cHM6Ly93d3cubmVvd2luLm5ldC9hbXAvY29tbWFuZC0tY29ucXVlci1yZW1hc3RlcmVkLWxhdW5jaGVzLWp1bmUtNS1vbi1vcmlnaW4tYW5kLXN0ZWFtLw?hl=en-US&gl=US&ceid=US%3Aen,"

              By
              
                Pulasthi Ariyasinghe

Neowin
@LoneWolfSL
              
               ·
            
Mar 10, 2020 10:36 EDT

with 14 comments

 Command & Conquer fans will soon get the chance to experience the games that started it all in a new light, as the remaster bundle of Tiberian Dawn and Red Alert now has a release date. EA today announced that the highly-anticipated Command & Conquer Remastered Collection will be releasing on June 5, and it will be available on both Origin and Steam. See the new reveal trailer detailing what is incoming in this new edition above. Developed by Petroglyph and Lemon Sky Studios, the double game pack touts all of the expansions, completely remastered visuals that can be toggled to the legacy version while playing, over seven hours of remastered and re-recorded music by the original composer Frank Klepacki and the Tiberian Suns, AI upscaled cinematics, and a modernized UI. Customizable hotkeys, quality of life updates to the controls for aspects like improved unit queuing and selection, as well as missions and cinematics that were console exclusive before are here as well. See the full feature list by heading over here. Command & Conquer Remastered Collection is now available for pre-order on Origin and Steam for $19.99. Subscribers to EA's Origin Access Premier service will also get the game on day one. Moreover, there's a couple of physical special editions available for pre-order on the game's newly setup portal as well, slated to arrive with a whole lot of extra goodies. 
5 hours ago

with 22 comments

 
7 hours ago

with 0 comments

 
15 hours ago

with 14 comments

 
Mar 13, 2020

with 10 comments

 
Mar 10, 2020
 
Mar 6, 2020
 
Mar 5, 2020
       · Hot!
 
Mar 3, 2020
 Please enter your reason for reporting this comment. The following codes can be used in comments. 
            ori and the will of the wisps
           
            yoga smart tab
           
            galaxy s20
           
            galaxy s20 ultra
           
            yoga c740
           
            samsung galaxy s20 plus
           
            snapdragon technology summit
           
        nintendo switch
       
        galaxy s20 ultra
       
        find x2 pro
       
        windows 10x
       
© Since 2000 Neowin LLC.
                All trademarks mentioned are the property of their respective owners.
              ",0.11363011988011988,0.3817432567432567
26,https://news.google.com/articles/CAIiEEOeRJ3pmldpOnjX8vfvHrgqFwgEKg4IACoGCAow3O8nMMqOBjCt2ugF?hl=en-US&gl=US&ceid=US%3Aen,"Filed under: DeepComposer is for developers to get into machine learning and music Amazon Web Services debuted a keyboard called DeepComposer this week, claiming it’s “the world’s first musical keyboard powered by generative AI.” It has 32 keys, costs $99, and connects to a software interface that uses machine learning and cloud computing to generate music based on what you play. It’s been unclear who this is for, and many have latched on to the fact that the music it creates just sounds bad. It looks like a consumer product, and Amazon used an over-the-top presentation to hype it, which included what AWS claimed was “the first hybrid AI human pop acoustic collaboration.” (It’s not.) But actually, the keyboard is intended to be a beginning tool for developers to get into machine learning and music. The device is AWS’s newest offering for developers to familiarize themselves with aspects of machine learning, following AWS DeepRacer (an RC car) and AWS DeepLens (a camera). DeepComposer is not meant to make music for entertainment purposes or push the state of generative AI. It will never be marketed to aspiring musicians. I took a deep look at AWS #DeepComposer and I have to say that I have no idea what to do with this nor who it is for. I can't imagine training ML models by playing thousands of tracks on a physical keyboard. Then what would I do with the inferred MIDI? How is that teaching me ML? Even though Amazon says DeepComposer is for developers, many devs don’t understand what to do with it. According to Amazon, since this is for developers, “no musical knowledge” is needed. But it still uses traditional music theory terms and is centered on a thing that very much requires a modicum of musical knowledge — a keyboard. The $99 physical keyboard isn’t even necessary since the DeepComposer software has a virtual keyboard. And AWS didn’t develop or design the keyboard. It’s a MIDI controller by Taiwanese company Midiplus that’s been around for years. Amazon sells it for $46.15. They’re the same. Amazon’s costlier version is not “powered by AI,” it sends MIDI to software that’s hooked up to the cloud. Any MIDI keyboard would technically work. At the end of the day, this is not for the everyday person. AWS does not claim it is. DeepComposer is not supposed to write the next radio hit. But it’s also hard to see how it’s meaningful for its target audience. It is compelling to see a company as large as Amazon getting into the world of democratizing music creation, even if this first iteration somewhat missed the mark. Who knows, maybe it’s all a long play to get more devs familiar with AWS’s machine learning platform, SageMaker. But given how simple DeepComposer is, developers could learn about the same basics of machine learning and music through any number of other interfaces, like Google Magenta, without being pitched an overpriced controller stamped with a logo. A newsletter about computers",0.0615945165945166,0.358023088023088
27,https://news.google.com/articles/CBMiS2h0dHBzOi8vbmF0aW9uYWxwb3N0LmNvbS9uZXdzL2NhbmFkYS93aGF0LWNhbi1haS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1kb9IBT2h0dHBzOi8vbmF0aW9uYWxwb3N0LmNvbS9uZXdzL2NhbmFkYS93aGF0LWNhbi1haS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1kby9hbXA?hl=en-US&gl=US&ceid=US%3Aen,"November 28, 20198:00 AM EST Last UpdatedDecember 5, 20194:38 PM EST By Nicole Schmidt Back in the ‘70s, renowned mathematician James Lighthill predicted that machines would never be capable of reasoning or even simple tasks, like being able to checkmate a chess pro. Fifty years later, artificial intelligence programs have not only defeated the world’s top chess players, but they can match — and in some cases, outperform — humans when it comes to art, science and even companionship. Here’s a look at nine outrageous things AI can do. 1. Impersonate Vincent van Gogh  Vincent van Gogh’s painting technique was considered to be so revolutionary that art critics thought it was inimitable, but scientists from Germany’s Bethge Lab invented a program that can replicate his iconic brush strokes. The AI system analysed the colours, shadows and highlights of “Starry Night,” then used the same techniques to create an original painting of the Neckar river in Tuebingen, Germany. 2. Compose classical music Most creatives will argue that human emotion — the one thing computers lack — is the key to producing meaningful art. But some companies, like Aiva Technologies, are challenging that notion. The startup created an “Artificial Intelligence Virtual Artist” that learned music theory by studying a large library of famous composers (including Bach, Beethoven and Mozart). The machine is now capable of creating original classical melodies in a matter of minutes, many of which have been used in films and video games. 3. Make Barbie come to life Barbie’s AI-powered doll is something straight out of a Toy Story movie. She has a tiny microphone concealed in her necklace that can analyze speech and respond accordingly (all in less than a second). If the ability to have a coherent conversation wasn’t impressive enough, she can also “remember” what children tell her and refer to it in future exchanges. (Whether the toy falls into the “cool” or “creepy” camp is still up for debate.) 4. Be your romantic partner Harmony AI took notes from Black Mirror to create their AI-powered sex doll. Just as clients can decide whether they want a blonde or brunette, they can also customize their doll’s personality. Similar to Hello Barbie (but the R-rated version), the doll uses Bluetooth to hear and respond. Since it comes with built-in memory, its makers say it can develop meaningful relationships with its user over time — and it could be all yours for a cool $20,000. 5. Make you dance Using just a single full-body photograph, researchers at Nvidia, a California-based gaming company, can turn anyone into a backup dancer from one of Beyonce’s music videos. Their software learns mapping functions from pre-recorded videos and applies those movements to photographs. 6. Turn you into a digital puppet One of AI’s biggest ethical conundrums is the “deepfake” — a fake video or audio recording that looks and sounds like the real deal. Similar to how Nvidia researchers create their dancing videos, the software can essentially perform a virtual head and body transplant by decoding and then reconstructing a brand new face. There is also software that can replicate someone’s voice, or essentially turn them into a digital puppet. Some of the results (Beck Bennett impersonating a shirtless Vladimir Putin) are hilarious, while others (hyper-realistic revenge porn) are downright terrifying. Back in 2017, a particularly realistic speech delivered by an AI Barack Obama made the rounds on the internet. 7. Detect illness via smell In a few years, diagnosing an illness could be as easy as taking a breathalyzer test. A team of scientists from around the globe have developed an artificial intelligence system that can detect 17 diseases — including Parkinson’s, Crohn’s and several types of cancer — using only a breath sample. The “NaNose” was trained to identify patterns in the chemical makeup of specific diseases by analyzing more than 8,000 patients scents. Right now, the system has an 86 per cent accuracy rate. 8. Read your mind In what’s been hailed as the “last privacy frontier,” Facebook is working on creating a device that can read your mind. The real life veritaserum (for the non-Harry Potter fans, that’s truth serum) can decode brain signals and translate them into words. The short-term goal is to use the technology to help patients with paralysis, but eventually, Facebook wants to make a wearable headset available to the masses that would enable users to control digital devices with their minds. 9. Find missing children Last year, police in New Delhi used a new facial recognition software to capture images of 45,000 kids around the city, then find matches in India’s database of missing and vulnerable children. They were able to identify 3,000 missing children in just four days. We want to improve your reading experience 3°C Overcast Feels like -1°C Postmedia is committed to maintaining a lively but civil forum for discussion and encourage all readers to share their views on our articles. Comments may take up to an hour for moderation before appearing on the site. We ask you to keep your comments relevant and respectful. We have enabled email notifications—you will now receive an email if you receive a reply to your comment, there is an update to a comment thread you follow or if a user you follow comments. Visit our community guidelines for more information and details on how to adjust your email settings.",0.0890444896468993,0.44507249778334107
28,https://news.google.com/articles/CAIiEJwCeE4oojWRPtUboqFqPEkqGQgEKhAIACoHCAow1t36CjDslvICMPaF0gU?hl=en-US&gl=US&ceid=US%3Aen,"Follow Us He was born into a deeply religious family at Rahon near Jalandhar in undivided Punjab on February 18, 1927. As a child, it is said, he ran away from home to pursue his passion. And it took over two decades to his family to accept Mohammad Zahoor Khayyam Hashmi as music composer Khayyam. During World War II, Khayyam joined the British Indian Army around 1943 and became a part of the cultural troupe headed by Urdu poet Faiz Ahmed Faiz, who had once described Khayyam as “a poet of melody”. After partition, he returned to Mumbai and joined forces with music composer Rahman. Interestingly, the duo named them as Sharmaji-Vermaji. It was only after film maker Zia Sarhadi and his friends Majrooh Sultanpuri, Ali Sardar Jafri and Chandulal Shahgave insisted that he should use his real name, that he became Khayyam when Footpath (1953) was in the making. In a TV interview, many years ago, responding to former Chief Election Commissioner of India SY Qureshi’s query whether Islam prohibits music and singing despite its “therapeutic value”, this is what Khayyam had to say: “Yes it is prohibited. I am a staunch Muslim. But our religious scholars don’t tell us which kind of music or melody is forbidden. There is a music that gives genuine happiness to the people. Then there is music that brings people nearer to the evil. This kind of music is forbidden.” And in the same breath, he told Qureshi, “Have you heard azaan? It has such a melody and attraction! We get to hear the same melody in the sounds of bells, conch shells and flute in Hindu temples. In churches also, prayers are offered through music. In Gurdwaras, Gurbani is sung…its all music.” He also told Qureshi how his father asked him to bow before Bhagat Singh’s ancestral home when they passed through freedom fighter’s ancestral village. With childlike innocence, he spoke enthusiastically about a patriotic song written by Jan Nisar Akhtar that he composed for the country in the event of war. Apne Sabhi Sukh Aik Hain, Apne Sabhi Gam Aik Hain, Awaaz Do Hum Aik Hain, Awaaz Do Hum Aik Hain! Those who had the privilege to visit Khayyam’s home in Juhu tell us that his music room was studded with images, idols and scriptures of all the religions besides the emblems of Islam. Though his wife, noted singer Jagjit Kaur, was a Sikh, the couple chose to give a Hindu name to their only son, Pradeep. Known for his uncompromising nature and simplicity in life and work, the maestro in his illustrious career demonstrated how a great poetry set to soulful music doesn’t age with time. For Bazaar (1982), he composed ghazals of poets who had left this world long ago. Whether it was the song sung by his wife Jagjit Kaur, Dekh lo aaj hum ko jee bhar kay or the poem penned by Bashar Nawaz, Karo ge yaad to har baat yaad aaye gi, or Makhdoom’s ghazal, Phir chidi raat baat pholon ki or Mir Tqi Mir’s verses, Dekhaai diye yon kay bekhud kiya, his compositions remain hauntingly beautiful as ever. Eminent poet and lyricist, Nida Fazli writes in his biographical novel how he got introduced to film world. After having read it in a literary magazine, Khayyam composed his iconic poem, “Kabhi kisi ko mukammal jahan nahi milta” and used it in Ahista Ahista (1981). Since Fazli was homeless during those days, the payment cheque, he says, kept chasing him for months. Though Khayyam worked in just 50 plus movies over five decades, he remains a towering figure of “golden era” in Hindi film music industry. Most of the songs, ghazals, mujras, bhajans, naats or even Urdu poems—that he composed became benchmarks in their own right. His soul-stirring compositions in movies such as Lala Rukh (1958), Shola Aur Shabnam (1961), Shagoon (1964), Aakhri Khat (1966), Kabhi Kabhie (1976), Trishul (1978), Kaala Patthar (1979), Noorie (1979), Thodi Si Bewafaai (1980) and Umrao Jaan (1981) remain an aural treat for music connoisseurs. Sahir Ludhianvi’s epic poem, Kabhi Kabhi was originally composed by Khayyam for film director Chetan Anand. But when Yash Chopra declared to make a move on the same title, the duo approached Anand. They were told that the lyrics and composition were too old to be used in a new film. Both Sahir and Khayyam proved him wrong eventually. Remarkably, his magical melodies remained uninfluenced by western music when his contemporaries were heavily borrowing from here and there. His compositions distinctively evoke the scent of the Indian soil.  Admired by many musicians for creating unique intricate tunes that have immaculate silent pauses, Khyaam never allowed music to dominate words in his compositions. And his compositions only enhance the essence of the lyrics. Take, for instance, Aap yoon faaslon se guzartay rahe or Apne aap raaton mein or Kahin aik massom nazuk si larki or Tum apna ranjo gamor Hazar raahain murd kay dekhin. These compositions speak for themselves and their creator, who considered creativity as “ibaadat” (prayer) to the Almighty. It was during the making of Phir Subah Hogi (1958), a movie that brought Khayyam to the limelight, that he became a natural choice for the filmmakers. They were looking for a composer who had read Fyodor Dostoevsky’s Crime and Punishment. On her 80th birthday during a TV interview, Lata Mangeshkar told film writer and poet Javed Akhtar that it was very difficult for her to name one music director as her favourite. She praised yesteryears’ music directors like Salil Choudhary, Madan Mohan, SD Burman, RD Burman and Jaidev …But without any hesitation, she replied that the one song that stayed with her after leaving the recording studio was “Ai dil-e-nadaan…” from Razia Sultan (1983). It was composed by none other than Khayyam. The legendary composer passed away in Mumbai last year. He was 92. This is how Javed Akhtar remembered him: “He has given many all-time great songs but to make him immortal only one was enough: Woh subah kabhi to aayegi.” Months before his death, Khayyam hadn’t celebrated his birthday last year due to Pulwama terror attack. He had also donated Rs 5 lakh towards relief efforts for the family members of the slain soldiers. In his lifetime, Khayyam was honoured with Sangeet Natak Akademi Award and later, the Padma Bhushan, third-highest civilian award in the country. For all the latest India News, Follow India Section. ",0.19089789520824002,0.5120447081653977
29,https://news.google.com/articles/CAIiEKzlt3vmDWHNxjmxhxNF1hoqGQgEKhAIACoHCAow2pqGCzD954MDMOClmgY?hl=en-US&gl=US&ceid=US%3Aen,"Away from the boardroom, Biocon boss likes to listen to Western classical music Aye Finance boss enjoys Hindustani classical music on a flight, says it helps him face biz challenges A DIY robot that reacts to Hindustani classical music Jazz, classical music help True Balance CEO relax & de-stress Trending Now Popular Categories Hot on Web In Case you missed it ET Verticals More from our network Other useful Links services Copyright © 2020 Bennett, Coleman & Co. Ltd. All rights reserved. For reprint rights: Times Syndication Service Log In/Connect with: Will be displayed Will not be displayed Will be displayed",0.17045454545454544,0.2977272727272728
30,https://news.google.com/articles/CAIiEG-Fhd3Kjs_hnjtMLn27JckqFQgEKg0IACoGCAowrqkBMKBFMMGBAg?hl=en-US&gl=US&ceid=US%3Aen,"Sign in to your Forbes account or register For instructions on how to disable your ad blocker, click here. If this is your first time registering, please check your inbox for more information about the benefits of your Forbes account and what you can do next! The days of debating if artificial intelligence (AI) will impact the music industry are over. Artificial intelligence is already used in many ways. Now it's time to consider how much it will influence how we create and consume music. Just as it does for other industries, in the music industry, AI automates services, discovers patterns and insights in enormous data sets, and helps create efficiencies. Companies in the music industry need to accept and prepare for how AI can transform business; those that won't will be left behind. The Amazing Ways Artificial Intelligence Is Transforming The Music Industry AI Catapults Growth in the Music Industry Businesses are being reshaped by technology, and those in the music industry are no exception. According to a McKinsey report, 70 per cent of companies will have adopted at least one AI technology by 2030. The promise of AI is that it will complement and augment our human capabilities. As we make better choices and become more effective and efficient thanks to the insights and support AI provides, it can drive growth and innovation. As a result of AI's impact, the creative process will likely transform. Scott Cohen is one thought leader in the music industry who saw the potential impact of technology on the music industry's future when others didn't. His thoughts for a distribution company in the digital music age ultimately became The Orchard, which was acquired by Sony in 2015 for $200 million. During his keynote presentation at the Eurosonic Nooderslag conference, he said, ""Every ten years something kills the music industry. If you want to know what's next look at the tech world."" 
 He explained that there are 20,000 new tracks uploaded to Spotify every day and AI is critical for helping sort through the options and delivering recommendations to listeners based on what they’ve listened to in the past. Cohen believes that AI and big data made the “music genre” obsolete because AI-generated playlists are made not based on genre, but what is determined to be good music. In addition, he suggests that our current paradigm of infinite choice is broken and recommends a new model of trusted recommendations. And technology such as blockchain could eliminate the need for brands being a conduit to connect people to music. The live music industry should also be considering how to incorporate augmented and virtual realities to the concert experience. AI Creates Music Back in 1951 British computer scientist, Alan Turing was the first to record computer-generated music using a machine that filled almost an entire floor of the lab. In recent years, the recording was restored by New Zealand researchers 65 years after it was created. Music composition either by AI or using AI continues today. AI starts by analyzing data from different compositions when it creates musical pieces. Through reinforcement learning, the algorithm learns what characteristics and patterns create music that is enjoyable or that mimics a certain genre. The AI model can also compose innovative musical numbers by combining elements in unique ways. Tech companies are investing in a future where AI creates or assists musicians in creating music. Google's Magenta project, an open-source platform, produced songs written and performed by AI and Sony developed Flow Machines, an AI system that’s already released “Daddy’s Car,” a song created by AI. Musicians and professionals in the music industry will need to acquire tech skills in order to leverage the power of AI tools that will help them do their jobs even better. Other AI services such as Jukedeck and Amper Music help amateur musicians to develop their own musical pieces with the assistance of AI. Audio Mastering with AI The listening experience is optimized for any device in a process called audio mastering. AI-based mastering services such as LANDR provide musicians with a more affordable alternative to human-based mastering, and so far more than 2 million musicians have used it to master more than 10 million songs. While there is still a creative component involved in audio mastering and some prefer to rely on humans to do this work, AI makes the services accessible to artists who wouldn’t be able to master their songs otherwise. AI, Marketing Music and Identifying the Next Stars How will listeners discover new artists and how will consumers know who to listen to? Artificial intelligence helps focus the effort. Audio-on-demand streams totaled 534.6 billion in the United States alone in 2018, according to BuzzAngle Music’s 2018 Year-End Report. Helping new artists get discovered is expensive, and without doing it effectively, a talented artist could remain unfound. AI technology can help determine which fans would enjoy an artist's music. Similarly, AI-powered features such as Spotify's Discovery Weekly, a curated list of music for each listener, help fans sort through the streams of music to find new music that's appealing to them. Artificial intelligence is also helping the industry with A&R (artist and repertoire) discovery. It’s always been challenging to comb through music and find promising artists that haven’t signed to a label, but it’s even more overwhelming with the deluge of streaming music today. Warner Music Group acquired a tech start-up last year that uses an algorithm to review social, streaming and touring data to find promising talent. Apple also acquired a start-up that specializes in music analytics to support the A&R process. AI is behind the scenes influencing the music we listen to in many ways. Bernard Marr is an internationally best-selling author, popular keynote speaker, futurist, and a strategic business & technology advisor to governments and companies. He… Bernard Marr is an internationally best-selling author, popular keynote speaker, futurist, and a strategic business & technology advisor to governments and companies. He helps organisations improve their business performance, use data more intelligently, and understand the implications of new technologies such as artificial intelligence, big data, blockchains, and the Internet of Things. Why don’t you connect with Bernard on Twitter (@bernardmarr), LinkedIn (https://uk.linkedin.com/in/bernardmarr) or instagram (bernard.marr)?",0.12998498100538916,0.5084514974821098
31,https://news.google.com/articles/CAIiEIM3H3NZEZ7YSpx9NYW4NmAqFAgEKg0IACoGCAowlIEBMLEXMOc_?hl=en-US&gl=US&ceid=US%3Aen,"Arcona Music took to the stage at Disrupt Berlin today to showcase its adaptive music service. The local startup utilizes machine learning to create musical beds capable of adapting to different contexts in real-time. The user simply needs to input a handful of parameters, and the service will adjust accordingly.  “Give it a style, an emotion and a musical theme, and you can say, ‘play this,’ and the engine will take that blueprint and realize it,” service cofounder Ryan Groves explained, in a conversation with TechCrunch. “If, at any point, the emotion or style changes, it will adapt to that and create this essentially infinite stream of music. You can play a particular song blueprint for as long as is necessary in any dynamic environment.” The service is still in its infancy, at the moment. Its two founders are its only two full-time employees, along with a part-time developer. Groves and co-founder Amélie Anglade bootstrapped the scrappy startup, which has yet to seek funding. Groves is a composer and musical theorist who formerly worked at popular AI-based music composition service, Ditty. Anglade is a music information retrieval specialist who worked at SoundCloud.  Rhythm gaming is the first clear application for the service. The popular gaming genre is built around a changing soundtrack and could potentially benefit from music that requires minimal pre-programing. Moving forward, the potential for such a service is far broader.  “In the very long term,” Groves said, “we should see this being almost your own personal orchestra, leveraging augmented reality, GPS and all that stuff, and just responding to your environment as you’re listening. 


",0.09237179487179488,0.4997619047619048
32,https://news.google.com/articles/CAIiEFpcPImgRTLjnRTWKUWTd8UqGQgEKhAIACoHCAowwKuQCzC0oKQDMJiYtAY?hl=en-US&gl=US&ceid=US%3Aen,You will be connected to www.thelocal.de in just a moment... Learn about Project Shield,0.0,0.0
33,https://news.google.com/articles/CBMiYGh0dHBzOi8vd3d3LmNsYXNzaWNmbS5jb20vY29tcG9zZXJzL2JlZXRob3Zlbi9uZXdzL2NvbXB1dGVyLWNvbXBsZXRlcy11bmZpbmlzaGVkLXRlbnRoLXN5bXBob255L9IBYGh0dHBzOi8vYW1wLmNsYXNzaWNmbS5jb20vY29tcG9zZXJzL2JlZXRob3Zlbi9uZXdzL2NvbXB1dGVyLWNvbXBsZXRlcy11bmZpbmlzaGVkLXRlbnRoLXN5bXBob255Lw?hl=en-US&gl=US&ceid=US%3Aen,"
Classic FM
 


                
                    Saturday Night at the Movies with Andrew Collins
                
                

5pm - 7pm
 
Peter Pan - Fairy Dance

                        

    
    James Newton Howard
    

                    
 
Composers
 
                        
                            Beethoven
                            


 16 December 2019, 16:31 | Updated: 17 December 2019, 14:25 
        By Maddy Shaw Roberts
     Beethoven’s unfinished symphony is set to be completed by artificial intelligence, in the run-up to celebrations around the 250th anniversary of the composer’s birth.  A computer is set to complete Beethoven’s unfinished tenth symphony, in the most ambitious project of its kind. Artificial intelligence has recently been used to complete Schubert’s ‘Unfinished’ Symphony No. 8, as well as to attempt to match the playing of revered 20th-century pianist, Glenn Gould. Beethoven famously wrote nine symphonies (you can read more here about the Curse of the Ninth). But alongside his Symphony No. 9, which contains the ‘Ode to Joy’, there is evidence that he began writing a tenth. Unfortunately, when the German composer died in 1827, he left only drafts and notes of the composition. Read more: What is the Curse of the Ninth – and does it really exist? > A team of musicologists and programmers have been training the artificial intelligence, by playing snippets of Beethoven’s unfinished Symphony No. 10, as well as sections from other works like his ‘Eroica’ Symphony. The AI is then left to improvise the rest. Matthias Roeder, project leader and director of the Herbert von Karajan institute, told Frankfurter Allgemeine Sonntagszeitung: “No machine has been able to do this for so long. This is unique.” “The quality of genius cannot be fully replicated, still less if you’re dealing with Beethoven’s late period,” said Christine Siegert, head of the Beethoven Archive in Bonn and one of those managing the project. “I think the project’s goal should be to integrate Beethoven’s existing musical fragments into a coherent musical flow,” she told the German broadcaster Deutshe Welle. “That’s difficult enough, and if this project can manage that, it will be an incredible accomplishment.” Read more: AI to compose classical music live in concert with over 100 musicians > It remains to be seen – and heard – whether the new completed composition will sound anything like Beethoven’s own compositions. But Mr Roeder has said the algorithm is making positive progress. Read more: Google’s piano gadget means ANYONE can improvise classical music > “The algorithm is unpredictable, it surprises us every day. It is like a small child who is exploring the world of Beethoven. “But it keeps going and, at some point, the system really surprises you. And that happened the first time a few weeks ago. We’re pleased that it’s making such big strides.” There will also, reliable sources have confirmed, be some human involvement in the project. Although the computer will write the music, a living composer will orchestrate it for playing. The results of the experiment will be premiered by a full symphony orchestra, in a public performance in Bonn – Beethoven’s birthplace in Germany – on 28 April 2020. See more Beethoven News See more Beethoven Music Discover Music See more Beethoven Pictures Discover Music Discover Music See more Beethoven Album Reviews See more Beethoven Guides",0.15480225988700563,0.4861581920903954
34,https://news.google.com/articles/CBMiOWh0dHBzOi8vd3d3Lm11c2ljdGVjaC5uZXQvbmV3cy9qYW0taW50cm9kdWNlLWFpLWNvbXBvc2VyL9IBPWh0dHBzOi8vd3d3Lm11c2ljdGVjaC5uZXQvbmV3cy9qYW0taW50cm9kdWNlLWFpLWNvbXBvc2VyLz9hbXA?hl=en-US&gl=US&ceid=US%3Aen,"Could your new favourite tune be composed by AI? JAM has developed an Artificial Intelligence platform that it says can “successfully generate high-quality music tracks using professional audio loops”. The company intends to showcase an initial wave of results from the system later next month on a new website. As incredible as it seems, JAM claim that “the AI’s neural network can now generate a new music composition in under ten seconds”. That’s slightly quicker than the six years it took Queen to write Bohemian Rhapsody. Whether the AI can compete with Queen, though, will be a different story. The JAM software boasts a catalogue of 100,000 audio loops, which are selected by the AI and arranged to create a whole track. Genres such as EDM, techno, house, hip hop, trap and synthwave can be built as directed by the user. JAM’s flagship product is a free music-making app, sharing the same name with the company. The new AI software aims to interact with users and collaborate, to herald the beginning of what JAM are calling the ‘Era of the AI composer.’ The software has been trained with a dataset of 3.5 million music tracks, with metadata such as moods, tempo, skip rates, likes, plays, shares, user comments and more. JAM estimates it possesses nearly 500 million data points to gather information from to create its music. Rory Kenny, CEO of JAM explains that the “AI behaves like a DNA sequencer – effortlessly arranging and generating an infinite variety of complex loop-based music compositions. We’re just at the very beginning of our development, but the results are already impressive – and it can only get better from here”. JAM will launch its website next month to showcase artist collaborations with the AI, with commercial release set for March 2020. Be updated with all the latest news, offers and special announcements. Subscribe now and receive a free chorus plug-in! We provide insight and opinion on the gear, tools, software and services to enhance and expand the minds of music makers and listeners. © 2020 MusicTech is a member of the media division of BandLab Technologies.",0.17230093139184044,0.4403417289780927
35,https://news.google.com/articles/CAIiEPrQrBYVSRjfs9KgVKI6VR8qFQgEKg0IACoGCAowrqkBMKBFMMGBAg?hl=en-US&gl=US&ceid=US%3Aen,"Sign in to your Forbes account or register For instructions on how to disable your ad blocker, click here. If this is your first time registering, please check your inbox for more information about the benefits of your Forbes account and what you can do next! Taryn Southern Nothing is more human than creativity. Humans have the ability to think, process, and create original and beautiful poetry, literature, works of art, and music. In particular, music is a powerful art that is as core to the human experience as communicating. However, recently, artificial intelligence has increasingly been making headway into some of the more creative pursuits, and music in particular. While currently humans are only capable to create music from scratch, it’s becoming increasingly clear that AI is already serving as an augmented intelligent assistant. But in the near future, AI-powered systems might get enough power that they can create and perform entire compositions on their own. In a recent AI Today podcast, YouTube celebrity Taryn Southern talked about her new album “I AM AI” which uses Artificial Intelligence to generate music and melodies for the entire album. (Disclosure: I am a principal analyst at Cognilytica and co-host of the AI Today podcast) I AM AI was produced entirely by AI, and it’s incredible to think how AI can augment humans to create entire albums without the need for backing from record labels, large production teams, or large budgets. AI powered music tools For casual musicians who are interested in creating music, it’s becoming surprisingly easy to use AI-based systems to compose and perform music with very little human intervention. With the help of various AI music programs, users are able to generate, transpose, edit, and produce creative works. Taryn Southern used some of these AI programs to help create her new album. One of these tools was Amper Music, which helps people create and customize original music. The software is easy and good for beginners who want to make their own AI-based music without the need to learn an instrument or hire backup musicians. Casual musician users are able to pick a mood or genre which is used to create custom music based on what it has learned from existing patterns of music. 
 In the interview, Taryn explains that using Amper is not that different than actually writing a song. She starts with inspiration and uses that to help guide her musical crafting experience. In addition to Amper, Taryn used tools provided by the IBM Watson suite. In the podcast, Taryn explains that despite some technical and marketing-driven challenges with using the IBM Watson suite, the system has proven capable using its neural networks to collect various music data and create samples of the data, learning key patterns. Users can pick inspiration for new songs, plug them in to the Watson suite and the AI will make new iterations of music. In addition, this AI-based tool suite allows fellow musicians to combine favorite melodies and clips to create an original song of their own. Taryn also used the Google Magenta tool, an open-source project created by Google that transposes music into certain genres based on the examples given to it. Magenta explores the role of machine learning in the creative process and helps musicians compose music. This made it a great tool for creating simple music by giving her the power to combine the base elements of the song she wanted to create.  In the podcast, Taryn also mentions using a fourth tool, called Aiva, which is a creative assistant for creative people. Aiva is an AI virtual artist, that transposes and creates themes for certain forms or genres of music, and then combines them into a piece or a song, or in Taryn’s case, an album. Aiva is highly trained in classical music. With the help of this tool, Taryn was able to experiment with different sounds combining a classical romantic period piece, with a classic pop song. The end result has a unique sound, and Taryn compared it to something you would hear in a (HBO) Westworld soundtrack.  Augmenting human capability Because Taryn is not a professionally trained musician, she relied heavily on the AI tools to help create her album, using one or more tool for every song. This allowed for each song to be unique but still sound professional. The openness of these programs gives people with no access to creative arts a chance to express themselves. These tools could create new genres of music and sounds that humanity hasn’t even contemplated. The use of AI and machine learning to create music is on a path from fringe to mainstream application.  In addition, as these tools learn from users and more advanced musicians, it will start to identify patterns that produce successful music and potential hits in the music market. At that point, these systems might create entire albums and musical productions without human assistance of any sort. Popular music has already explored manufactured music, from K-Pop to Boy Bands and has figured out many of the elements of what makes popular music tick. With the addition of AI will humans even be needed at all for music creation? Furthermore, as more people use AI tools to augment their natural ability, you need to ask if these systems are exhibiting real creativity or simply mimicking the creative ability of humans? And who owns the rights to these AI enhanced songs? Certainly the future is interesting for AI and music, and the overlap especially so.   Kathleen Walch is Managing Partner & Principal Analyst at AI Focused Research and Advisory firm Cognilytica (http://cognilytica.com), a leading analyst firm focused on… Kathleen Walch is Managing Partner & Principal Analyst at AI Focused Research and Advisory firm Cognilytica (http://cognilytica.com), a leading analyst firm focused on Kathleen Walch is Managing Partner & Principal Analyst at AI Focused Research and Advisory firm Cognilytica (http://cognilytica.com), a leading analyst firm focused on application and use of artificial intelligence (AI) in both the public and private sectors. She is also co-host of the popular AI Today podcast, a top AI related podcast that highlights various AI use cases for both the public and private sector as well as interviews guest experts on AI related topics. Kathleen Walch is Managing Partner & Principal Analyst at AI Focused Research and Advisory firm Cognilytica (http://cognilytica.com), a leading analyst firm focused on application and use of artificial intelligence (AI) in both the public and private sectors. She is also co-host of the popular AI Today podcast, a top AI related podcast that highlights various AI use cases for both the public and private sector as well as interviews guest experts on AI related topics.",0.16015225481952106,0.5339216419072536
36,https://news.google.com/articles/CBMiYGh0dHBzOi8vYW5hbHl0aWNzaW5kaWFtYWcuY29tLzctb25saW5lLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLXRvb2xzLXRvLWdlbmVyYXRlLXlvdXItb3duLW11c2ljL9IBAA?hl=en-US&gl=US&ceid=US%3Aen,"   The history of music is more than just creativity, it’s about technological innovation as well. From the invention of musical instruments to augmenting it with electric in the 1950s, to the advent of synthesizers and electronic music – with each innovation, the style and pallet that music offered also expanded. AI is one such big innovation coming music’s way. AI generated music, though a relatively newer tech, has gained enough prominence. Surprisingly, the music generated by computers is also getting pretty good and remember that we are in the early days of this innovation. Here are 7 online tools available for anyone to play with AI generated music. AIVA is an AI-powered online tool that lets you create soundtrack and theme music. There’s a decent number of features provided to the users and the output is actually very decent. Their preset algorithms allow composing music in pre-defined styles like Cinematic, pop, rock, etc. There’s a free version of the tool that actually lets you create unlimited tracks and have some fun out of it, though restricted to just 3 downloads per month. Here’s what their website says about AIVA, “She has been learning the art of music composition by reading through a large collection of music partitions, written by the greatest Composers (Mozart, Beethoven, Bach, …) to create a mathematical model representation of what music is. This model is then used by Aiva to write completely unique music.” Another hi profile project by Elon Musk’s AI research organization OpenAI, Musenet is an interesting online tool that lets you combine different styles of music.  For eg, combine Chopin with the style of lady gaga. According to the website, “MuseNet was not explicitly programmed with our understanding of music, but instead discovered patterns of harmony, rhythm, and style by learning to predict the next token in hundreds of thousands of MIDI files.” Magenta is an open source, Tensorflow based project by Google that lets you train a model on music and images and finally generate new model out of it. What makes this great is that besides being open source, it also provides for pre-trained models. So for eg: Magenta Studio is a plugin to the popular DAW for musicians called Ableton Live. The music creators that import this tool and let AI do some of its magic. The plugin consists of 5 tools, each with its own utility: Continue, Groove, Generate, Drumify, and Interpolate. What’s more, the tools are also available standalone for download. We tested the tools ourselves on Ableton Live and have decent feedback on it (more on it in next one). Though still in early stages, the company plans to do a multitude of innovation using AI in music. For eg: there is a plan to create a piece of workout music that syncs with your workout rhythm, or an online radio channel that users can tweak according to their style and mood. Humtap is a mobile app that lets anyone create music on runtime on the phone. You can just hum or tap to create a melody or beat in runtime and in different styles and instruments. This one is a little different. Rather than create music, this tool master the tracks to sound like a pro and uses AI/ machine learning for exactly that. So, the tool would look at the production elements of the composer, match it with a large number of album grade music it is fed and then create output filters that are musically pro. Flow Machines is a project by Sony Computer Science Labs in Paris and received funding from the European research council. According to their “Flow Machines Professional is an AI assisted music composing system. Using this system, creators can compose melody in many different styles which he wants to achieve, based on its own music rules created by various music analysis. Creators can generate melody, chord, and base by operating Flow Machines. Furthermore, their own ideas inspired by Flow Machines. From here, the process will be the same as regular music production.”  comments Bhasker is a Data Science evangelist and practitioner with proven record of thought leadership and incubating analytics practices for various organizations. With over 16 years of experience in the area of Business Analytics, he is well recognized as an expert within the industry. Earlier, Bhasker worked as Vice President at Goldman Sachs. 

He is B.Tech from Indian Institute of Technology, Varanasi and MBA from Indian Institute of Management, Lucknow.                   Subscribe now to receive in-depth stories on AI & Machine Learning.  Copyright 2020 Analytics India Magazine Pvt Ltd. All RIGHTS RESERVED. ",0.2045860389610389,0.4835601898101899
37,https://news.google.com/articles/CBMiQ2h0dHBzOi8vd3d3LnRlY2hyYWRhci5jb20vbmV3cy9hcmUtYWktY29tcG9zZXJzLXRoZS1mdXR1cmUtb2YtbXVzaWPSAQA?hl=en-US&gl=US&ceid=US%3Aen,"
By
Cat Ellis
01 March 2019
 Wired for sound Ecrett Music is an AI-powered tool that generates unique music soundtracks for videos. Its creations are royalty free and sound surprisingly natural, and access to the tool costs just a few dollars a month. I believe that AI will help human musicians, not put them out of work It's a boon for YouTubers, but it's also slightly unsettling – art is supposed to be the realm where human ingenuity wins out over machine efficiency. Could Ecrett Music, through its affordability and convenience, put composers of incidental music out of work? “I believe that AI will help human musicians, not put them out of work,” says Ecrett Music's founder, Daigo Kusunoki. “We don’t have to be afraid of AI taking our job, what’s important is to figure out how to take advantage of AI’s capabilities and use them to enhance our own creativity."" It's a valid point; AIs have been lending musicians a hand since 1995, when David Bowie used an application called The Verbasizer to pull sentence fragments from stories and articles, and slam them together to create unexpected lyrics. Kusonoki has loved music since he was a child. He took up guitar in junior high school, and began dancing at university. He also enjoys creating things, and says his dream has always been to combine those two passions by creating something related to music.   His first foray into music tech was the SoundMoovz wearable. “Users can wear it on wrist or ankle, assign sounds through the app, and create a series of sounds by moving,” he explains. From there, he began to consider other ways non-musicians could create their own songs. “Since SoundMoovz was rather like an instrument gadget, I wanted to develop a tool that allows everyone to compose music intuitively,” he says.   Ecrett Music works entirely in your web browser. Just visit the site, select the type of content (options include workout, party, travel, relaxing, cooking and fashion), pick a mood (happy, chill, sad, exciting or serious) and choose how long you want the piece to be. You can also upload your video to preview how the music and visuals will work together. The system will then generate a clip for you. If you like, you can edit the structure of the composition to better fit your video, and change the bass, drums, melody and backing. It’s all very simple, and requires no knowledge of music theory. When you’re happy with the result, click ‘Download’, create an account, and choose which membership tier suits you best. There are two membership tiers available, both of which are fine for commercial use. Standard membership costs $5 (about £4, AU$7) per month, and offers unlimited creation and downloads for teams of up to nine people. For businesses with 10 people or more, there’s the Pro plan, which comes in at $9.99 (about £8, AU$14) per month. There’s a small discount if you choose to pay annually rather than monthly. All tracks generated by Ecrett are royalty free, so there’s no charge for using them in personal or commercial work, and no credits are required (though a link to the site is appreciated). The only thing you’re not allowed to do it resell, sublicense, or redistribute the music yourself. Kusonoki says early reactions to Ecrett Music (which came out of beta last week) have been overwhelmingly positive, with filmmakers praising its speed and convenience. There is no concrete definition of what makes the music emotional and human-like However, early users have also requested extra 'scene' and 'mood' options – which will only be possible with extra work from the human creatives.  Any AI application is only as good as the data it's given to work with. In the case of Ecrett, this was provided by a team of six people, including a director, software and AI engineers, designers and composers. “One of the biggest challenges so far was to improve the AI technology because there is no concrete definition of what makes the music emotional and human-like. We are still trying various methods to improve the AI,"" Kusonoki says. While AI-powered composers can be useful tools, it doesn't look like Ecrett will be putting human composers of incidental music out of a job any time soon. Get the best tech deals, reviews, product advice, competitions, unmissable tech news and more! Thank you for signing up to TechRadar. You will receive a verification email shortly. There was a problem. Please refesh the page and try again TechRadar is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site. ©
Future US, Inc. 11 West 42nd Street, 15th Floor,
New York,
NY 10036. ",0.15112287256122878,0.4619308545335943
38,https://news.google.com/articles/CBMiV2h0dHA6Ly93d3cuZnQubGsvY29sdW1ucy9Kb3RoaXBhbGEtRnJvbS10aGUtZGVwdGhzLW9mLWRlc3BhaXItdG8taGVhZHktaGVpZ2h0cy80LTY5NzQ4MtIBAA?hl=en-US&gl=US&ceid=US%3Aen,"Saturday Mar 14, 2020  Saturday Mar 14, 2020      Comments  / 

{{hitsCtrl.values.hits}}  Views   / Saturday, 14 March 2020 00:01 Sinhala cinema’s super star Gamini Fonseka once described H.R. Jothipala as a playback singer “who sang to joy, emotion and grief, the three essential areas in film songs”. In an excerpted interview published in these columns a fortnight ago, Gamini stated forthrightly: “Of all singers, H.R. Jothipala is exclusive. He has come to stay. As long as the island – Sri Lanka – exists in the world map, Jothipala will never die.”
The full impact of Gamini Fonseka’s assessment about the immortality of Jothipala the singer and his songs hit me over the past few weeks after I had written the first part of the article on Jothi for ‘Spotlight’. 
During the first week, I received several mails from people thanking me for the article. Some said they were looking forward to the second part. When the article did not appear last week, I got a second lot of mails querying as to why the second part did not appear. Several people requested that the article on Jothi should be continued.
These mails relating to a singer who had passed away almost 34 years ago made me realise the unassailable truth and wisdom of Gamini Fonseka’s words. If this was the response to an article in English about H.R. Jothipala, what would be the response to a Sinhala article on Jothi, I wondered. A loveable personality As stated in the first part of this article HR Jothipala had a loveable personality. What endeared Jothi to most persons who interacted with him was his unpretentious simplicity, easy accessibility, good-natured friendliness and a joyful lifestyle. He was not of the privileged classes. He rose up from humble beginnings and reached stardom as a singer through perseverance and talent. H.R. Jothipala’s rise to fame is an impressive tale worth recounting.
Hettiarachchige Reginald Jothipala was born on 12 February 1936 at Ketawalamulla in Dematagoda. His father Hettiarachchige Reginald James was a tailor by profession. His mother H.K. Podinona Perera was employed in a hospital. Both his parents hailed from Matara in the Southern Province. They sought a new life by moving to Colombo after marriage. 
Jothipala was the eldest child in a family of four girls and two boys. Jothi studied at the newly established St. John’s College in Dematagoda and at St. Lawrence College in Maradana. After leaving school, he enrolled at a vocational training institute in Maradana to learn welding and machine operating.
Jothipala’s efforts to acquire academic learning or vocational training never succeeded because he was far more interested in music, song and singing than in obtaining academic certificates or technical skills. Some of his erstwhile schoolmates have revealed in media interviews that Jothi – as he was known since childhood days – was always keen to sing and entertain his friends with songs. He would do so by drumming on tables or “playing drums” on empty tins and cans. 
The family was not well-off financially and could not afford a radio at home. So young Jothi would loiter around a tea kiosk that had a radio blaring forth perpetually. Jothi used to sing along at times when familiar songs were aired. He was rather fond of Hindi songs. Talat Mahmood and Mohammed Rafi were his favourite singers then. 
Jothipala, who never learnt music in school or underwent formal training as a singer, was a “natural” who could play it by ear. His goal in life was to become a popular singer. He sang wherever and whenever he got a chance – at parties, weddings, stage shows and processions. Stanley Omar 
to the rescue Jothipala faced an uphill struggle in trying to break through into the musical world as an accredited singer. In spite of his lack of training as a singer, he did succeed ultimately due to two factors. One was his sheer talent. The other was the help of good souls who were impressed by that talent. The first among those who discovered Jothi’s talent and helped him in this regard was the well-known musician of Malay heritage, “Kalasuri” Stanley Omar.
In those days several people interested and involved in music and singing used to gather at the Gunaratne Hotel in Maradana. It was there that Omar came across Jothi. It was Omar who introduced the 16-year-old Jothipala to well-known journalist and radio personality Ariyadasa Peiris in 1952. Ariyadasa Peiris pioneered several radio shows on ‘Radio Ceylon’ aimed at fostering the arts. One of these was ‘Adhunika Peya’ that encouraged singing by amateurs. It was sponsored by a confectionery.
Ariyadasa Peiris obliged Stanley Omar by letting Jothipala participate in ‘Adhunika Peya’. The Hindi film ‘Aaram’ starring Dev Anand and Madhubala had been released in 1951. The film’s songs for which music was composed by Anil Biswas had become widely popular. Jothipala sang a number from this film ‘Shukriyaa, Shukriya Ai Pyaar Tera, Shukriya’. Jothipala’s favourite singer, the legendary Talat Mahmood, had sung it for the film. It was known as the “Shukriya (Thank You) song”. 
Jothipala won first place on that occasion and got his first-ever singing ‘prize’. It was a box of honey-flavoured lozenges deemed good for the throat. Subsequently Jothipala won the Best Singer award in another show, ‘Jayagrahaka Pelapaliya,’ too. This was also hosted by Ariyadasa Peiris. The prize this time was a Parker 51 pen. 
It was once again Stanley Omar who went the extra mile to promote Jothipala’s singing among reputed musicians and singers on a festive occasion. The well-known singer G.S.B. Rani got married to Ranawaka Arachchige Anton Perera in February 1953. Though known professionally as G.S.B. Rani, the singer’s initials represented her real name Gnai Seenar Bangsajayah. 
G.S.B. Rani Perera who later served as a Director of ITV was of Malay ethnicity. The journalist Sisira Kumara Manikkarachchi organised a musical event at Nugegoda to felicitate the newlywed couple. The music was conducted by R.A. Chandrasena. Stanley Omar too was an invitee.
Stanley Omar took along with him the uninvited Jothipala. He introduced him to Chandrasena and his wife the singer Sriyani and asked the music composer to give the young lad a chance to sing. This was done. Jothipala sang and captivated the audience. He was asked to sing again and again. The event provided Jothi a tremendous boost.
One of the persons impressed by Jothi was the singer-actor Maurice Dahanayake. Due to the efforts of Maurice Dahanayake, Jothipala got a chance to voice one word three times in a song. The song was ‘Mahaweli Nadiye’ sung by Wasantha Sandanayake. All that Jothipala had to do was shout out the word “Thotiyo” thrice. This Jothi did and was elated. His voice was on a record for the first time. 
Later on Jothipala got more chances to really sing for records as opposed to shouting “Thotiyo”. He sang his first duet ‘Labeiee Sithalada Ale Kale’ with G.S.B. Rani. His first duet with a male singer was again with Wasantha Sandanayake. This was ‘Ada Ada Eyiee Maruwa,’ where Jothi shared a full song instead of interjecting three words. Jothipala’s first solo was ‘Mage Ran Ranjanee’. This was made possible by his benefactor and friend Stanley Omar who composed the music. The Chandrasenas step in Meanwhile Jothipala’s singing at the Anton Perera-G.S.B. Rani felicitation event had greatly impressed maestro R.A. Chandrasena and spouse Sriyani Chandrasena. Ranasinghe Aarachchige Chandrasena known as “RAC” and “Chandrasena Master” founded the “Chandrasena Sangeethaayathanaya” or Chandrasena Institute of Music in 1951. He was a musician, singer, music composer and music teacher. Chandrasena married the singer Shriyaawathie Perera who later became known as Sriyani Chandrasena.
The Chandrasenas liked Jothipala’s singing and took the aspiring artiste under their wing. Chandrasena included Jothipala as a singer in his popular musical extravaganzas like ‘Chathurangani’ and ‘Panchangani’. More importantly it was Chandrasena who was instrumental in giving Jothipala the first opportunity to be a playback singer in films. This however did not materialise due to an entirely tragi-comic reason.
B.A.W. Jayamanne, (Eddie Jayamanne’s brother) of Negombo’s Minerva Theatre Group fame, produced, directed, wrote screenplays and acted in many Sinhala films in the late forties and fifties of the 20th century. The Jayamannes were integral to the making of the first Sinhala talkie ‘Kadawuna Poronduwa’ in 1947. B.A.W. was regarded as a pioneer of Sinhala cinema though many of his creations were heavily influenced by Indian movies.
R.A. Chandrasena along with the South Indian music director S.S. Vedha had composed music for B.A.W.’s film ‘Iranganie’ in 1954. Jayamanne embarked on producing another film, ‘Mathabhedaya’ in 1955. B.A.W. wanted R.A. Chandrasena to be in charge of music for this film. The film – as in the case of most Sinhala films then – was to be made in India. Wanting to give Jothipala his first break in playback singing, Chandrasena persuaded Jayamanne to let the budding songster sing one song in the movie. B.A.W. agreed.
Thereafter Chandrasena organised a series of rehearsals for Jothipala at his residence. The song Jothipala was to sing for ‘Mathabhedaya’ was ‘Ginnaki Hada Mage’. The original tune was from the philosophical Hindi film song ‘Zindagi Dene Wale Sun’ from the film ‘Dil-E-Nadan’ released in 1953. It was sung by the great Talat Mahmood – Jothipala’s favourite singer – who also acted in the film. 
The song was picturised on Talat and actress Shyama. The music composer was Ghulam Mohammed. The musical arrangement by Ghulam was rather exquisite. Western string instrumental music blended with a melody based on the Hindustani raga ‘Bhoop’. Therefore Chandrasena wanted Jothi to rehearse diligently.
This Jothipala did with great enthusiasm. Jothi was delighted that he was going to sing his first number as a playback singer. Furthermore it was going to be a melody sung by Talat Mahmood whom he idolised. He arduously practiced under Chandrasena’s strict guidance. Finally the maestro was satisfied. Since most Sinhala films were made in India in those days, the songs too were to be recorded in an Indian studio. So a studio in Madras (now Chennai) was booked and dates fixed. Jothipala was instructed to be ready to fly out to India on a specific date and time. Passport fiasco An excited Jothipala made preparations. In those days flights between Colombo and Chennai were from Ratmalana to Meenambaakkam. Katunayake was yet a British air base. Jothipala made his way to Ratmalana on the specified day and time with his baggage. When Jothi arrived at the Ratmalana airport, he was in for a shock. Jothipala discovered that he had neither a passport nor visa to travel to India. The naïve, childlike Jothi had not realised that such documents were required to travel abroad. He had not even bothered to inquire from anyone in this regard. Chandrasena was furious. He had never imagined Jothipala would be so ignorant about such matters. So Chandrasena too had not inquired from Jothi earlier. Now it was too late. The others proceeded as planned to India without Jothi.
‘Mathabhedaya’ was released on 14 April 1955 to coincide with the Sinhala-Tamil New Year. The song ‘Ginnaki Hada Mage’ was sung by Mohideen Baig for the film. Jothipala was shattered. He wrote a poignant letter to R.A. Chandrasena whom he addressed as his friend and teacher.
Jothipala’s passport fiasco story began circulating in the world of cinema and music. Though amused, many felt sorry for him. Jothi’s friends and well-wishers tried hard to get him another chance to sing for films. 
After the release of ‘Mathabhedaya,’ B.A.W. Jayamanne began producing another film, ‘Perakadoru Bena’. The director was Antony Baskar Raj known as A.B. Raj. He was an Indian national who lived in Sri Lanka for several years and directed over 12 Sinhala films. A.B. Raj is the father of award-winning Tamil actress Saranya Ponvannan. 
The music for the film was composed by S.S. Vedha, the well-known Tamil music composer who used to work in many films produced by B.A.W. Jayamanne. The musical orchestra was conducted by B.S. Perera. Jothipala approached B.A.W. Jayamanne along with B.S. Perera. B.A.W. was somewhat sympathetic to Jothi’s plight. The producer told B.S. Perera to give Jothi a song in the film with the director A.B. Raj’s approval.
One of the songs planned for the film was the duet ‘Muhudey Pathuley’. B.S. Perera felt Jothi could be the male singer in the duet with Rukmani Devi. When Jothi was taken to the A.B. Raj, the director asked Jothi to sing. After listening to Jothi twice, A.B. Raj bluntly rejected him saying the voice was unsuitable for films. A crestfallen Jothipala returned home. ‘Perakadoru Bena’ was released on 14 October 1955. Mohideen Baig sang ‘Muhudey Pathuley’ instead of Jothipala. More bad news Jothi’s friends continued with their efforts to promote him. The reputed clarinetist and music composer T.F. Latheef was a good friend of Jothi. Latheef was assigned the task of music direction for the film ‘Podi Putha’ by Sirisena Wimalaweera. The reputed film maker who set up the film studio ‘Navajeewana’ in Kiribathgoda had directed films such as ‘Saradiel’ and ‘Asoka’. Now Wimalaweera was producing, directing and acting the key role in ‘Podi Putha’. Latheef wanted to give his friend Jothipala a chance to sing for ‘Podi Putha’.
Among the songs planned by Latheef for the film was ‘Kiri Muhuda’ based on the film duet ‘Dekho Mane Nahin Roothi Hasina’ from the Hindi film ‘Taxi Driver’ starring Dev Anand. The singers were Jagmohan Bakshi and Asha Bhosle. Music was composed by S.D. Burman. Latheef decided to let G.S.B. Rani Perera and H.R. Jothipala sing the ‘Kiri Muhuda’ duet. Wimalaweera too was agreeable. So Jothipala went to Nawajeewana studio and recorded the song with Rani Perera. He was very happy and eagerly awaited the release of ‘Podi Putha’.
Sirisena Wimalaweera’s award-winning ‘Podi Putha’ was screened to the public on 25 November 1955. Jothipala was on cloud nine. Jothi went with a group of friends to see the matinee show on the first day. They waited for ‘Kiri Muhuda’ song to play on screen. When it did, Jothipala was shocked. It was not Jothi’s voice that was heard. It was another singer, Haroon Lantra, who sang along with G.S.B. Rani Perera. Jothi and friends were thoroughly dismayed. Just to make sure, Jothi and a friend watched the next show also to see the song sequence again. All doubts vanished. Haroon Lantra and GSB Rani Perera were the singers. A saddened Jothi and his friend left the theatre.
Jothipala made inquiries from T.F. Latheef. The apologetic music director told Jothi what had happened. A few weeks before the release the Indian sound engineer at the studio had told Wimalaweera that Jothipala’s voice was of poor standard. He had insisted that Jothipala be replaced. Wimalweera too concurred. Latheef protested but he was overruled by the producer-director who agreed with the sound engineer. So Haroon Lantra and Rani Perera were summoned at short notice and the song was re-recorded again. Latheef had not told Jothi of this because he could not face his friend to break the bad news. Destiny intervenes Nineteen-year-old Jothipala was devastated. Years later Jothi was to disclose in a media interview that the ‘Kiri Muhuda’ episode was one of the lowest points in his life. He had even contemplated suicide. He felt humiliated and did not venture outside home. He avoided friends for a while. Jothi recovered after some time and resumed singing. How could a songbird refrain from singing? But Jothi decided that he would never be a playback singer. He stopped trying for a chance to sing in films again and told his friends also to stop.
For several months Jothipala remained firm in his resolve of not singing in films. Destiny however intervened in the form of his friend and benefactor Stanley Omar. The man who had helped in so many ways to launch Jothipala’s career acted without Jothi’s knowledge to boost his friend’s career.
Utilising his influence and contacts Stanley Omar enabled Jothipala to obtain another chance to sing for a film. After much persuasion, Jothi grasped the opportunity. H.R. Jothipala became a playback singer in 1956 by singing for ‘Surathalee’. From then onwards, there was no looking back. 
How this state of affairs came about and the astounding ascendancy of Jothipala as a playback singer will be related in detail in the third and final part of this article. (D.B.S. Jeyaraj can be reached at dbsjeyaraj@yahoo.com.)  Share This Article

 Facebook

 Twitter


 1. All comments will be moderated by the Daily FT Web Editor. 2. Comments that are abusive, obscene, incendiary, defamatory or irrelevant will not be published. 3. We may remove hyperlinks within comments. 4. Kindly use a genuine email ID and provide your name. 5. Spamming the comments section under different user names may result in being blacklisted.  Travel / Tourism IT / Telecom / Tech Financial Services Business News  Saturday, 14 March 2020  Sinhala cinema’s super star Gamini Fonseka once described H.R. Jothipala as a playback singer “who sang to joy, emotion and grief, the three essential areas in film songs”. In an excerpted interview published in these columns a fortnight ago, G   Saturday, 14 March 2020  The world is grappling with the big as they have ignored the small. What about kind, merciful, compassionate and altruistic thoughts to douse flames of bitterness, hatred, greed, pride and vengeance, in this wild cat race that has resulted in inequal   Friday, 13 March 2020  Sri Lanka is at a crossroads with its economy registering 2.7% GDP growth last year and being ranked the poorest performing economy in South Asia. We are now confronted with the COVID-19 outbreak, which will affect the life of every Sri Lankan while    Friday, 13 March 2020  The forthcoming Parliamentary Election has become the main concern nowadays of not only educated people but also the whole of society. Yet it is rather unfortunate that they do not see the streaks of dark shadows of anarchy splash beneath the carniva   Columnists More    
(adsbygoogle = window.adsbygoogle || []).push({});
 Jothipala: From the depths of despair to heady heights  Channel practice: Sequence observed in the breach  Kachchatheevu: A feast to connect two nations  Accountant sentenced to 37 years RI for stealing Rs. 12 m but worse offenders go scot-free   OPINION & ISSUES MORE  Uber Eats to empower 1,000 Sri Lankan women in 2020  Uber Eats’ Intl. Women’s Day resolution to break barriers and enrol more women in earning cycle  Demystifying the latest tax reforms  Forum on creating shared value presents management take outs to nation   SPECIAL REPORT MORE  Time to act  COVID-19 impact   Changing  labour laws  Stop the ragging monster   FT VIEW - Editorial MORE  Agriculture 

Business 
Lifestyle 
CSR / Events 
Dining 
Entertainment / Art 
Entrepreneurship 
 Fashion
Financial Services 
Energy 
Front page 
Healthcare 
HR 
In Depth 
International 
  IT / Telecom / Tech
Leadership 
Leisure 
Letters to the Editor 
Management 
Marketing 
Motor 
News 
 Other Sectors 
Shipping / Aviation 
Special Report 
Technology
Travel / Tourism 
Youth / Careers / Higher Education 
  General : +94 0112 436 998, +94 0112 479 780 
        Fax : +94 0112 447 848
        Circulation : +94 0112 479 626, +94 0112 479 628
        Advertising : +94 0112 479 540, +94 0112 479 555
		Technical : helpdesk@wijeya.lk , +94 0112 479 437   +94 77 372 7288  - Dilan 
        +94 77 567 1710 -  Charith
  Group Sites : Lankadeepa Dailymirror Sunday Times Daily FT Ada Deshaya Tamil Mirror Mirror Sports HI TV Kelimandala Wisden Sri Lanka  Life   LW  Mirrorcitizen    Hitad Print   HI Magz   Times Jobs   Times Education   Wijeya   Tamil Radio   WNL Home   e-papers :  Lankadeepa   Sunday Lankadeepa   Daily Mirror   Ada   Tamil Mirror   Daily FT   Deshaya  Wijeya e-paper portal  Services : 

 Home delivery
Webmaster 
Web Ads
Editorial 
Help Desk
News Alerts 
Book Print Ads 
 All the content on this website is copyright protected and can be reproduced only by giving the due courtesy to 'ft.lk'
        Copyright © 2004 Wijeya Newspapers Ltd.",0.16022674736266967,0.42105363622353936
39,https://news.google.com/articles/CBMiSWh0dHBzOi8vZnV0dXJpc20uY29tL2EtbmV3LWFpLWNhbi13cml0ZS1tdXNpYy1hcy13ZWxsLWFzLWEtaHVtYW4tY29tcG9zZXLSAU1odHRwczovL2Z1dHVyaXNtLmNvbS9hLW5ldy1haS1jYW4td3JpdGUtbXVzaWMtYXMtd2VsbC1hcy1hLWh1bWFuLWNvbXBvc2VyL2FtcA?hl=en-US&gl=US&ceid=US%3Aen,"Artificial intelligence (AI) is set to become the most groundbreaking and defining technology of the 21st century. But what exactly comes to mind when people think about AI? Programs that can beat you at poker? Robots that can perform complex tasks with perfection? Or perhaps autonomous beings that are on the verge of replacing your job? When we think about AI, we often look at those areas where humans can easily be replaced: high-level computation, manual labour, or data-driven optimization. Yet, there is now a new wave of emerging potential for AI in creative industries – one of which happens to be musical composition. Aiva Technologies is one of the leading startups in the field of AI music composition. It was founded just last year in Luxembourg and London by Pierre Barreau, Denis Shtefan, Arnaud Decker, and Vincent Barreau. They have created an AI called “Aiva” (Artificial Intelligence Virtual Artist) and taught it how to compose classical music – an emotional art which is usually considered to be a uniquely human quality. Aiva’s musical pieces are used as soundtracks for film directors, advertising agencies, and even game studios. This February, they were invited to participate in the highly-acclaimed European Film Market in Berlin, as well as the Artificial Intelligence in Business & Entrepreneurship (AIBE) Summit in London. In the past, they were backed by Luxinnovation’s incubator program, and have even received praise from Xavier Bettel, the Prime Minister of Luxembourg himself. Having already released its first album called Genesis, as well as many single tracks, Aiva recently became the first AI ever to officially acquire the worldwide status of Composer. It was registered under the France and Luxembourg authors’ right society (SACEM), where all of its works reside with a copyright to its own name. Give Aiva’s music a listen and see what you think:  The technology behind Aiva is based on deep learning algorithms which use reinforcement learning techniques. Deep learning is a particular type of machine learning whereby multiple layers of “neural networks” are programmed to process information between various input and output points. Although only loosely based on the human brain’s neural structure, it helps to think of it that way. This allows the AI to understand and model high-level abstractions in data, such as the patterns in a melody or the features in a person’s face. Reinforcement learning, on the other hand, is a machine learning technique which teaches a software agent (AI) to decide what action to take next in order to reach certain objectives by maximizing its “cumulative reward.” Unlike supervised learning, reinforcement learning does not require labelled inputs and outputs of data. This allows the AI to “find its own way” around the data and improve its performance without being given any explicit instructions, which makes it easier to capture the diversity and variation found in creative arts like music. According to the team: “We have taught a deep neural network to understand the art of music composition by reading through a large database of classical partitions written by the most famous composers (Bach, Beethoven, Mozart, etc). Aiva is capable of capturing concepts of music theory just by doing this acquisition of existing musical works.” After having listened to a large amount of music and learned its own models of music theory, Aiva composes its very own sheet music. These partitions are then played by professional artists on real instruments in a recording studio, achieving the best sound quality possible. Although Aiva is able to compose classical melodies in a matter of minutes, its clients are usually looking for music which “supports the storytelling of their visual content.” To achieve that, it can sometimes take them several iterations before the right sound is generated. When asked why they chose to focus on classical music, the founders of Aiva Technologies explain: “(1) it is the predominant style used in movies, games, commercials, and trailer soundtracks, and (2) all of the partitions we use to train Aiva are copyright-expired”. Although the music Aiva listens to and learns from is indeed copyright-free, its own compositions are not in the public domain as they are registered under SACEM. In the future, the team plans to teach its AI how to learn any style of music. The interesting challenge presented by modern music is not with the composition itself, but the instrumentation and sound design. For example, the most iconic bands have a distinct sound that makes them so unique. Trying to create those unique sounds that makes their music stand out with an AI will be the next problem to crack in order to reach human-level performance. Will AI-composed music ever be indistinguishable from the work of human musicians? Well, according to the team, they have already conducted several Turing tests by asking professionals to listen to Aiva’s pieces – and so far none of them were able to tell that they were composed by an AI. However, there is no need to worry just yet. Aiva’s compositions still require human input with regards to orchestration and musical production. In fact, Aiva’s creators envisage a future where man and machine will collaborate to fulfill their creative potential, rather than replace one another.",0.14341441891441895,0.4375525400525401
40,https://news.google.com/articles/CBMieWh0dHBzOi8vd3d3Lm5wci5vcmcvMjAxOS8wNS8wNy83MjExNzIxODYvcGlhbmlzdC1hbmQtY29kZXItZGFuLXRlcGZlci1jb21wb3Nlcy1tdXNpYy13aXRoLXRoZS1oZWxwLW9mLWFydGlmaWNpYWwtaW50ZWxsaWfSAQA?hl=en-US&gl=US&ceid=US%3Aen,"

      Ailsa Chang
    
 

      Noah Caldwell
    
 
                Dan Tepfer's latest album, Natural Machines, is out now.
                
                
                    
                    Judy Natal/Courtesy of the artist
                    
                
hide caption
 Dan Tepfer's latest album, Natural Machines, is out now. When musician Dan Tepfer was a kid, he taught himself to code on an early Macintosh computer that his dad brought home one day. ""What I love about programming is it's so powerful,"" Tepfer says. ""You can be a little kid, with very little power over the world, and you can tell this very powerful machine to do whatever you want it to do, and it will do it for you, flawlessly, over and over again."" That little kid grew up to be a world-renowned jazz pianist. And now, he has programmed another machine: his piano. For the past five years, Tepfer has been writing algorithms that direct a specially modified Yamaha piano to play along with him. When he plays, the piano plays back according to the algorithm's rules, as if phantom hands were pressing the keys. Even though each algorithm is carefully calculated to produce a specific output, Tepfer says the piano's response still surprises him. ""There's stuff even rhythmically that I wouldn't expect,"" he says. ""I have to be listening to what the piano's doing in exactly the same way as I am listening to a human that I'm playing with.""  ""Tepfer's new album, called Natural Machines, is an exploration of this musical fusion of human and machine — a dynamic that he says has been at the heart of music for centuries. ""If you look at medieval composers like [Johannes] Ockeghem, Baroque classical composers like Bach, their music lives at the intersection of the algorithmic and the spiritual,"" Tepfer says. ""Equal parts rules, and equal parts intuition and, for lack of a better word, spirituality."" He also used data from the piano's keystrokes to depict the album's music with complex visualizations. At his live shows, audiences can wear virtual reality goggles to see this visual component in real time. Tepfer predicts that the use of artificial intelligence in musical production will only grow. ""I just can't imagine humans shying away from the possibilities that it brings. And I think over time, the allure of novelty wears off. And then we're left with something much deeper, which is simply how these new tools can open up new artistic horizons to creators."" NPR thanks our sponsors Become an NPR sponsor",0.09376980289214333,0.4168554849405913
41,https://news.google.com/articles/CAIiEGmex7sCD7ohggCtiNnjticqFwgEKg4IACoGCAowis8wMLmCBjCfkLEG?hl=en-US&gl=US&ceid=US%3Aen,"Image: Steve Buissinne/Pixabay  This November, the Prague Philharmonic will perform the third and final movement of “From the Future World,” an AI-completed composition based on an unfinished piano piece by the famous composer Antonín Dvořák, 115 years after his death. Emmanuel Villaume will conduct.  The concert is the culmination of an experiment in AI and creativity that got its start last year when Richard Stiebitz and Filip Humpl, creative directors at the Prague office of global ad agency Wunderman, approached Luxembourg-based AIVA Technologies, an AI-music startup.  “We wanted to see if you can use AI in the creative process in a positive way,” Stiebitz said. “Because everyone is afraid of AI, thinking AI will replace humans. But I am an optimist, and I think when people are clever enough and learn how to apply AI, it can be very helpful.”  AIVA, which stands for Artificial Intelligence Virtual Artist, has been composing ""emotional soundtracks"" for films, TV ads, and video games since 2016.  AIVA's challenge: to complete a symphony from an unfinished, unnamed two-page fragment of a Dvořák piano composition in E-minor. Pierre Barreau, CEO and co-founder of AIVA, said it was a “happy challenge” to take on.   “AI is often abstract, so this will help show what AI can do, as a response to a lot of fears around AI,"" he said. ""It's not the point to have humans out of the equation, but rather to have humans augmented and to collaborate with AI.”  Although this is not the first time that AI has finished the work of a composer, the November concert will be the first time such a composition has been performed on such a large scale and in a major concert venue. “So it's an experiment to see what happens, and maybe the discussion will be important as well. Now, classical music lovers are digesting that this classical music is coming from AI—and they are starting to consider that it is valid,” Stiebitz said.  Although the composition process for typical clients is usually under a minute because the request is based on an existing style of music supported by AIVA, the Dvořák composition took about 72 hours. This included the time needed to re-train AIVA on a database of 30,000 scores, as well asall 115 of Dvořak's opuses (including his famous 9th Symphony, “From the New World”) to craft a whole new style specifically for this project.  During the composition process, AIVA generated hundreds of different examples, from which Barreau's team selected the one with the most stylistic resemblance to Dvořák. All three movements were completed in December 2018.  While Barreau's company has a digital recording of the three movements played by virtual instruments, there is so far only one human recording of the first movement, performed in Prague by renowned Czech pianist Ivo Kahánek in April this year. The second movement will be performed (also by the Prague Philharmonic) during the Rock for People music festival in early July in the Czech town of Hradec. Recordings of the second and third movements are still in the works.    Everyone is an interpreter  Although performing an AI-generated composition may seem like a radical departure from the norm for members of the Prague Philharmonic, Stiebitz saw that the musicians remained the interpreters of the composer's music, whether that composer was human or not.   “The interpreter is saying 'It's my music, because I put the soul into it, and composers are just making the notes.' Of course I know the composers do more than that, but the interpreters are thinking that they are giving more to the music—and I witnessed this. I could compare the midi recording to the live version, which does have something more to it, and that is the role of humans,” Stiebitz said.   But here's the thing about music: it's not just the musicians who play the role of interpreter. The listeners are interpreters as well.   Stiebitz was surprised when the Prague Philharmonic manager told him she was deeply touched by the midi recording of AIVA's composition, especially the third movement. Another unexpected reaction he received came after playing AIVA's creation to students he teaches at the University of Economics in Prague. One student said he heard water bubbles in the music. This was after Stiebitz had told his class that Dvořák is said to have planned to compose another symphony after returning to Europe from America, with the inclusion of water themes as a reference to his oceanic voyage—and the unfinished piano fragment may have been an attempt at writing this new symphony.  “I said to him, if you hear it there, it is there, because it is about imagination and humans, and imagination is the most important thing. I was surprised, but everyone hears music in their own way. This is why AI music works. I– it must always have a human touch,” Stiebitz said.   The future of virtual artists  Because AIVA Technologies is comprised entirely of musicians, and the startup's clients are mostly composers, Barreau said it's important for his team to convey the message that their AI virtual artist won't replace anyone—humans must always be around to make the decisions about how the AI will work.  The humans who consider working with AIVA often come with initial doubts about the place of AI in creativity and wonder if AI should or will replace humans. However, Barreau said that after trying the product, the clients realize they are not a slave to AI.   “It's kind of the opposite—AI is assisting them with a specific need. AI can create ideas for you, and you decide which ideas you want to push forward,"" he said. ""AI can help those not trained in music to write better music. And I think it's a pretty powerful idea, because it's not a bad thing to be more creative, regardless of the tools you are using.""  However, AIVA has already become the world's first virtual artist officially registered as a composer with an author's rights organization, France and Luxembourg's SACEM. This means that AIVA’s creations are copyrighted, just like those of a human composer.  Vivienne Ming, an artificial intelligence expert and theoretical neuroscientist, said that “while distinct limitations to AI-generated art remain, AI can still be a powerful tool that greatly speeds the process for human artists and other creatives.” Current and near future artificial intelligence, like deep neural networks and similar technologies, is a tool that can be used “to explore unlikely or untested combinations...and extend works of human creativity.”   But Ming added that “there is no theoretical limit preventing artificial intelligence from some day being creative in the same sense of exploration and agency that human artists possess. For now, however, bringing meaning to the unknown is still a pretty human domain.” © 2020 VICE MEDIA LLC",0.12167870394911215,0.5062516565067585
42,https://news.google.com/articles/CBMiOmh0dHBzOi8vbWl4bWFnLm5ldC9yZWFkL2Jlcmxpbi1tdXNpYy10ZWNoLWNvbXBhbnktamFtLW5ld3PSATRodHRwczovL21peG1hZy5uZXQvYW1wL2Jlcmxpbi1tdXNpYy10ZWNoLWNvbXBhbnktamFt?hl=en-US&gl=US&ceid=US%3Aen,"Producing music’s never been more intelligent  Berlin music tech company JAM has created the first AI-powered platform to produce high-quality tracks using professional audio loops.  The AI’s neural network can produce a new music composition in just 10 seconds. It scans the vast JAM database of 100,000 pro audio loops before selecting and sequencing them into full tracks. In the “era of the AI composer”, according to JAM, the user is able to easily work alongside the AI via simple inputs to compose music across 32 different genres including house, techno, hip hop, trap, synthwave and electro.  Read this next: 5 of the best studio hacks JAM CEO Rory Kenny, says: “Based on JAM’s prior experience in successfully democratizing music for millions of young creators globally, we saw an exciting opportunity to develop an AI platform which could generate high quality ‘instant music’ based on a person’s command.  “In parallel, as we see music production increasingly move into an intelligent cloud, we predict there will be a huge impact on the music industry on the same scale as the shift from analogue to digital.”  Read this next: 5 of the best FM synths The unique combination of their library of 3.5 million music tracks and 500 million associated data points has enabled the rapid advancements in JAM’s AI platform. Results will start showing on a new interactive website this month, with a larger commercial release set for early 2020.  Read this next: Get the best of Mixmag direct to your Facebook DMs Get closer to dance music. Sign up for the Mixmag newsletter",0.26545887445887445,0.43113048855906005
43,https://news.google.com/articles/CAIiEJcukHALpeZZLMhMDt8Co6gqGQgEKhAIACoHCAow9sveCjC0m9YBMPDtoQM?hl=en-US&gl=US&ceid=US%3Aen," A FEW months back, I wrote about how Artificial Intelligence (AI) is being used to help write articles. What about writing music? Not surprisingly, it’s already being used although it has yet to become widespread or mainstream. But it’s clear that in the future, more and more music will be composed with the help of AI.  Actually, the use of AI in music can be traced back to as early as the 1990s when David Bowie helped develop an app called Verbasizer which helped to come up with word suggestions that could be used for song lyrics.  It would be much later before AI would be used for composition of music though. In 2016, researchers at Sony used a software called Flow Machines to help create songs that sounded like something The Beatles would have written. The basic melodies were then given to a French composer named Benoît Carré, who has written songs for some of France’s biggest stars, including Johnny Halliday and Françoise Hardy.  You can listen to one of the songs created through this process by going to YouTube and looking up a song called Daddy’s Car published by Sony CSL (Computer Science Laboratories). It’s not great but it’s pretty good.  If you’re not a Beatles fan and prefer Bob Dylan, you might be amused to learn that some UK researchers have come up with an AI programme called Bot Dylan. It’s designed to create folk tunes.  To achieve this, the researchers fed it 23,000 folk songs. The algorithm then processed this information to create some original folk music. You can listen to some samples on YouTube by looking for “The Bottomless Tune Box”.   EASE OF USE   Amper Music that has won acclaim because of its ease of use.   “We didn’t expect any of the machine-generated melodies to be very good,” Dr Oded Ben-Tal, a music technology expert at Kingston University in London, told The Daily Mail. “But we, and several other musicians we worked with, were really surprised at the quality of the music the system created.”  Today, many of the tech industry’s biggest names are involved in AI for music. IBM, for example, has something called Watson Beat while Google has NSynth Super. But it’s Amper Music that has won acclaim because of its ease of use.  You don’t need to be a musician or a programmer to use it. Just go to the Amper website and pick a type of music and the mood you like it to be in. And you’re off to making some music of your own with the help of AI.  What Amper does is build new tracks from pre-recorded samples. Of course, you can then modify it by changing various elements, such as the tempo or the mood of the song, the key it’s in and so on.  One notable artist who has used AI for help in composing songs is Taryn Southern, a “digital personality” (she’s human but she became famous for exploring the co-evolution of humanity and technology).  A former contestant on American Idol, Southern has produced an entire album (appropriately entitled I Am AI) with the help of AI. If you look at her track listing, you’ll see that she credits the likes of Amper, (IBM) Watson Beat and (Google) NSynth as her co-writers.  She turned to AI not as a publicity stunt but because she wasn’t a musician. She had song ideas in her head but she didn’t know music theory. “I’d find a beautiful chord on the piano,” Southern shared, adding: “…and I’d write an entire song around that, but then I couldn’t get to the next few chords because I just didn’t know how to play what I was hearing in my head. Now I’m able to iterate with the music and give it feedback and parameters and edit as many times as I need. It still feels like it’s mine in a sense.”   HUMAN TOUCH   Taryn Southern has used AI for help in composing songs.  It’s worth noting that in the case of both Carre and Southern, they used AI merely to help create original songs. They didn’t take the AI-created melodies wholesale without any modifications. Otherwise the songs probably wouldn’t have been very good. A lot of human input was still involved to make the songs good enough to be released as an album.  Even as AI algorithms improve and get better and better at composing tunes, it’s highly unlikely there ever will come a time when music will be produced entirely by machines without any human involvement. Rather, what will most likely happen is artists will use these programmes to give them ideas which they can build upon and create something original.  Carre maintains that while AI systems like Sony’s Flow Machines can output melodies and suggest chords, you’ll still need a human musician to put the songs together into something cohesive and to imbue them with emotion.  “There were many people involved in this,” Carre said about Hello World, the AI-assisted album he released. “They gave their soul, their enthusiasm. I think that’s the most important point of the album, in a way — that it’s a very human one.”     Oon Yeoh is a consultant with experiences in print, online and mobile media. Reach him at [email protected].  ©  New Straits Times,New Straits Times Press (M) Bhd.A part of Media Prima Group.",0.1588177550149381,0.41653649167733686
44,https://news.google.com/articles/CAIiEFzZPccw1aH0TVN2HUNAteMqFggEKg4IACoGCAowl6p7MN-zCTCOvRU?hl=en-US&gl=US&ceid=US%3Aen,"Breakthroughs in artificial intelligence make music composition easier than ever – because a machine is doing half the work. Could computers soon go it alone? 
Tirhakah Love 

Mon 22 Oct 2018 14.00 BST


Last modified on Wed 24 Oct 2018 16.30 BST

 The first testing sessions for SampleRNN – an artificially intelligent software originally developed by machine-learning researcher Dr. Soroush Mehri and expanded by Carr and Zukowski, aka the Dadabots – sounded more like a screamo gig than a machine-learning experiment. Carr and Zukowski hoped their program could generate full-length black metal and math rock albums by feeding it small chunks of sound. The first trial consisted of encoding and entering in a few Nirvana a cappellas. “When it produced its first output,” Carr tells me over email, “I was expecting to hear silence or noise because of an error we made, or else some semblance of singing. But no. The first thing it did was scream about Jesus. We looked at each other like, ‘What the fuck?’” But while the platform could convert Cobain’s grizzled pining into bizarre testimonies to the goodness of the Lord, it couldn’t keep a steady rhythm, much less create a coherent song. Artificial intelligence is already used in music by streaming services such as Spotify, which scan what we listen to so they can better recommend what we might enjoy next. But AI is increasingly being asked to compose music itself – and this is the problem confronting many more computer scientists besides Dadabots. Musicians – popular, experimental and otherwise – have been using AI to varying degrees over the last three decades. Pop’s chief theoretician, Brian Eno, used it not only to create new endlessly perpetuating music on his recent album Reflection but to render an entire visual experience in 2016’s The Ship. The arrangements on Mexican composer Ivan Paz’s album Visions of Space, which sounds a bit like an intergalactic traffic jam, were done by algorithms he created himself. Most recently, producer Baauer – who topped the US charts in 2012 with his viral track Harlem Shake – made Hate Me with Lil Miquela, an artificial digital Instagram avatar. The next step for synthetic beings like these is to create music on their own – that is, if they can get the software to shut up about Jesus. If you have a barrier to entry, you hack your way into figuring it out The first computer-generated score, a string quartet called the Illiac Suite, was developed in 1957 by Lejaren Hiller, and was met with massive controversy among the classical community. Composers at the time were intensely purist. “Most musicians, academic or composers, have always held this idea that the creation of music is innately human,” Californian music professor David Cope explains. “Somehow the computer program was a threat to that unique human aspect of creation.” Fast forward to 1980, and after an insufferable bout of composer’s block, Cope began building a computer that could read music from a database written in numerical code. Seven years later, he’d created Emi (Experiments in Musical Intelligence, pronounced “Emmy”). Cope would compose a piece of music and pass it along to his staff to transcribe the notation into code for Emi to analyse. After many hours of digestion, Emi would spit out an entirely new composition written in code that Cope’s staff would re-transcribe on to staves. Emi could respond not just to Cope’s music, but take in the sounds of Bach, Mozart and other classical music staples and conjure a piece that could fit their compositional style. In the nearly 40 years since, this foundational process has been improved in all manner of ways. YouTube singing sensation Taryn Southern has constructed an LP composed and produced completely by AI using a reworking of Cope’s methods. On her album I AM AI, Southern uses an open source AI platform called Amper to input preferences such as genre, instrumentation, key and beats per minute. Amper is an artificially intelligent music composer founded by film composers Drew Silverstein, Sam Estes and Michael Hobe: it takes commands such as “moody pop” or “modern classical” and creates mostly coherent records that match in tone. From there, an artist can choose to select specific changes in melody, rhythm instrumentation and more. Southern, who says she “doesn’t have a traditional music background,” sometimes rejects as many as 30 versions of each song generated by Amper from her parameters; once Amper creates something she likes the sound of, she exports it to GarageBand, arranges what the program has come up with and adds lyrics. Southern’s DIY model foretells a future of musicians making music with AI on their personal computers. “As an artist,” she says, “if you have a barrier to entry, like whether costs are prohibiting you to make something or not having a team, you kind of hack your way into figuring it out.” Her persistence in low-cost collaboration opened up partnerships with Samsung, Google and HTC, and she is now working on ways to make AI and VR art more accessible. AI isn’t just a useful tool, though – it can be used to explore vital questions about the nature of human expression. This self-reflective impulse epitomises the ethic of New York’s art-tech collective the Mill. “The overarching theme of my work,” explains creative director Rama Allen, “is playing with the concept of the ‘ghost in the machine’: the ghost being the human spirit and the machine being whatever advanced technology we try to apply. I’m interested in the collaboration between the two and the unexpected results that can come from it.” This is the central theme behind the Mill’s musical AI project See Sound, which debuted at last year’s SXSW festival in Austin – a highly reactive sound-sculpture program engineered by the human voice. Hum, sing or rap and See Sound etches a digital sculpture from your vocals on its colourful interface. From there, Allen and his team 3D-print the brand new shape. The sculptures themselves are, Allen says, “like a fingerprint”. The project was born when Allen saw London beatboxer Reeps One live and began to think about the reciprocal relationship between his machine mimicry and music. “I thought, what if I could create another piece of technology that allows him to visualise a shape in his mind, and breathe that shape into existence?” See Sound is one of the most intriguing examples of the ways artists and AI can learn from one another to create something new – advancing the idea of data-driven input into a more reciprocal relationship between man and machine. But while Allen, Southern and Dadabots exemplify an ideal harmony, the reality is that artificial intelligence is an industry, and the utopian synergy of the experimenters’ projects will undoubtedly give way to manipulation – even outright exploitation – by commerce. Spotify’s brilliant machine-learning program delivers recommendations to its listeners, and the company hired AI savant François Pachet to further the company’s entrenchment in the field. Pachet worked on Sony’s Flow Machines, a program that uses AI to compose pop songs. Spotify has already been accused of pushing fake artists in its playlists – mere pseudonyms for conveyor-belt pop made by a team of shadowy producers to evade royalty payouts, a charge it denies – so will they begin to produce their own AI-developed music? Amazon’s AI assistant Alexa features a new skill called DeepMusic which interweaves layers of audio samples to match the vibe set by users. And while the tunes sound too much like a computer drummed them up, that Amazon can already pipe music developed by AI into homes makes for a murky interplay between corporation, in-home AI assistant and creator. As the Guardian reported last year, startups such as AI Music are working on tools that “shape shift” existing songs to match the context in which they’re played; such as switching up the rhythms while driving or increasing the bass as the listener is jogging. Patrick Stobbs, co-founder of AI music composer Jukedeck, recently mentioned that the program would move from tweaking music to actually synthesising it. The London-based company, whose customer base requires what has been described as “functional music”, has already produced work for the likes of Coca-Cola and Google. You can imagine a future where not only hold music is produced by AI, but also music for adverts, TV series and unobtrusive dinner party soundtracks; as well as the blue- and increasingly white-collar workers whose jobs are being replaced by AI, composers could be under threat too. But artists who use AI are steadfast that their work aims to augment the lives of artists and non-artists alike, not replace them. “At this point it’s more surprising to us to hear what humans will do with it,” Carr of Dadabots tells me. “Everything we’re doing is one big scheme to collaborate with bands we love.” An AI-assisted future raises questions around existing inequalities, corporate domination, and artistic integrity: how can we thrive in a world of automation and AI-assisted work without exacerbating the social and economic schisms that have persisted for centuries? It’s likely we won’t. But in the most utopian vision, music will be the first foray into machine-learning for many people, allowing collaboration that edifies the listener, the musician and the machine.",0.1356121618873297,0.43591010778930245
45,https://news.google.com/articles/CBMib2h0dHBzOi8vd3d3LmhvdHByZXNzLmNvbS9tdXNpYy9hcnRpZmljaWFsLWludGVsbGlnZW5jZS1tdXNpY3MtbmV4dC1mcm9udGllci1kcmV3LXNpbHZlcnN0ZWluLWludGVydmlldy0yMjc4NjMwN9IBAA?hl=en-US&gl=US&ceid=US%3Aen,"Named alongside Cardi B and Lizzo in Forbes’ 30 Under 30 class of 2018 – and joining all-star alumni including Drake and The Weeknd – Drew Silverstein keeps good company. Although sitting comfortably with the brightest musicians, songwriters, DJs, agents, managers and entrepreneurs in the industry, Silverstein is first in class when it comes music’s next frontier – the use of artificial intelligence to create songs.

 After a career as a film composer in L.A., Drew Silverstein moved to New York where he co-founded Amper Music. His mission? To combine the highest levels of artistry with groundbreaking artificial intelligence technology to empower anyone to create unique music, instantly. In 2017, Amper raised $4 million in seed funding and is now at the cutting edge of the race to crack AI music. Hot Press caught up with the Amper CEO in Rome, after his recent TEDx Talk, to find out what AI means for our future music consumption, how it will affect our approach to songwriting, how to collaborate with a digital version of yourself, the exploitation of intellectual property by Facebook and Google, the probability of robot composers with feelings, and why he predicts AI to be the greatest creative revolution in the history of music.     MARK HOGAN: What is Amper? DREW SILVERSTEIN: Amper is an AI composer, performer and producer that creates unique and professional music in a matter of seconds. The music can be tailored to content or it can be standalone. Our mission is to enable anyone around the world to express themselves creatively through music regardless of their background, expertise or access to resources. Because fundamentally, every person is creative – by the fact that we’re people. But just being creative doesn’t mean we have the ability to express our creativity. Singing in the shower is easy. Painting a landscape is hard. Composing an orchestral piece of music is probably harder. So the challenge is not creativity, the challenge is expressing it. Who will use AI in musical composition? Ultimately, it provides even the most non-musical individuals – oftentimes video editors, podcast creators and other content creators who utilise functional music, or music that’s valued for its use-case more than its artistic creativity – the ability to take their idea and turn it directly into music. It democratises that expressive ability. On the other end of the spectrum there are creators of what we call artistic music; music that’s valued for the collaboration and creativity that goes into making it, much more so than just its use-case. These artists, musicians and composers already use technology in their process; Amper is the next generation of technology to help further their music-making, by fleshing out new ideas, enabling quick and easy experimentation, working through writer’s block, collaborating with digital versions of themselves… How does it work? Amper is designed to require a minimum amount of input to create a unique piece of music. Every piece of music is created note by note from scratch; there’s no pre-created material, no licenced music, no loops. It really is composed out of thin air, then performed, produced and recorded. So if all you know is the style of music you want to create, the mood you want to convey and the length of your piece of music, you can create a piece of music in a matter of seconds. If you want to talk instrumentation, timing, spotting, harmony, rhythm, you can give as much input as you want. And so the hurdle to create is very low. If you know more about music, the ability to utilise it more powerfully is massive. Tell me about collaborating with a digital version of yourself… Amper has the ability to learn how to make music like anyone. We’re very careful about not doing this regularly, because of legal issues, but have worked directly with musicians who wanted us to create music that sounds like them, in order to create a digital version of their own creative artistry. You basically have you as Amper make music that then you get to collaborate with. I would imagine in the future it will become more the norm until everyone has their own AI version of themselves, and can make music with themselves as a computer. None of Amper’s music is extracted from a bank of songs? No, the songs don’t exist ahead of time. We build data sets that define everything about music, emotion and genre. So you’ve got music theory, orchestration, instrumentation, performance, the foundational objective things of music … and then we define genre and emotion. Ultimately, the technology derives from both of these data sets how to compose a piece of music that will make someone feel a certain way. So if you say ‘Amper, we need you to create a piece of orchestral hip hop that feels determined’, it will figure out how to write the sheet music that will feel determined, in the genre of orchestral hip hop.* And then it’s ‘performed’… Yes, we’ve written the ‘sheet music’; the notes are written down in code. The next step is the performance; turning it into audio. To do that we’ve got one of the world’s largest audio sample libraries. We record every note of every instrument thousands and thousands of times, every possible way you would ever play it. So you take a violin, every note, every dynamic, articulation, velocity, anything you could ever imagine. We capture every way this instrument could ever be played…. That was a big investment… A huge investment, millions of dollars over five years. It’s now one of the world’s largest libraries and it is the only sample library created as a set of data first, as opposed to a library meant to be utilised by humans first. Because of that, there are massive advantages to being able to understand and manipulate the data but ultimately we’ve got this composed piece of sheet music, which Amper then looks at and ‘performs’. It puts thousands of snippets of audio files in a row and stitches them together so it sounds like one audio performance that came out of a studio. These are some of the world’s finest musicians recording their instruments in the world’s best studios. Chord progressions are one thing, is creating a good melody the most difficult part? The melody, chord progressions, rhythm and performance each have their own challenges. What’s wonderfully exciting, and also frustrating, is how to combine all of those elements into a piece of music. Ultimately, music – like everything else in the creative world – is subjective. There’s no right answer. You and I can listen to the same piece of music, have different opinions and both be correct. We can change our opinions, and we’re still correct. Obviously, we believe from an objective perspective that the music we create is quite good but then it’s critical that what Amper creates can be collaborative and editable. No-one will ever get it right 100% of the time; we give you the ability to make changes. There are copyright issues for AI platforms building their music bank from Spotify, for example, even if they’re breaking it down to a granular level. If an AI song is made up of one hundred pieces of music, and if my song on Spotify is used, I want 100th of the split, if it’s a hit, yeah? Totally. It’s a great question. There’s a philosophical argument and then there’s the reality; my philosophical argument is much more liberal than the business reality. With human composition, if you use one song as inspiration to create a piece of music, and it sounds exactly like that song, no-one’s going to argue that it’s a not copyright infringement. If you use a hundred songs as inspiration, and you use one-hundredth of each song, who knows? Maybe it’s infringement, maybe it’s not. If you use a million songs for inspiration, and one-millionth of each song… at some point you would say it’s just being creatively informed by the work. And so there is somewhat subjective delineation in terms of what the sample size of your inspiration would need to be before it’s considered just the creative zeitgeist. That’s never been defined and Amper certainly doesn’t want to be the one to go through a lawsuit to create that. Tell me about your legal journey to protect against potential copyright infringement. We build our data sets and our sample library because we want to be 100% clear of any potential copyright issues even to the extent that we can indemnify our users. Speaking to US copyright law, the two things that go into copyright infringement are access and intent. Did you have access to the material you’re accused of infringing? And / or did you intend to infringe on it? The only sure-fire way to ensure that there is no infringement is to have zero access and zero intent. Anything else is a grey area. That’s why we don’t train it on the world, we build our own internal data sets, so there’s no access (to already-recorded music). When you use Amper, you’re describing music with genres, moods and emotions, and not with artists and song names – so the user has no intent. So you can’t say ‘I’d like Beyoncé, Single Ladies’. Because of that, there’s no access and no intent.  And so every piece of music created is not only unique but is free of any potential copyright issues. What’s the business model? We have an API (Application Programming Interface) that integrates into content creation tools and content distribution platforms. It allows you to control anything about the music. With our web app, designed for those less-musical individuals, you pay for monthly or annual access to create as much music as you like. It’s typically used by video editors, game developers and podcast creators because they want to use the music wholesale. Who owns the copyright? You made creative decisions in the creation of a novel piece of work, so you’ve created a copyright. By the terms of your agreement with Amper, you assign the copyright and all the rights back to Amper, and we provide you a royalty-free global perpetual license to use that music however you like. So, if you create a piece for an advert or a video using Amper’s music front-to-back, that music is owned by Amper. What’s the deal for musicians? When musicians use it as an ingredient in a newly created work – and to be clear, while they sometimes use it, the platform is not necessarily designed for musicians ­– we relinquish rights. If you use Amper as you use sample libraries, as an ingredient in your new creative work but not as the core of the whole thing, then we will grant you a license and not claim any ownership over the resulting music. Have you done deals with labels? We’ve done deals with artists. We’ve had a lot of good conversations with labels, and I think at some point it will just be commonplace. Has there been push-back from the music industry? Not anymore. Five years ago there certainly was. AI music is not a new idea. You can say that computer music has been around since the 1950s, since computers. Algorithm music has been around since at least the days of Bach, the 1700s. But Amper is the first company that’s been able to use that on a commercially successful level. Bowie used the cut-up technique and subsequently developed a digital lyric randomiser for ""igniting anything"" that might be in his imagination. What are your thoughts on creating lyrics with AI? We don’t do lyrics right now. Music is a hard challenge by itself! They’re getting better and there’s a parallel development. In some ways it’s a similar problem; it’s translating creative ideas into reality. In other ways, there are very different problems; with lyrics, you’ve got the actual written words, which can have all sorts of subjective meanings – and then you’ve got the performance of those lyrics. In music, you’ve got the writing and the performance of the notes. So the path is similar, but the details are different enough that we want to stay focused on our core creative abilities now, and we’ll save lyrics for a later day.    If AI could discern the formula for writing a hit, surely everything should be a hit? And If everything is a hit, then nothing is… No, just because every person can create a piece of music, it doesn’t mean that every piece of music will be a hit. What makes music a hit is in itself a very separate thing. There are people whose careers are built around trying to recognise it; some people say you can predict a hit because of the things that people respond to, others say it’s almost impossible, Should we leave songwriting to the talented songwriters? Will the democratisation of music creation muddy the water in terms of what we listen to, and are we adding clutter to an already-crowded environment? I would say very much no. If we think about writing… just because anyone today can create a blog or can self-publish a book, it doesn’t mean that we don’t value great authors any less. In fact, we probably value them more, because we recognise how much better they are than everything else. They’re using a lot of similar tools but they use them to further their artistry, rather than just to enable their baseline expression, and the parallel to music is very similar. Isn’t there a valid aesthetic position that the imperfections in a performance are what make a piece of music? You’ve created original sounds, but can you feel the push and pull of a drummer playing in the moment? Totally, yeah. One of the most wonderful things about humanity is that we make art. It’s inevitable that AI music will be indistinguishable from human-created music. Because even the imperfections, the push and the pull, will be solved for; whether it’s today or in a thousand years, provided that humanity is around. It’s a matter of when, not if. But the differentiation is on the value that we place on the music; are we valuing it for the artistry of the creation, of the performance, of watching a person make a thing? Or are we valuing the music because we have a video that we need music behind? Take coffee as an example. I live in New York, where you can get coffee in McDonald’s for a dollar; you get caffeine. Or you can go to an artisan coffee shop in Brooklyn and spend $15 for an artistically created drink from Rainforest Alliance growers, that’s conflict-free, organic and non-GMO. I’m still getting caffeine, right? But what I value about it is very different. I value the process that was used to make it: the collaboration and creativity. Even though the end result is that both give me the same amount of caffeine, the value I place on them is very different.    Will robots become sentient? I think the answer is probably yes.   When? Between now and 10,000 years from now. I mean that seriously. I don’t know when but I think provided that humanity still exists, it’s an inevitability. Much like almost any technological evolution probably is, provided it doesn’t break the laws of physics. Given enough time. I wouldn’t be the one to say it’ll be next month or next decade or next century but given enough time I think it’ll probably happen. So a robot will be able to feel the music? Yes. That might be 300 years from now but who knows? Will AI ever be able to write a line like ‘I can almost smell your T.B. sheets, on your sick bed’, as Van Morrison did? Probably…   And deliver the line with the same emotion? Probably, yes. Which, again, no time soon, to be clear… He broke down in tears after recording it, such was the feeling that went into the session… Yeah, totally. One of the tropes used to describe AI or leading edge technology is that it seems like magic… until it’s not. A hundred years ago, people could have been talking about this electricity thing, ‘will it be able to do this manual job or that manual job?’. In that time period, it was almost inconceivable – now it’s just commonplace. So from a philosophical perspective, if pushed to give a binary answer on ‘can a thing happen?’ – again, provided it doesn’t break fundamental rules of physics and existence – probably ‘yes’. But even when that day comes, there will always be a differentiation and distinction with something made by a robot – or even something made by a person, that’s valued functionally, versus artistically. We can listen to Van Morrison on Spotify and it’s cool. But there’s something different if you see him play live. There’s an emotional connection with his creativity and that is something that’s just innately human. So even when robots can do things themselves, it doesn’t mean the value we place on it will be the same. Do you see a time when people will stay tuned to an AI-only radio station? Sure. Amper’s music has already been proven to be indistinguishable from human-created music. That is to say, almost all of the time, if you didn’t know you were listening to AI music, you wouldn’t know. Even a musician might not know. Or they might say, ‘it’s not an Academy Award-winning song yet’ but almost 100% of the time people say ‘oh, it sounds like a person has made the music.’ Where will AI music figure in the Top 40 within the next five years? I would say 80% of the Top 40 will use AI music technology in its creation – as a tool in the creative process – within five years. I think very little of Top 40 will be made solely and 100% by AI. People fall in love with musicians. Can they fall in love with AI-created music and is that something you’re trying to encourage? Yes, with an asterisk. Empowering musicians to help make music is what we love to do. People fall in love with Bono, right? And there’s a relationship. In Japan and China, there are artists – digital avatars –  with huge fan bases. They’re completely ones and zeros! It’s not necessarily for us to decide where people invest emotionally. We just want to make sure that whoever wants to be that creator has the ability to do so. Should Facebook and Google be defined as publishers and be made to take responsibility for what is published on their platforms? Yeah, I think so. The DMCA cover they’ve had for a long time is very enabling to creators but also provides a lot of opportunity for unfair exploitation. One might argue that that was necessary to create value and the opportunity to make these things work in the nascent stages of social media and online sharing. But in the mature stage that we’re at now, it’s critical that those who enable the exploitation of IPs – Facebook, Google and others – take responsibility and be held responsible for what they do. Because they’re very powerful concepts… technology platforms that enable the world to connect globally. And oftentimes the content creator gets the short end of the stick. There’s an argument of fairness and equality. I’m not the one to decide where that line is but I would certainly say that right now much more of the value goes to the platforms, publishers and distributors than to the creators. And the pendulum can definitely move back towards those who really create the value. There is a growing realisation that the Internet has been used to colonise individuals, creating a new kind of digital serfdom... AI music is a great democratising force. Typically, lordship and serfdom come about when there are inequalities in access and ability. When technology advances, it ultimately lowers the bar from the time, cost and resource perspective of what it takes to be able to do a ‘thing’. Then more people can do that thing, it becomes more democratised, resulting in less disparity between the haves and have nots, which ultimately diminishes the gap between those who previously were the lords and the serfs. So technology and music AI are great democratisers and enablers of expression. How we utilise that is up to any individual. There will continue to be artists we revere and those we don’t. They’ll all have access to the same technology, so it comes down to what do you do with it? How can the malign influence of social media be halted? One of the biggest societal changes with the advent of social media and global connectivity is that our interactions have become less personal. When that happens many people are emboldened. It gets harder to vet material. Ideas spread more quickly and easily. And when the barriers to malevolent behaviour are diminished, and when the cost and commitment of sharing something is as simple as a click – rather than reading, discussing and debating – it creates an environment where the safeguards that we’ve historically had as a human race are largely eroded. The teams in all the large social companies are working on figuring out, in essence, how to put safeguards back in, without unfairly limiting and undermining the core value proposition of their platforms in the first place. And that’s an imperfect part that certainly is very important both to the consumers and creators of a platform. I think everyone agrees it hasn’t been perfectly solved yet. You said in your TEDx Talk that AI will be the greatest creative revolution in music history. Do you have concerns as to how experienced musicians will embrace it? Many may see it as scary thing at first, and again largely it’s because there’s fear around its creative opaqueness in terms of the consequences. But I hope everyone sees it as a massively enabling tool. AI is the next generation of technology in music-making. It should allow less-musical individuals to all of a sudden express themselves through music. And it should allow more-musical individuals to harness a new superpower. Think about how much creative expression benefited from the digital revolution over tape. And how before that, creative expression benefited from tape over non-recorded music. Every step of the way, things – the world – changed.  What advice do you have for Irish songwriters? I would say to Irish musicians and songwriters that how we make music will continue to evolve, it always has. Our career – or our ability to help others accomplish their goals – might never change. In the same light, new tools should enhance rather than undermine our ability to help others achieve their goals. So if the music we create today is valued as functional music, that value is being democratised and so you can move up the value chain with your music. If the music you create is valued as artistic music, your career can be as prosperous as ever. How do Irish musicians get their music into movies in the US? To get your music into production, typically you either compose the score, which is the path that I took, or get placements for your songs. Each has a very different route. Composers build relationships with directors and producers in studios to become a part of the core creative team. Artists seeking placements need to build relationships with music supervisors – and sometimes producers as well – to hopefully be top of mind for new projects. How do you crack the music supervisor nut? Half the game is persistence, a quarter is talent and the other quarter is being a good person and working hard. It’s true now more than ever that people want to work with people they can get along with. And so if you put your head down, you make good music and you’re a good person, I’m confident that most people can crack it. What are your thoughts on Irish music? I love Irish music! As a film composer, it’s obviously a huge inspiration for a whole genre of projects. I started out writing music for movies, TV shows and video games in Los Angeles. U2 is one of the bands I grew up listening to; the Edge’s guitar is part of the foundational sound I grew up with. It certainly was an influence. To finish, will AI help to create a song better than The Beatles or Van? Songs better than we’ve heard to date? Better is subjective. It will help make songs more efficiently, it will help expand the creative boundaries. It will help us to experiment more easily and cheaply. It will help flesh out new ideas. Whether they’re better songs? My grandparents would say the best music was made in the fifties, someone else might say the best music is made today. It’s such a subjective thing. But we can say it will help further enable the expression of creativity through music. For certain. What we do with that capability is up to us. Drew Silverstein appeared at TEDx Roma. Go to ampermusic.com to compose using artificial intelligence. Go to hotpress.com/aimusic for more.  *(A)I COMPOSED ORCHESTRAL HIP HOP Intrigued by the idea of an AI-created piece of orchestral hip hop as described by Drew Silverstein, Mark decided to put Amper to the test. He tasked Amper to write and perform a piece of music that, in Drew’s words, “will feel determined, in the genre of orchestral hip hop”. Listen to the result at hotpress.com/aimusic",0.17626043637040706,0.5305894732507639
46,https://news.google.com/articles/CBMiYGh0dHBzOi8vbXVzaWNhbGx5LmNvbS8yMDIwLzAyLzEwL2FpLXBvd2VyZWQtc3ltcGhvbmlhLWFwcC10dXJucy1zaW5naW5nLWludG8taW5zdHJ1bWVudGFsLW11c2ljL9IBAA?hl=en-US&gl=US&ceid=US%3Aen,"
Username or Email Address

 
Password

  Remember Me 


 French startup Symphonia claims that its iPhone app is “the future of music composing”. Released this year, it uses AI to help people turn their musical ideas into actual music. “Just sing a melody. And Symphonia automatically detects the notes you sing and magically transforms them into amazing musical instruments in MIDI format,” explains its App Store blurb. “Thanks to an incredible collection of instruments, you can build your musical project – track by track – by superimposing as many instruments as you have in mind and compose stunning songs with professional rendering.” It’s not the first app to explore this territory: HumTap and HumOn both work by getting people to hum a melody into their smartphone, with AI used to turn that into music. Symphonia aims to do that with a wide choice of instruments, plus the multi-track aspect. At this point, the app is completely free to download and use, although the developers say they are still discussing its business model – while confirming that “core features are and will remain free”. (All fields required)   Music Ally Ltd., Holborn Studios,
49-50 Eagle Wharf Rd, London, N1 7ED,
United Kingdom Music Ally is a Registered Learning Provider 10029483",0.25,0.42107843137254897
47,https://news.google.com/articles/CBMiaWh0dHBzOi8vd3d3LnRpbWVzb2Zpc3JhZWwuY29tL2FpLXRvLXVwZW5kLW1lbG9keS1tYWtpbmctdGVhY2gtYXJ0aXN0cy1ob3ctdG8tcGxlYXNlLXRlY2gtbXVzaWMtZ3VydS1zYXlzL9IBAA?hl=en-US&gl=US&ceid=US%3Aen,"The use of artificial intelligence is set to revolutionize how people create music but still, robots will not replace humans in the art of making melodies, attendees of a conference about art and music were told on Sunday.
“Artificial intelligence will not replace good artists and composers,” François Pachet, a scientist, composer and the director of the Spotify Creator Technology Research Lab, told participants of the TechnoArt 2019 conference in Tel Aviv. “AI will change the way people make art, but it won’t replace them.”
    


            if(typeof rgb_remove_toi_dfp_banner != ""function"" || !rgb_remove_toi_dfp_banner(""#div-gpt-ad-336x280_Middle_1"")){
                googletag.cmd.push(function() { googletag.display(""div-gpt-ad-336x280_Middle_1""); });
            };
        

Pachet is considered a pioneer of computer music, and specifically its interaction with AI. At Spotify he leads development of AI-based tools for musicians.
							

									Get The Start-Up Israel's Daily Start-Up by email and never miss our top stories
								

									Free Sign Up
								

At the conference Pachet explained how as director of SONY Computer Science Laboratory in Paris, where he worked before joining Spotify, he and his team used artificial intelligence software — called Flow Machines — to make new kinds of music. For example, his team fed the software 45 Beatles songs and produced a new song, “Daddy’s Car,” based on the Fab Four’s style. The song is on YouTube and has not been commercialized, he said.

The AI system, Pachet explained, studied the notes and patterns of the Beatles songs and “then used it in a new context to create something new.”
“The song here is new; it is original and it is not doing any plagiarism — but yet it is using a lot of patterns and features of the style of the Beatles,” he said.
Pachet also used Flow Machines to create an album of 15 songs called “Hello World” — the first music album composed with artificial intelligence — together with French songwriter and producer Benoît Carré last year.
The album was streamed by 12 million users and music critics liked it, with the BBC calling it “the world’s first good robot album.”
Pachet’s work at Spotify has had him mixing genres — such as folk songs with gospel or jazz — to come up with new orchestrations that are very different from the originals. In another twist, his team might blend the rhythm of one song with the harmony of a second to create a “completely brand-new piece” of music, he said.
Critically, however, said Pachet, the decision of what to mix with what is still done by a human artist, and not by a machine. “That is the responsibility of the artist,” he said, and that will remain the difference between musical art and robotically developed music.

François Pachet, director of the Spotify Creator Technology Research Lab, playing the guitar with The Beatles featured in the background at the TechnoArt 2019 conference in Tel Aviv, Nov. 10, 2019 (Shoshanna Solomon/Times of Israel)

Outside of composition, artificial intelligence tools are used to gain insights into how consumers listen to music, Pachet said. In a study published in March, Pachet and two other researchers talk about the phenomenon of “music skipping.”
Online streaming services are a preferred way to consume music. But because there are so many songs available via these services, consumers tend to “skip” quickly from one song to another. This has allowed the researchers to create a “skip profile” — which shows how long people actually listen to music before skipping to the next song.
“Skipping is a crucial feature in understanding modern listening behaviors,” the researchers say in the paper. “For the first time in the history of musicology, researchers can systematically collect and analyze massive amounts of data about music listening behavior.”
Research data presented in the study shows that a quarter of all streamed songs are skipped within the first five seconds, 29% in the first 10 seconds, and 35% in the first 30 seconds, and only some 48% of all songs listened to in their entirety, Pachet said.
What the data showed, however, was that the skip profile for each song is the same every day, every week and every month for everyone included in the study.
“This means that skipping is not people centered, but it is because of the song, it is the signature of the song,” Pachet said.
The research showed a correlation between skipping and the structure of the music piece. With this information, he said, musicians will be able study how people react to their music and will be able to develop compositions that are better-received.
Music makers would be able to “manipulate their audience in a very precise way,” Pachet said. And “that is going to change a lot of things in future,” he said.
In addition, Pachet said, with the advancement of the use of AI tools in music, regulators may need to redefine the concept of what is considered “original” content, and perhaps create new copyright laws and a new royalties system for music that is reinvented by AI, but based on the remixing and reworking of originals.
 “Artificial intelligence will not replace good artists and composers,” François Pachet, a scientist, composer and the director of the Spotify Creator Technology Research Lab, told participants of the TechnoArt 2019 conference in Tel Aviv. “AI will change the way people make art, but it won’t replace them.”
    


            if(typeof rgb_remove_toi_dfp_banner != ""function"" || !rgb_remove_toi_dfp_banner(""#div-gpt-ad-336x280_Middle_1"")){
                googletag.cmd.push(function() { googletag.display(""div-gpt-ad-336x280_Middle_1""); });
            };
        

Pachet is considered a pioneer of computer music, and specifically its interaction with AI. At Spotify he leads development of AI-based tools for musicians.
							

									Get The Start-Up Israel's Daily Start-Up by email and never miss our top stories
								

									Free Sign Up
								

At the conference Pachet explained how as director of SONY Computer Science Laboratory in Paris, where he worked before joining Spotify, he and his team used artificial intelligence software — called Flow Machines — to make new kinds of music. For example, his team fed the software 45 Beatles songs and produced a new song, “Daddy’s Car,” based on the Fab Four’s style. The song is on YouTube and has not been commercialized, he said.

The AI system, Pachet explained, studied the notes and patterns of the Beatles songs and “then used it in a new context to create something new.”
“The song here is new; it is original and it is not doing any plagiarism — but yet it is using a lot of patterns and features of the style of the Beatles,” he said.
Pachet also used Flow Machines to create an album of 15 songs called “Hello World” — the first music album composed with artificial intelligence — together with French songwriter and producer Benoît Carré last year.
The album was streamed by 12 million users and music critics liked it, with the BBC calling it “the world’s first good robot album.”
Pachet’s work at Spotify has had him mixing genres — such as folk songs with gospel or jazz — to come up with new orchestrations that are very different from the originals. In another twist, his team might blend the rhythm of one song with the harmony of a second to create a “completely brand-new piece” of music, he said.
Critically, however, said Pachet, the decision of what to mix with what is still done by a human artist, and not by a machine. “That is the responsibility of the artist,” he said, and that will remain the difference between musical art and robotically developed music.

François Pachet, director of the Spotify Creator Technology Research Lab, playing the guitar with The Beatles featured in the background at the TechnoArt 2019 conference in Tel Aviv, Nov. 10, 2019 (Shoshanna Solomon/Times of Israel)

Outside of composition, artificial intelligence tools are used to gain insights into how consumers listen to music, Pachet said. In a study published in March, Pachet and two other researchers talk about the phenomenon of “music skipping.”
Online streaming services are a preferred way to consume music. But because there are so many songs available via these services, consumers tend to “skip” quickly from one song to another. This has allowed the researchers to create a “skip profile” — which shows how long people actually listen to music before skipping to the next song.
“Skipping is a crucial feature in understanding modern listening behaviors,” the researchers say in the paper. “For the first time in the history of musicology, researchers can systematically collect and analyze massive amounts of data about music listening behavior.”
Research data presented in the study shows that a quarter of all streamed songs are skipped within the first five seconds, 29% in the first 10 seconds, and 35% in the first 30 seconds, and only some 48% of all songs listened to in their entirety, Pachet said.
What the data showed, however, was that the skip profile for each song is the same every day, every week and every month for everyone included in the study.
“This means that skipping is not people centered, but it is because of the song, it is the signature of the song,” Pachet said.
The research showed a correlation between skipping and the structure of the music piece. With this information, he said, musicians will be able study how people react to their music and will be able to develop compositions that are better-received.
Music makers would be able to “manipulate their audience in a very precise way,” Pachet said. And “that is going to change a lot of things in future,” he said.
In addition, Pachet said, with the advancement of the use of AI tools in music, regulators may need to redefine the concept of what is considered “original” content, and perhaps create new copyright laws and a new royalties system for music that is reinvented by AI, but based on the remixing and reworking of originals.
 Pachet is considered a pioneer of computer music, and specifically its interaction with AI. At Spotify he leads development of AI-based tools for musicians.
							

									Get The Start-Up Israel's Daily Start-Up by email and never miss our top stories
								

									Free Sign Up
								

At the conference Pachet explained how as director of SONY Computer Science Laboratory in Paris, where he worked before joining Spotify, he and his team used artificial intelligence software — called Flow Machines — to make new kinds of music. For example, his team fed the software 45 Beatles songs and produced a new song, “Daddy’s Car,” based on the Fab Four’s style. The song is on YouTube and has not been commercialized, he said.

The AI system, Pachet explained, studied the notes and patterns of the Beatles songs and “then used it in a new context to create something new.”
“The song here is new; it is original and it is not doing any plagiarism — but yet it is using a lot of patterns and features of the style of the Beatles,” he said.
Pachet also used Flow Machines to create an album of 15 songs called “Hello World” — the first music album composed with artificial intelligence — together with French songwriter and producer Benoît Carré last year.
The album was streamed by 12 million users and music critics liked it, with the BBC calling it “the world’s first good robot album.”
Pachet’s work at Spotify has had him mixing genres — such as folk songs with gospel or jazz — to come up with new orchestrations that are very different from the originals. In another twist, his team might blend the rhythm of one song with the harmony of a second to create a “completely brand-new piece” of music, he said.
Critically, however, said Pachet, the decision of what to mix with what is still done by a human artist, and not by a machine. “That is the responsibility of the artist,” he said, and that will remain the difference between musical art and robotically developed music.

François Pachet, director of the Spotify Creator Technology Research Lab, playing the guitar with The Beatles featured in the background at the TechnoArt 2019 conference in Tel Aviv, Nov. 10, 2019 (Shoshanna Solomon/Times of Israel)

Outside of composition, artificial intelligence tools are used to gain insights into how consumers listen to music, Pachet said. In a study published in March, Pachet and two other researchers talk about the phenomenon of “music skipping.”
Online streaming services are a preferred way to consume music. But because there are so many songs available via these services, consumers tend to “skip” quickly from one song to another. This has allowed the researchers to create a “skip profile” — which shows how long people actually listen to music before skipping to the next song.
“Skipping is a crucial feature in understanding modern listening behaviors,” the researchers say in the paper. “For the first time in the history of musicology, researchers can systematically collect and analyze massive amounts of data about music listening behavior.”
Research data presented in the study shows that a quarter of all streamed songs are skipped within the first five seconds, 29% in the first 10 seconds, and 35% in the first 30 seconds, and only some 48% of all songs listened to in their entirety, Pachet said.
What the data showed, however, was that the skip profile for each song is the same every day, every week and every month for everyone included in the study.
“This means that skipping is not people centered, but it is because of the song, it is the signature of the song,” Pachet said.
The research showed a correlation between skipping and the structure of the music piece. With this information, he said, musicians will be able study how people react to their music and will be able to develop compositions that are better-received.
Music makers would be able to “manipulate their audience in a very precise way,” Pachet said. And “that is going to change a lot of things in future,” he said.
In addition, Pachet said, with the advancement of the use of AI tools in music, regulators may need to redefine the concept of what is considered “original” content, and perhaps create new copyright laws and a new royalties system for music that is reinvented by AI, but based on the remixing and reworking of originals.
 

									Get The Start-Up Israel's Daily Start-Up by email and never miss our top stories
								

									Free Sign Up
								
 At the conference Pachet explained how as director of SONY Computer Science Laboratory in Paris, where he worked before joining Spotify, he and his team used artificial intelligence software — called Flow Machines — to make new kinds of music. For example, his team fed the software 45 Beatles songs and produced a new song, “Daddy’s Car,” based on the Fab Four’s style. The song is on YouTube and has not been commercialized, he said.

The AI system, Pachet explained, studied the notes and patterns of the Beatles songs and “then used it in a new context to create something new.”
“The song here is new; it is original and it is not doing any plagiarism — but yet it is using a lot of patterns and features of the style of the Beatles,” he said.
Pachet also used Flow Machines to create an album of 15 songs called “Hello World” — the first music album composed with artificial intelligence — together with French songwriter and producer Benoît Carré last year.
The album was streamed by 12 million users and music critics liked it, with the BBC calling it “the world’s first good robot album.”
Pachet’s work at Spotify has had him mixing genres — such as folk songs with gospel or jazz — to come up with new orchestrations that are very different from the originals. In another twist, his team might blend the rhythm of one song with the harmony of a second to create a “completely brand-new piece” of music, he said.
Critically, however, said Pachet, the decision of what to mix with what is still done by a human artist, and not by a machine. “That is the responsibility of the artist,” he said, and that will remain the difference between musical art and robotically developed music.

François Pachet, director of the Spotify Creator Technology Research Lab, playing the guitar with The Beatles featured in the background at the TechnoArt 2019 conference in Tel Aviv, Nov. 10, 2019 (Shoshanna Solomon/Times of Israel)

Outside of composition, artificial intelligence tools are used to gain insights into how consumers listen to music, Pachet said. In a study published in March, Pachet and two other researchers talk about the phenomenon of “music skipping.”
Online streaming services are a preferred way to consume music. But because there are so many songs available via these services, consumers tend to “skip” quickly from one song to another. This has allowed the researchers to create a “skip profile” — which shows how long people actually listen to music before skipping to the next song.
“Skipping is a crucial feature in understanding modern listening behaviors,” the researchers say in the paper. “For the first time in the history of musicology, researchers can systematically collect and analyze massive amounts of data about music listening behavior.”
Research data presented in the study shows that a quarter of all streamed songs are skipped within the first five seconds, 29% in the first 10 seconds, and 35% in the first 30 seconds, and only some 48% of all songs listened to in their entirety, Pachet said.
What the data showed, however, was that the skip profile for each song is the same every day, every week and every month for everyone included in the study.
“This means that skipping is not people centered, but it is because of the song, it is the signature of the song,” Pachet said.
The research showed a correlation between skipping and the structure of the music piece. With this information, he said, musicians will be able study how people react to their music and will be able to develop compositions that are better-received.
Music makers would be able to “manipulate their audience in a very precise way,” Pachet said. And “that is going to change a lot of things in future,” he said.
In addition, Pachet said, with the advancement of the use of AI tools in music, regulators may need to redefine the concept of what is considered “original” content, and perhaps create new copyright laws and a new royalties system for music that is reinvented by AI, but based on the remixing and reworking of originals.
 
The AI system, Pachet explained, studied the notes and patterns of the Beatles songs and “then used it in a new context to create something new.”
“The song here is new; it is original and it is not doing any plagiarism — but yet it is using a lot of patterns and features of the style of the Beatles,” he said.
Pachet also used Flow Machines to create an album of 15 songs called “Hello World” — the first music album composed with artificial intelligence — together with French songwriter and producer Benoît Carré last year.
The album was streamed by 12 million users and music critics liked it, with the BBC calling it “the world’s first good robot album.”
Pachet’s work at Spotify has had him mixing genres — such as folk songs with gospel or jazz — to come up with new orchestrations that are very different from the originals. In another twist, his team might blend the rhythm of one song with the harmony of a second to create a “completely brand-new piece” of music, he said.
Critically, however, said Pachet, the decision of what to mix with what is still done by a human artist, and not by a machine. “That is the responsibility of the artist,” he said, and that will remain the difference between musical art and robotically developed music.

François Pachet, director of the Spotify Creator Technology Research Lab, playing the guitar with The Beatles featured in the background at the TechnoArt 2019 conference in Tel Aviv, Nov. 10, 2019 (Shoshanna Solomon/Times of Israel)

Outside of composition, artificial intelligence tools are used to gain insights into how consumers listen to music, Pachet said. In a study published in March, Pachet and two other researchers talk about the phenomenon of “music skipping.”
Online streaming services are a preferred way to consume music. But because there are so many songs available via these services, consumers tend to “skip” quickly from one song to another. This has allowed the researchers to create a “skip profile” — which shows how long people actually listen to music before skipping to the next song.
“Skipping is a crucial feature in understanding modern listening behaviors,” the researchers say in the paper. “For the first time in the history of musicology, researchers can systematically collect and analyze massive amounts of data about music listening behavior.”
Research data presented in the study shows that a quarter of all streamed songs are skipped within the first five seconds, 29% in the first 10 seconds, and 35% in the first 30 seconds, and only some 48% of all songs listened to in their entirety, Pachet said.
What the data showed, however, was that the skip profile for each song is the same every day, every week and every month for everyone included in the study.
“This means that skipping is not people centered, but it is because of the song, it is the signature of the song,” Pachet said.
The research showed a correlation between skipping and the structure of the music piece. With this information, he said, musicians will be able study how people react to their music and will be able to develop compositions that are better-received.
Music makers would be able to “manipulate their audience in a very precise way,” Pachet said. And “that is going to change a lot of things in future,” he said.
In addition, Pachet said, with the advancement of the use of AI tools in music, regulators may need to redefine the concept of what is considered “original” content, and perhaps create new copyright laws and a new royalties system for music that is reinvented by AI, but based on the remixing and reworking of originals.
 The AI system, Pachet explained, studied the notes and patterns of the Beatles songs and “then used it in a new context to create something new.”
“The song here is new; it is original and it is not doing any plagiarism — but yet it is using a lot of patterns and features of the style of the Beatles,” he said.
Pachet also used Flow Machines to create an album of 15 songs called “Hello World” — the first music album composed with artificial intelligence — together with French songwriter and producer Benoît Carré last year.
The album was streamed by 12 million users and music critics liked it, with the BBC calling it “the world’s first good robot album.”
Pachet’s work at Spotify has had him mixing genres — such as folk songs with gospel or jazz — to come up with new orchestrations that are very different from the originals. In another twist, his team might blend the rhythm of one song with the harmony of a second to create a “completely brand-new piece” of music, he said.
Critically, however, said Pachet, the decision of what to mix with what is still done by a human artist, and not by a machine. “That is the responsibility of the artist,” he said, and that will remain the difference between musical art and robotically developed music.

François Pachet, director of the Spotify Creator Technology Research Lab, playing the guitar with The Beatles featured in the background at the TechnoArt 2019 conference in Tel Aviv, Nov. 10, 2019 (Shoshanna Solomon/Times of Israel)

Outside of composition, artificial intelligence tools are used to gain insights into how consumers listen to music, Pachet said. In a study published in March, Pachet and two other researchers talk about the phenomenon of “music skipping.”
Online streaming services are a preferred way to consume music. But because there are so many songs available via these services, consumers tend to “skip” quickly from one song to another. This has allowed the researchers to create a “skip profile” — which shows how long people actually listen to music before skipping to the next song.
“Skipping is a crucial feature in understanding modern listening behaviors,” the researchers say in the paper. “For the first time in the history of musicology, researchers can systematically collect and analyze massive amounts of data about music listening behavior.”
Research data presented in the study shows that a quarter of all streamed songs are skipped within the first five seconds, 29% in the first 10 seconds, and 35% in the first 30 seconds, and only some 48% of all songs listened to in their entirety, Pachet said.
What the data showed, however, was that the skip profile for each song is the same every day, every week and every month for everyone included in the study.
“This means that skipping is not people centered, but it is because of the song, it is the signature of the song,” Pachet said.
The research showed a correlation between skipping and the structure of the music piece. With this information, he said, musicians will be able study how people react to their music and will be able to develop compositions that are better-received.
Music makers would be able to “manipulate their audience in a very precise way,” Pachet said. And “that is going to change a lot of things in future,” he said.
In addition, Pachet said, with the advancement of the use of AI tools in music, regulators may need to redefine the concept of what is considered “original” content, and perhaps create new copyright laws and a new royalties system for music that is reinvented by AI, but based on the remixing and reworking of originals.
 “The song here is new; it is original and it is not doing any plagiarism — but yet it is using a lot of patterns and features of the style of the Beatles,” he said.
Pachet also used Flow Machines to create an album of 15 songs called “Hello World” — the first music album composed with artificial intelligence — together with French songwriter and producer Benoît Carré last year.
The album was streamed by 12 million users and music critics liked it, with the BBC calling it “the world’s first good robot album.”
Pachet’s work at Spotify has had him mixing genres — such as folk songs with gospel or jazz — to come up with new orchestrations that are very different from the originals. In another twist, his team might blend the rhythm of one song with the harmony of a second to create a “completely brand-new piece” of music, he said.
Critically, however, said Pachet, the decision of what to mix with what is still done by a human artist, and not by a machine. “That is the responsibility of the artist,” he said, and that will remain the difference between musical art and robotically developed music.

François Pachet, director of the Spotify Creator Technology Research Lab, playing the guitar with The Beatles featured in the background at the TechnoArt 2019 conference in Tel Aviv, Nov. 10, 2019 (Shoshanna Solomon/Times of Israel)

Outside of composition, artificial intelligence tools are used to gain insights into how consumers listen to music, Pachet said. In a study published in March, Pachet and two other researchers talk about the phenomenon of “music skipping.”
Online streaming services are a preferred way to consume music. But because there are so many songs available via these services, consumers tend to “skip” quickly from one song to another. This has allowed the researchers to create a “skip profile” — which shows how long people actually listen to music before skipping to the next song.
“Skipping is a crucial feature in understanding modern listening behaviors,” the researchers say in the paper. “For the first time in the history of musicology, researchers can systematically collect and analyze massive amounts of data about music listening behavior.”
Research data presented in the study shows that a quarter of all streamed songs are skipped within the first five seconds, 29% in the first 10 seconds, and 35% in the first 30 seconds, and only some 48% of all songs listened to in their entirety, Pachet said.
What the data showed, however, was that the skip profile for each song is the same every day, every week and every month for everyone included in the study.
“This means that skipping is not people centered, but it is because of the song, it is the signature of the song,” Pachet said.
The research showed a correlation between skipping and the structure of the music piece. With this information, he said, musicians will be able study how people react to their music and will be able to develop compositions that are better-received.
Music makers would be able to “manipulate their audience in a very precise way,” Pachet said. And “that is going to change a lot of things in future,” he said.
In addition, Pachet said, with the advancement of the use of AI tools in music, regulators may need to redefine the concept of what is considered “original” content, and perhaps create new copyright laws and a new royalties system for music that is reinvented by AI, but based on the remixing and reworking of originals.
 Pachet also used Flow Machines to create an album of 15 songs called “Hello World” — the first music album composed with artificial intelligence — together with French songwriter and producer Benoît Carré last year.
The album was streamed by 12 million users and music critics liked it, with the BBC calling it “the world’s first good robot album.”
Pachet’s work at Spotify has had him mixing genres — such as folk songs with gospel or jazz — to come up with new orchestrations that are very different from the originals. In another twist, his team might blend the rhythm of one song with the harmony of a second to create a “completely brand-new piece” of music, he said.
Critically, however, said Pachet, the decision of what to mix with what is still done by a human artist, and not by a machine. “That is the responsibility of the artist,” he said, and that will remain the difference between musical art and robotically developed music.

François Pachet, director of the Spotify Creator Technology Research Lab, playing the guitar with The Beatles featured in the background at the TechnoArt 2019 conference in Tel Aviv, Nov. 10, 2019 (Shoshanna Solomon/Times of Israel)

Outside of composition, artificial intelligence tools are used to gain insights into how consumers listen to music, Pachet said. In a study published in March, Pachet and two other researchers talk about the phenomenon of “music skipping.”
Online streaming services are a preferred way to consume music. But because there are so many songs available via these services, consumers tend to “skip” quickly from one song to another. This has allowed the researchers to create a “skip profile” — which shows how long people actually listen to music before skipping to the next song.
“Skipping is a crucial feature in understanding modern listening behaviors,” the researchers say in the paper. “For the first time in the history of musicology, researchers can systematically collect and analyze massive amounts of data about music listening behavior.”
Research data presented in the study shows that a quarter of all streamed songs are skipped within the first five seconds, 29% in the first 10 seconds, and 35% in the first 30 seconds, and only some 48% of all songs listened to in their entirety, Pachet said.
What the data showed, however, was that the skip profile for each song is the same every day, every week and every month for everyone included in the study.
“This means that skipping is not people centered, but it is because of the song, it is the signature of the song,” Pachet said.
The research showed a correlation between skipping and the structure of the music piece. With this information, he said, musicians will be able study how people react to their music and will be able to develop compositions that are better-received.
Music makers would be able to “manipulate their audience in a very precise way,” Pachet said. And “that is going to change a lot of things in future,” he said.
In addition, Pachet said, with the advancement of the use of AI tools in music, regulators may need to redefine the concept of what is considered “original” content, and perhaps create new copyright laws and a new royalties system for music that is reinvented by AI, but based on the remixing and reworking of originals.
 The album was streamed by 12 million users and music critics liked it, with the BBC calling it “the world’s first good robot album.”
Pachet’s work at Spotify has had him mixing genres — such as folk songs with gospel or jazz — to come up with new orchestrations that are very different from the originals. In another twist, his team might blend the rhythm of one song with the harmony of a second to create a “completely brand-new piece” of music, he said.
Critically, however, said Pachet, the decision of what to mix with what is still done by a human artist, and not by a machine. “That is the responsibility of the artist,” he said, and that will remain the difference between musical art and robotically developed music.

François Pachet, director of the Spotify Creator Technology Research Lab, playing the guitar with The Beatles featured in the background at the TechnoArt 2019 conference in Tel Aviv, Nov. 10, 2019 (Shoshanna Solomon/Times of Israel)

Outside of composition, artificial intelligence tools are used to gain insights into how consumers listen to music, Pachet said. In a study published in March, Pachet and two other researchers talk about the phenomenon of “music skipping.”
Online streaming services are a preferred way to consume music. But because there are so many songs available via these services, consumers tend to “skip” quickly from one song to another. This has allowed the researchers to create a “skip profile” — which shows how long people actually listen to music before skipping to the next song.
“Skipping is a crucial feature in understanding modern listening behaviors,” the researchers say in the paper. “For the first time in the history of musicology, researchers can systematically collect and analyze massive amounts of data about music listening behavior.”
Research data presented in the study shows that a quarter of all streamed songs are skipped within the first five seconds, 29% in the first 10 seconds, and 35% in the first 30 seconds, and only some 48% of all songs listened to in their entirety, Pachet said.
What the data showed, however, was that the skip profile for each song is the same every day, every week and every month for everyone included in the study.
“This means that skipping is not people centered, but it is because of the song, it is the signature of the song,” Pachet said.
The research showed a correlation between skipping and the structure of the music piece. With this information, he said, musicians will be able study how people react to their music and will be able to develop compositions that are better-received.
Music makers would be able to “manipulate their audience in a very precise way,” Pachet said. And “that is going to change a lot of things in future,” he said.
In addition, Pachet said, with the advancement of the use of AI tools in music, regulators may need to redefine the concept of what is considered “original” content, and perhaps create new copyright laws and a new royalties system for music that is reinvented by AI, but based on the remixing and reworking of originals.
 Pachet’s work at Spotify has had him mixing genres — such as folk songs with gospel or jazz — to come up with new orchestrations that are very different from the originals. In another twist, his team might blend the rhythm of one song with the harmony of a second to create a “completely brand-new piece” of music, he said.
Critically, however, said Pachet, the decision of what to mix with what is still done by a human artist, and not by a machine. “That is the responsibility of the artist,” he said, and that will remain the difference between musical art and robotically developed music.

François Pachet, director of the Spotify Creator Technology Research Lab, playing the guitar with The Beatles featured in the background at the TechnoArt 2019 conference in Tel Aviv, Nov. 10, 2019 (Shoshanna Solomon/Times of Israel)

Outside of composition, artificial intelligence tools are used to gain insights into how consumers listen to music, Pachet said. In a study published in March, Pachet and two other researchers talk about the phenomenon of “music skipping.”
Online streaming services are a preferred way to consume music. But because there are so many songs available via these services, consumers tend to “skip” quickly from one song to another. This has allowed the researchers to create a “skip profile” — which shows how long people actually listen to music before skipping to the next song.
“Skipping is a crucial feature in understanding modern listening behaviors,” the researchers say in the paper. “For the first time in the history of musicology, researchers can systematically collect and analyze massive amounts of data about music listening behavior.”
Research data presented in the study shows that a quarter of all streamed songs are skipped within the first five seconds, 29% in the first 10 seconds, and 35% in the first 30 seconds, and only some 48% of all songs listened to in their entirety, Pachet said.
What the data showed, however, was that the skip profile for each song is the same every day, every week and every month for everyone included in the study.
“This means that skipping is not people centered, but it is because of the song, it is the signature of the song,” Pachet said.
The research showed a correlation between skipping and the structure of the music piece. With this information, he said, musicians will be able study how people react to their music and will be able to develop compositions that are better-received.
Music makers would be able to “manipulate their audience in a very precise way,” Pachet said. And “that is going to change a lot of things in future,” he said.
In addition, Pachet said, with the advancement of the use of AI tools in music, regulators may need to redefine the concept of what is considered “original” content, and perhaps create new copyright laws and a new royalties system for music that is reinvented by AI, but based on the remixing and reworking of originals.
 Critically, however, said Pachet, the decision of what to mix with what is still done by a human artist, and not by a machine. “That is the responsibility of the artist,” he said, and that will remain the difference between musical art and robotically developed music. Outside of composition, artificial intelligence tools are used to gain insights into how consumers listen to music, Pachet said. In a study published in March, Pachet and two other researchers talk about the phenomenon of “music skipping.” Online streaming services are a preferred way to consume music. But because there are so many songs available via these services, consumers tend to “skip” quickly from one song to another. This has allowed the researchers to create a “skip profile” — which shows how long people actually listen to music before skipping to the next song. “Skipping is a crucial feature in understanding modern listening behaviors,” the researchers say in the paper. “For the first time in the history of musicology, researchers can systematically collect and analyze massive amounts of data about music listening behavior.” Research data presented in the study shows that a quarter of all streamed songs are skipped within the first five seconds, 29% in the first 10 seconds, and 35% in the first 30 seconds, and only some 48% of all songs listened to in their entirety, Pachet said. What the data showed, however, was that the skip profile for each song is the same every day, every week and every month for everyone included in the study. “This means that skipping is not people centered, but it is because of the song, it is the signature of the song,” Pachet said. The research showed a correlation between skipping and the structure of the music piece. With this information, he said, musicians will be able study how people react to their music and will be able to develop compositions that are better-received. Music makers would be able to “manipulate their audience in a very precise way,” Pachet said. And “that is going to change a lot of things in future,” he said. In addition, Pachet said, with the advancement of the use of AI tools in music, regulators may need to redefine the concept of what is considered “original” content, and perhaps create new copyright laws and a new royalties system for music that is reinvented by AI, but based on the remixing and reworking of originals.",0.1415123911176543,0.4705158876211521
48,https://news.google.com/articles/CBMiYmh0dHBzOi8vd3d3LndidXIub3JnL29ucG9pbnQvMjAxOS8xMi8zMC9kYW4tdGVwZmVyLXJvYmJpZS1iYXJyYXQtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtbXVzaWMtYXJ00gEA?hl=en-US&gl=US&ceid=US%3Aen," Support the news <iframe width=""100%"" height=""124"" scrolling=""no"" frameborder=""no"" src=""https://player.wbur.org/onpoint/2019/12/30/dan-tepfer-robbie-barrat-artificial-intelligence-music-art""></iframe> This show originally aired on April 22, 2019. Art and artificial intelligence. How humans and machines are collaborating on paintings, music compositions and more. Dan Tepfer, pianist, composer and coder. (@tepferdan) Robbie Barrat, 19-year-old artist who works with artificial intelligence. (@DrBeef_) Artnome: ""AI Artist Robbie Barrat And Painter Ronan Barrot Collaborate On 'Infinite Skulls' "" — ""It is early in the year, but the most compelling show for art and tech in 2019 may already be happening. AI artist and Artnome favorite Robbie Barrat has teamed up with renowned French painter Ronan Barrot for a fascinating show that lives somewhere in the margin between collaboration and confrontation. ""The L'Avant Galerie Vossen emailed Robbie last April after seeing his AI nude portraits and asked if he would be willing to fly out to Paris to work with Ronan. Robbie agreed and flew out last July to meet with Ronan, and the two have been working together ever since. The show titled 'BARRAT/BARROT: Infinite Skulls' opens Thursday, February 7th, and literally features an 'infinite' number of skulls. ""For the last two decades, it has been artist Ronan Barrot’s tradition to use the remaining paint on his palette to paint a skull each time he stops, interrupts, or finishes a painting. As it was explained to me, the skulls are like a side process of the main painting, it’s like when you clean out your motor after driving for miles and miles. Ronan now estimates that he has painted a few thousand of these, and this massive visual data set of painted skulls was perfect for AI artist Robbie Barrat to use in training his GANs (generative adversarial networks). ""GANs are comprised of two neural networks, which are essentially programs designed to think like a human brain. In our case, we can think of these neural networks as being like two people: first, a 'generator,' whom we will think of as an art forger, and second, as a 'discriminator,' whom we will think of as the art critic. Now imagine that we gave the art forger a book with 500 skulls painted by Ronan as training material that he could use to create a forgery to fool the critic. If the forger looked at only three or four of Ronan’s paintings, he may not be very good at making a forgery, and the critic would likely figure out the forgery pretty quickly. But after looking at enough of the paintings and trying over and over again, the forger may actually start producing paintings good enough to fool the critic, right? This is precisely what happens with GANs in AI art."" WBGO: ""With 'Natural Machines,' Pianist Dan Tepfer Brings Coding and VR Into an Improv Arena"" — ""Dan Tepfer is a jazz pianist whose newest project, Natural Machines, brings virtual reality and algorithmic learning into the realm of the concert hall. He appears next Tuesday at National Sawdust in Brooklyn, where his audience will join him in an immersive audiovisual saga. ""What kind of music J.S. Bach might be making today? Tepfer has long been obsessed with that composer’s famous Goldberg Variations — making it the subject of his 2011 album, Golberg Variations / Variations — and recently started thinking about how the same idea could propel a state-of-the-art musical idea. ""Tepfer, who was formally trained as an astrophysicist but self-taught as a computer programmer, writes his own code for a Yamaha Disklavier, enabling the digital player piano to react to his improvisations in real time."" This program aired on December 30, 2019. Meghna Chakrabarti Twitter Host, On PointMeghna Chakrabarti is the host of On Point. More… Brian Hardzinski  Associate Producer, On PointBrian Hardzinski was a producer with WBUR's On Point More… Support the news",0.16658263305322124,0.5155042016806721
49,https://news.google.com/articles/CBMiW2h0dHBzOi8vd3d3LnNwaW4uY29tL2ZlYXR1cmVkL2FpLW11c2ljLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWZlYXR1cmUtaG9sbHktaGVybmRvbi15YWNodC_SAQA?hl=en-US&gl=US&ceid=US%3Aen,,0.0,0.0
50,https://news.google.com/articles/CBMihQFodHRwczovL3d3dy5qYXBhbnRpbWVzLmNvLmpwL2N1bHR1cmUvMjAxOS8xMi8xMy9lbnRlcnRhaW5tZW50LW5ld3MvYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtcHV0cy1maW5hbC1ub3Rlcy1iZWV0aG92ZW5zLTEwdGgtc3ltcGhvbnkv0gEA?hl=en-US&gl=US&ceid=US%3Aen,"5 M/CLOUDY 


 AFP-JIJI BERLIN – A few notes scribbled in a notebook are all that German composer Ludwig van Beethoven left of his 10th Symphony before his death in 1827. Now, a team of musicologists and programmers is racing to complete a version of the piece using artificial intelligence, ahead of the 250th anniversary of his birth next year. 


googletag.cmd.push(function() { googletag.display('div-gpt-ad-1499653692894-0'); });

 “The progress has been impressive, even if the computer still has a lot to learn,” said Christine Siegert, head of archives at Beethoven House in the composer’s hometown of Bonn. Siegert said she was “convinced” that Beethoven would have approved since he too was an innovator at the time, citing his compositions for the panharmonicon — a type of organ that reproduces the sounds of wind and percussion instruments. And she insisted the work would not affect his legacy because it would never be regarded as part of his oeuvre. The final result of the project will be performed by a full orchestra on April 28 next year in Bonn, a centerpiece of celebrations for a composer who defined the romantic era of classical music. Beethoven, Germany’s most famous musical figure, is so loved in his homeland that a duty to prepare for the anniversary was written into the governing coalition’s agreement in 2013. The year of celebrations will begin on Monday, Dec. 16 — believed to be his 249th birthday — with the opening of his home in Bonn as a museum after extensive renovation. Beethoven began working on the Tenth Symphony alongside his Ninth, which includes the world-famous “Ode To Joy.” But he quickly gave up on the Tenth, leaving only a few notes and drafts by the time he died at age 57. In the project, machine-learning software has been fed all of Beethoven’s work and is now composing possible continuations of the symphony in the composer’s style. Deutsche Telekom, which is sponsoring the project, hopes to use the findings to develop technology such as voice recognition. The team said the first results a few months ago were seen as too mechanical and repetitive but the latest AI compositions have been more promising. Barry Cooper, a British composer and musicologist who himself wrote a hypothetical first movement for the Tenth Symphony in 1988, was more doubtful. “I listened to a short excerpt that has been created. It did not sound remotely like a convincing reconstruction of what Beethoven intended,” said Cooper, a professor at the University of Manchester and the author of several works on Beethoven. “There is, however, scope for improvement with further work.” Cooper warned that “in any performance of Beethoven’s music, there is a risk of distorting his intentions” and this is particularly the case for the Tenth Symphony because the composer had left only fragmentary material. Similar AI experiments based on works by Bach, Mahler and Schubert have been less than impressive. A project earlier this year to complete Schubert’s Eighth Symphony was seen by some reviewers as being closer to an American film soundtrack than the Austrian composer’s work. 

						Click to enlarge
					 
music, Germany, computers, AI, ludwig van beethoven  Michi no eki: Japan's rest areas are kings of the road			  Tokyo Olympics will most likely go ahead as planned, but coronavirus could still damage attendance			  Harnessing the power of AI: Japanese delivery firms and restaurants look to tech to boost businesses			  Delicacies and the devout in northern Tsuruoka			  Top 5: Tokyo's most affordable Michelin-starred restaurants			  DEEP DIVE			 Episode 42: Will the coronavirus cancel the Tokyo Olympics?			 LAST UPDATED: Mar 12, 2020			  Just for kicks: Japan’s sneaker obsession rebounds  LAST UPDATED: Feb 29, 2020  
				Directory of who’s who in the world of business in Japan
			 LAST UPDATED: Mar 9, 2020			  Sponsored contents planned and edited by JT Media Enterprise Division. 
広告出稿に関するおといあわせはこちらまで
 
Read more
   The Japan Times LTD. All rights reserved.",0.12988505747126433,0.42241379310344823
51,https://news.google.com/articles/CAIiEHHAC_YGeJux88xLPQcJbsgqGAgEKg8IACoHCAowlOzSATCaiDUw672eBg?hl=en-US&gl=US&ceid=US%3Aen,"From painting to poetry, music to math, we can view AI as a collaborator rather than a competitor. Music lovers gathered in a London theater one night in March to take part in an unusual event: half classical concert, half futuristic experiment. Their task was to listen to music that had been composed partly by Bach and partly by artificial intelligence — and try to guess which parts were which. Throughout the performance, the audience members voted by holding up cards with a blue human face on one side and a red robot face on the other.  “It was quite shocking,” Marcus du Sautoy, an Oxford mathematician who masterminded the event, told me. “There were moments when I think Bach would have turned in his grave! Moments when Bach was playing and people were saying it was the AI.”  Experiments like these are becoming more common as researchers try to create AI tools that can generate music — and paintings, and poetry — that’s just as compelling as the human-made kind. They often rely on machine learning, a type of AI that involves feeding computers example after example of something until they learn to pick out the patterns in it and create their own version.  In the case of the London event, the AI had been fed enough of Bach’s music that it was able to learn and then mimic the composer’s signature style, fooling the audience.  The AI researchers designing these tools aren’t doing it for the fun of tricking people. They’re trying to prove that they can take AI farther than we’d previously thought possible — that they can make machines creative, just like human beings.  And some creative humans are happy to have AI join the art world. They see it not as a threatening interloper, but as a potential collaborator that can get them out of their usual ruts and spur them to think in new directions.  Du Sautoy is among those optimists. In his new book, The Creativity Code: Art and Innovation in the Age of AI, he examines the likelihood that AI will become creative in its own right, where creativity is defined as “coming up with something that is new, that is surprising, and that has value.” He explores the way AI is changing music, visual art, literature, and math.  Refreshingly, he ends up arguing that we should see the relationship between AI and humans not as adversarial, but as collaborative. As a novelist, I wanted to speak to him about what to do when we nevertheless feel unmoored by the computerization of everything. A transcript of our conversation, lightly edited for length and clarity, follows. In popular media, the relationship between AI and humans is often pitched as adversarial rather than collaborative. Why is that?  The public is in an uncertain place about where the future is going. One of the frightening things is that the AI revolution is very fast — within one decade, you’ll see a lot of jobs disappear. So everyone is going through a slight existential crisis. And the movies love feeding this dystopian Terminator image of AI.  But we’ve all felt the one thing that’s uniquely human is our creativity. Now we’re having to ask, what if AI can approach even that? That’s a real challenge.  I tend to think that if we come to realize human creativity isn’t so unique, there’s actually something potentially very freeing about that. Just like the Copernican revolution made us realize not everything revolves around us and the Darwinian revolution made us realize we’re actually a lot like other animals — that can be very unmooring, but it can also take away a lot of human-centric ego, which could be a healthy thing for us and the planet. Absolutely! That’s one of the strands I wanted to pull out: People think art is something very mystical — that there’s something appearing out of nothing, the creative genius. I wanted to reveal that a lot of creative acts do have structure and pattern and algorithms and logic.  Especially with music. Many people think emotions are just being spilled out onto the page, but any composer will tell you, “I’m actually doing something very structured, and the emotion arises out of the controlled acts I’m using in creating a piece of music.”  Do you think an audience would be less impressed by AI-generated music if you primed them in advance by telling them which parts were composed by AI?  I’m thinking of this part in your book where you describe a critic who reacted very negatively to Microsoft’s 2016 attempt to have AI create a painting in the style of Rembrandt. He called it “a horrible, tasteless, insensitive, and soulless travesty.” I had the feeling the critic wouldn’t have reacted as negatively if he hadn’t known the painting was made by AI. That’s what was interesting about Art Basel [a major contemporary art fair in 2016]. There, they didn’t tell people the art was created by AI, and the people had a positive emotional reaction to it — actually more positive than for the art produced by humans!  I think knowing [an artwork] is produced by AI does color your reaction to it. And perhaps justifiably so, because you want to feel that you’re connecting to another human being. And so if you find out there is no human being behind it, you do feel cheated.  Should we be trying to override that part of our brain, though? The part that says, “Hey, I expected to be communing with another human’s soul here”? Part of me wonders if it makes sense to be super pragmatic about art and our interactions with it: Maybe if I enjoy art made by AI, I should just learn to value my positive emotional response and try to divorce that from any presuppositions about the intentionality behind the art. In literary theory, there’s this whole idea of the intentional fallacy — basically, the intention of the author isn’t actually what matters.  It’s worth remembering that for the moment, AI is producing things that still connect to our human creativity because it’s learning from our own art. So we are still having a reaction to human creativity but through this new filter. So even though you might feel cheated when you realize something was made by AI, you’re responding to something that ultimately had its source in the human.  That’s true. I think the problem arises when we think we’re having a human interaction and then we find out it’s not that and we feel tricked. But if we know from the get-go that the art is generated by AI, maybe we can enjoy it for what it is.  I actually had this experience of feeling tricked when I was reading your book. You sneakily put in a 350-word paragraph that’s written by AI, and only later cop to the fact that it was AI-written. I knew when I was reading it that something was off because it had typos and it didn’t fully make sense. And then later, when you explained why, I felt so prickly about it! I yelled, “I knew something was wrong!” and felt so vindicated.   Interesting! Yeah, I had to fight to get my copy editor to not correct those glitches.  Aside from playing those little tricks on your readers, you do sound in the book like you sincerely believe machine learning can revolutionize creativity. As a mathematician, the work you do is fundamentally creative (even though math isn’t classically seen as one of the creative arts) and you talk a lot about how you’ve been feeling nervous as you watch AI horning in on the territory of creativity. Did you write this book because you were having an existential crisis? I did, yes! The whole thing was initiated by this sense that the new AI that’s emerging may well be able to do the thing I never thought it would be able to do, which is to create mathematics. That word “create” is what I always thought protected my subject from computers. I didn’t regard computers as at all creative, and creativity is a huge part of doing mathematics. But then I started to see AI being creative in the game of Go [in 2016, when Google’s DeepMind team challenged a human champion to a series of matches against its AlphaGo program]. I watched the Go matches on YouTube obsessively because that game was always one of my protective shields against computers doing math — everyone always said computers will never be able to play Go. As I watched AI besting humans and actually showing them how to play the game in a new way, I thought, this thing is creating its own genuinely surprising moves. This is a significant moment. For now, though, it seems like a lot of codes attempting to make art are pretty good at mimicking human artists on the local level (say, a few notes at a time), but they fail at generating a larger structure that feels satisfying (like a complete work of art). Why is that? The way machine learning is working at the moment is, it’s doing quite a local analysis on text or music or pixels. So, for example, the jazz Continuator [an algorithmic composer created by François Pachet that learns your musical style as you play and continues your melody in real time] hears a few notes and then produces sound based on an analysis of the music it’s heard up to that point. That level of coding doesn’t seem able to get at a global arc or narrative structure. I think that’s why when you turn to literature, where AI has been most successful is in poetry. Because poetry is a gnomic form. It’s got a lot of pattern in it — rhythm and rhyme — but also I have to do a lot of work as the reader, I have to bring my understanding to the poem. AI is quite successful in poetry because it’s able to create something that leaves enough ambiguity so the reader can use a lot of their creativity to bring the poems to life. But when you turn to [longform] literature, AI has been quite unsuccessful. It can do pretty good short-form prose, sound pretty convincing with a bit of Harry Potter, but not longer prose. I don’t think that means it’s unsolvable; it’s just a harder thing to tease out. We know there are formulas for narratives in film, and archetypal stories in the novel, so in theory, why couldn’t you take one of these templates and use a good text generation algorithm to fill the gaps? Yeah, and it’s actually been said there are really only six main story arcs in all of literature. Given the possibility of AI mastering them, do you think that as a novelist, I’ll be out of a job in a few decades? Rather than both of us being out of a job, what I hope is that maybe we’ll be able to push ourselves in interesting ways as the AI becomes a partner or tool to extend our own creativity. I get so stuck in ways of thinking and sometimes I need something to kick me out of that. AI can help us behave less like machines and more like creative humans. That’s the most exciting thing. Do you think machine learning will prove an important new theorem in the next few decades? I actually think it’s way off. By the end of [writing] the book, I was encouraged because I felt I’m still very much in the game. For AI to actually come up with new ideas in math — I think we’re still a long way from that. DeepMind actually released a paper a couple weeks ago where AI had been trained on mathematical ways of thinking and then took a school-level paper and it didn’t even get a pass on it. But we’ve all got to start somewhere.  What you do as a novelist is very interesting to me because I think we actually come much more hardwired for language than we realize. The evidence suggests that a child learns language with exposure to very little data, which chimes in with Chomsky’s idea that we do come preprogrammed. And that preprogramming is millions of years of evolution. Whereas we haven’t been through such an evolutionary process in creating our mathematical language. So math is something that AI could perhaps fast-track, much quicker than being able to tell the stories that you tell.  You mean my job is a bit safer than yours?  I think it might be, yes.  Great. Let’s stop there so I don’t have to have an existential crisis. ",0.16355502481811382,0.4956681058906188
52,https://news.google.com/articles/CBMiX2h0dHBzOi8vdGVjaGNydW5jaC5jb20vMjAxOS8xMi8wNS93aHktYXdzLWlzLXNlbGxpbmctYS1taWRpLWtleWJvYXJkLXRvLXRlYWNoLW1hY2hpbmUtbGVhcm5pbmcv0gFjaHR0cHM6Ly90ZWNoY3J1bmNoLmNvbS8yMDE5LzEyLzA1L3doeS1hd3MtaXMtc2VsbGluZy1hLW1pZGkta2V5Ym9hcmQtdG8tdGVhY2gtbWFjaGluZS1sZWFybmluZy9hbXAv?hl=en-US&gl=US&ceid=US%3Aen,"Earlier this week, AWS launched DeepComposer, a set of web-based tools for learning about AI to make music and a $99 MIDI keyboard for inputting melodies. That launch created a fair bit of confusion, though, so we sat down with Mike Miller, the director of AWS’s AI Devices group, to talk about where DeepComposer fits into the company’s lineup of AI devices, which includes the DeepLens camera and the DeepRacer AI car, both of which are meant to teach developers about specific AI concepts, too. The first thing that’s important to remember here is that DeepComposer is a learning tool. It’s not meant for musicians — it’s meant for engineers who want to learn about generative AI. But AWS didn’t help itself by calling this “the world’s first machine learning-enabled musical keyboard for developers.” The keyboard itself, after all, is just a standard, basic MIDI keyboard. There’s no intelligence in it. All of the AI work is happening in the cloud.  “The goal here is to teach generative AI as one of the most interesting trends in machine learning in the last 10 years,” Miller told us. “We specifically told GANs, generative adversarial networks, where there are two networks that are trained together. The reason that’s interesting from our perspective for developers is that it’s very complicated and a lot of the things that developers learn about training machine learning models get jumbled up when you’re training two together.” With DeepComposer, the developer steps through a process of learning the basics. With the keyboard, you can input a basic melody — but if you don’t have it, you also can use an on-screen keyboard to get started or use a few default melodies (think Ode to Joy). From a practical perspective, the system then goes out and generates a background track for that melody based on a musical style you choose. To keep things simple, the system ignores some values from the keyboard, though, including velocity (just in case you needed more evidence that this is not a keyboard for musicians). But more importantly, developers can then also dig into the actual models the system generated — and even export them to a Jupyter notebook.  For the purpose of DeepComposer, the MIDI data is just another data source to teach developers about GANs and SageMaker, AWS’s machine learning platform that powers DeepComposer behind the scenes. “The advantage of using MIDI files and basing out training on MIDI is that the representation of the data that goes into the training is in a format that is actually the same representation of data in an image, for example,” explained Miller. “And so it’s actually very applicable and analogous, so as a developer look at that SageMaker notebook and understands the data formatting and how we pass the data in, that’s applicable to other domains as well.”  That’s why the tools expose all of the raw data, too, including loss functions, analytics and the results of the various models as they try to get to an acceptable result, etc. Because this is obviously a tool for generating music, it’ll also expose some of the data about the music, like pitch and empty bars. “We believe that as developers get into the SageMaker models, they’ll see that, hey, I can apply this to other domains and I can take this and make it my own and see what I can generate,” said Miller. Having heard the results so far, I think it’s safe to say that DeepComposer won’t produce any hits soon. It seems pretty good at creating a drum track, but bass lines seem a bit erratic. Still, it’s a cool demo of this machine learning technique, even though my guess is that its success will be a bit more limited than DeepRacer, which is a concept that is a bit easier to understand for most since the majority of developers will look at it, think they need to be able to play an instrument to use it, and move on. Additional reporting by Ron Miller. AWS announces DeepComposer, a machine-learning keyboard for developers  Why AWS is building tiny AI race cars to teach machine learning  ",0.16153716260099238,0.43635661029278044
53,https://news.google.com/articles/CAIiEPkfK11tk0_d6iu6y0PPdHkqFggEKg4IACoGCAowl6p7MN-zCTDMwRU?hl=en-US&gl=US&ceid=US%3Aen,"When you think of great songwriting teams, music and artificial intelligence aren’t an obvious mix, but a number of startups are bringing smart tech into the studio 
Hazel Davis 

Tue 18 Jun 2019 12.43 BST


Last modified on Tue 6 Aug 2019 16.14 BST

 Mention the words “AI” and “music” in the same sentence and you may attract some tuts and eye rolls, as people wax lyrical about the good old days. If robots are making our call centre jobs redundant, then at least let us have our Sunday jam sessions. But music, like anything else, has to move on: “In a world of personalisation and on-demand services, music is one of very few remaining static artefacts,” says Ken Lythgoe, head of business development at creative AI technology company MXX. The company has created what it says is the world’s first AI tech that allows individual users to instantly edit music to fit their own video footage, complete with rises and fades. AI doesn’t need to be the enemy of music, says Lythgoe: “There are two types of AI – the AI that is here to replace us and AI that is here to empower us – we are definitely in the empowerment camp. We are not about computers replacing musicians or editors; we are firm believers in the creative process.” “Businesses and individuals want to alter their experience with music, depending on their needs,” says Lythgoe. “They want a world where music can be adapted to perfectly fit certain experiences – such as gym workouts and runs, gaming, user-generated content and virtual or augmented-reality experiences.” MXX was founded in 2015 by AI specialist and composer Joe Lyske and Philip Walsh, former financial services CEO and founder of investment vehicle and AI incubator, Time Machine Capital. Lyske’s research included devising a tool that generated film scores that fooled 50% of professional film composers into thinking they were listening to a fellow professional.  MXX’s tech listens to music and creates metadata of its understanding of it. This data includes where it can edit in and out of sections, as well as what the sections might mean to a human, such as “building tension”, “climax”, “chorus” and “verse”. When the user provides the brief for the music they want, the AI can edit the original track to fit the brief. Lythgoe explains: “Just as a silent film pianist would adapt the music to the mood of the audience or speed of the projector, we can have our favourite music adapt to our car journey time, the pace of our run or gym workout or the way the cat falls off the sofa in our social media post.” The technology’s target users are advertising agencies, game creators, production companies, music supervisors, labels and sync agencies, a market that, says Lythgoe, is crying out for tech like this: “There are 400,000 hours of video uploaded to YouTube every 60 seconds alone. There aren’t enough music editing professionals in the world to cope with that kind of content.” AI-generated music does not take jobs, says Lythgoe: “In fact editors say the opposite. They often have to edit dozens of versions of different music to adverts for free, only being able to charge for the final version. This tech allows them to set up a template to sell to their customers, that the customer can play with all day till they have found the track they are looking for.” Valerio Velardo is co-founder and CEO of Berlin-based Melodrive, which is using AI to automatically generate soundtracks for virtual reality and video games in real time. A classically trained musician, he has a PhD in music and AI, and a degree in astrophysics. Melodrive (which has five staff) essentially composes an infinite stream of original, “emotionally variable” music in real time. Velardo explains: “In a video game, if a player encounters more than 10 enemies, the music can move from being ‘almost angry’ to ‘completely crazy’.” Far from putting musicians and composers out of business, Velardo says tech like Melodrive is “more of a sparring partner. We are just building tools to be more creative with and this is no different from the evolution of the piano from the harpsichord, to things like [digital audio workstation] Cubase.” The UK’s AI sector could be worth £232bn to the UK economy by 2030, according to a report (pdf) from PwC, and back in 2018 the government announced a £1bn deal to put the UK at the forefront of the AI industry. UK Music’s Measuring Music 2018 report puts the UK music industry’s gross value added (GVA) at £4.5bn.  “AI and music is certainly a growth area and likely to continue to be so,” says Craig Hamilton, research fellow at the Birmingham Centre for Media and Cultural Research: “A number of startups have attracted investment over the last couple of years – such as Amper and AI Music – and there is also a lot of activity within many of the larger tech and entertainment firms, notably Sony, Google, Spotify, IBM and Facebook. This should not be surprising – AI systems rely on large amounts of data to build and train models and on powerful computational, connected systems to process data and deploy the results.” Since the turn of the century, music consumption has increasingly shifted to systems capable of generating and analysing data on both user activity and musical works, says Hamilton, “and there has also been an exponential growth in the scale and power of computational processing systems.” In that sense, he adds: “AI and music together does have a certain air of inevitability.”",0.12262051734273953,0.4743195179306291
54,https://news.google.com/articles/CBMifGh0dHBzOi8vd3d3Lm1vdGhlcmpvbmVzLmNvbS9tZWRpYS8yMDE5LzAzL3doYXQtd2lsbC1oYXBwZW4td2hlbi1tYWNoaW5lcy13cml0ZS1zb25ncy1qdXN0LWFzLXdlbGwtYXMteW91ci1mYXZvcml0ZS1tdXNpY2lhbi_SAQA?hl=en-US&gl=US&ceid=US%3Aen,"
Clive ThompsonMarch/April 2019 Issue Ed Newton-Rex grew up immersed in music. As a child, he sang in the King’s College Choir in Eng­land and played piano. He went on to earn a music degree, and one of the things he studied was, “Why do people like music?” he told me. The answer, he learned, is that there’s no simple answer: It’s a deeply complex stew of art, timbre, and emotion. And math. As Pythagoras discovered about 2,500 years ago, music is deeply mathematical, and it’s possible to represent melody using numbers and ratios. After finishing his undergraduate degree in 2010, Newton-Rex went to visit his girlfriend, who was studying at Harvard. He sat in on a coding lecture and became enraptured with the idea of writing software that could generate songs by harnessing the machine’s ability to semi-randomly recombine numbers. “Why haven’t computers been able to do this yet?” he wondered. Over the next year, he set out to create a composing machine. He taught himself enough to code up a prototype that would create songs based on a set of simple rules. Before long, his system, Jukedeck, was cranking out instrumental tunes good enough to convince some investors to back him. He then hired programmers to rebuild his system using “deep learning” neural networks, the hot new artificial-intelligence technique. Neural nets can, in effect, learn on their own. Newton-Rex would feed thousands of melodies his team composed—pop, blues, folk, and other genres—into the system. The neural net would decode the deep patterns in the music and crank out new melodies based on what it had intuited. Jukedeck has since penned more than 1 million songs, and in the past few years several similar firms—Amper in New York, Popgun in Australia, and AIVA in Luxembourg—have emerged to join this weird new industry. Their tools are point-and-click easy: Pick a genre, a “mood,” and a duration, and boom—Jukedeck churns out a free composition for your personal project or, if you pay a fee, for commercial use. Songs composed by Jukedeck and its ilk are already showing up in podcasts, video games, and YouTube content, “from explainer videos to family holiday videos to sports videos,” says Patrick Stobbs, Jukedeck’s co-founder. For years, DIY video makers have licensed tunes from huge “libraries” of Muzak-y stuff produced by humans. Now, AI offers fresh compositions at the press of a button. The songs can be surprisingly good. I generated a 90-second folk-pop tune on Jukedeck using the “uplifting” option, with bass, drums, synthesizers, and jangly artificial guitar. The robot composer even threw in a few slick little melodic breaks. As a part-time musician, I’ve composed and recorded enough to be impressed. The tune wasn’t brilliant or memorable, but it easily matched the quality of human work you’d hear in videos and ads. It would take a human composer at least an hour to create such a piece—Jukedeck did it in less than a minute. All of which raises some thorny questions. We’ve all heard about how AI is getting progressively better at accomplishing eerily lifelike tasks: driving cars, recognizing faces, translating languages. But when a machine can compose songs as well as a talented musician can, the implications run deep—not only for people’s livelihoods, but for the very notion of what makes human beings unique. Newton-Rex and his fellow pioneers are, historically, in good company. For centuries, musicians have been mesmerized by the idea of writing algorithmically, usually by finding some device to add randomness to their craft. In the 18th century, composers played Würfelspiel, a dice game, to generate compositions. This became so common that one composer even wrote a satire about an artist who splattered paint on musical scores and tried to play whatever emerged. In Amsterdam, Dietrich Winkel, inventor of the common metronome, built a mammoth automated pipe organ that recombined melodies using two barrels that interacted on a “random walk.” In the 1930s, with the help of Léon Theremin, Henry Cowell created the Rhythmicon, a robotic drum machine. “He could hear these rhythms that he was theorizing, basically, that no human could play,” explains Margaret Schedel, a professor of music at New York’s Stony Brook University. The innovations picked up again in the 1960s, as the first generation of computer nerds coaxed room-size mainframes to generate simple melodies. A couple of decades later, composition tools arrived on the first blast of personal computers—with Laurie Spiegel’s Music Mouse software, you could wave your mouse around and hit keys to influence the algorithm, making you a partner in your Mac’s auditory creation. There are two forces propelling today’s robotic music explosion. One is the rise of neural nets, a technique AI scientists beavered at for decades before enjoying key breakthroughs in the early 2010s. Companies like Google have released free, easy-to-use neural net code, so now nearly any competent programmer can dabble. And neural nets allow for subtler compositions than past technologies did. Rather than telling the system precisely how to compose a tune or a beat, the coder simply gathers thousands of examples and lets the system make its own rules. The second factor is demand. The US market for background music hit $660 million in 2017, up 18 percent from two years earlier, according to industry consultant Barry Massarsky, and preliminary figures show 11 percent growth in 2018. Composers worldwide make ends meet by contributing to the tune libraries used by You­Tubers, corporations, radio shows—whoever needs a sonic backdrop. This is basically the audio version of the market for stock photos: The songs are predictable, often hackneyed, but good enough for a how-to makeup video or sports podcast. RELATED: You Will Lose Your Job to a Robot—and Sooner Than You Think  AI will seriously disrupt that labor market. Background tracks are pretty algorithmic even when humans write them: You introduce one motif, then another, layer them together, rinse, and repeat. It’s what Amper founder Drew Silverstein, a former Hollywood composer, calls “functional” music. “We don’t necessarily care how it was created and where it came from,” he says. “The most extreme example is elevator music, right? It’s music that is serving a purpose.” Here, Amper radically outperforms humans. “Amper is not a music tool. It’s not a music solution,” Silverstein insists. “Amper is an efficiency tool. Fundamentally.” Given the choice between paying a slow human or asking a lightning-fast, almost-free bot to generate a purely functional soundtrack, which would you choose? But the debate extends beyond elevator music. As AI capabilities improve, it’s possible­—probable even—that the songs will become good enough that we’d opt to listen to them, for instance, while working or driving. The economics are enticing for streaming services. Imagine Spotify self­generating thousands of hours of chill-out ambient tracks with no need to pay human composers a dime. This isn’t far-fetched. In November 2017, Jukedeck churned out 11 songs for the Slush tech conference after-party. Pretty good ones, too—these songs would hold up next to the human-created, endless-loop instrumentals that are already racking up millions of listens on streaming sites. In July 2017, Spotify hired a major music-AI scientist, François Pachet, with the goal, it says, of developing a machine that can assist musicians. Whenever we ponder the impacts of auto­mation, there are dismal prophecies and sunny ones. The optimists argue that, sure, AI will destroy some jobs, but it will create new ones that pay better and require more creative smarts. Okay, the pessimists reply, but those jobs are never plentiful enough to employ the hordes hurled out of work, and rarely do they materialize fast enough. The entrepreneurs behind the one-click compositions, as you might imagine, mostly fall into the first camp. Their efforts may erode prospects for low-end, entry-level composers, they say, but they will never eliminate the need for top talent, the people writing complex scores for movies, TV, and videos—or just songs we want to listen to. “I don’t think any of these systems are anywhere near that place, and I’m glad that they’re not,” Newton-Rex says. “We as a company don’t think that should be the aim. We don’t think that would be a good thing for musicians.” His AI tools can’t understand context or purpose: “It’s like having a composer who has no other experience in life except reading 1,000 pieces of music and then trying to come up with something similar. They won’t know what they’re doing. They won’t be able to bring any emotion, any life experiences, into it. They won’t be able to cross-pollinate ideas from other fields.” As Adam Hibble, creator of Popgun’s music-writing tool, puts it, “This AI has no idea what’s culturally relevant or what is politically relevant or whatever it is that is currently important in the zeitgeist. It’s a mindless but very intelligent music creation system.” Humans, of course, will need to adapt. The ability to generate a three-minute instrumental probably won’t cut it anymore. To feed their families, composers likely will have to move up the food chain and do work that requires collaboration, stuff bots can’t achieve. “Musicians and composers, your job will not exist in five years,” Silverstein says. “Your career certainly will.” But AI music poses other gnarly ethical and philosophical questions. Bob Sturm, an accordion player and computer scientist in Stockholm, is a fan of traditional Irish folk, so in 2017 he trained a neural net on 23,000 Irish tunes and had it crank out more than 60,000 new ones. He then asked composer Daren Banarsë to plow through the output, and while the majority of the songs were uninspired (or outright duds), perhaps 1 in 5 were quite good. “Some of them sound 100 percent like traditional Irish tunes—I’m confident no one could tell the difference,” Banarsë told me in an email. “And there is the occasional one with that combination of musical quality and interesting character, a special tune that could become a classic.” Sturm and Banarsë recorded an album of human musicians performing the best of their AI creations and sent it to reviewers as the Ó Conaill Family and Friends. “No one suspected a thing; it actually got great reviews,” Banarsë says. When they later emailed the reviewers to explain the genesis of the songs, most just shrugged. Hey, a good song’s a good song, right? But some members of the Irish folk crowd objected on the grounds that AI-generated songs are inherently cannibalistic, each one derived from the creativity of the people who wrote the originals. Sturm understands their misgivings: “What gives us a right to take 23,000 tunes that have been collected by hundreds of volunteers online for one purpose and to use them in a completely different purpose?” There’s also some philosophical weight here, because musical pursuits—historically, anyway—have always seemed like quintessentially human activities. Our melodic creations are deeply tied into our everyday emotional lives and woven into the ceremonies of civilization. This is why the prospect of automating them can seem unsettling, even depressing. There’s a bit of bleakness in realizing that something we so often associate with soulfulness and spirituality can be spewed out by a computer. Yet in one sense, the neural nets are mere­ly mimicking the way humans compose. We, too, consume hundreds or thousands of songs over a lifetime, intuit patterns, and recombine our knowledge into something new. We sample, we steal, and we transmogrify. Our creativity, too, is built on the creativity of those who came before. But when a machine does this, it can feel like an impersonal, even vampiric act. As of now, no commercial AI system is good enough to create, by itself, a half-­decent symphony, or even an entire pop song with words. (Some programmer-­artists have tried robo-generating lyrics with limited success.) So if you wanted to draw a Rubicon between human and computer creativity, that’d be it: a hit song at the push of a button. Humanity’s last stand! But there’s a vanguard of artists using AI tech to complement, not replace, their composing skills: neural nets as writing partner. Singer Taryn Southern built an online audience starting in 2008 for comedic songs she posted on YouTube, but “I was never an adept musician,” she told me. So she had to coax musician friends to create the backing tracks or license instrumental music. After she produced a dozen or so videos that way, the human logistics were getting to be a bit much. Then in 2017, Southern discovered the AI tools and thought, “Why not have them make the music?” She began using Jukedeck and Amper for her soundtracks—“a lot of the cinematic pop.” She also had AIVA compose a classical piano track for her song “Lovesick.” AIVA is trained on work by the major composers of the 18th and 19th centuries, so “it’s kind of like collaborating with Beethoven!” Southern jokes. And she has explored more complex experimental tools, such as the Watson Beat, with IBM programmer Anna Chaney. Feed the Watson Beat a few bars of a melody and it generates a recommendation of where that melody ought to go. After assembling dozens of snippets of music and sounds, Southern fired up her editing software, extracting a bit of melody here, a chunk of a drum pattern there, to craft an album’s worth of instrumental backups for her lyrics. The result: I AM AI, an album of sparkly, synth-heavy tracks. Southern says her project is a harbinger of a cyborg future in which AIs assist human composers rather than replace them. After all, she figures, few people truly want to listen to software-­generated music: “If it was all made by a robot, then it’s just not interesting.” Human-machine collaborations, musicians are also discovering, open up new aesthetic possibilities if only because AI creativity can be rather alien—particularly at this rudimentary phase. Last year, the French pop songwriter Benoit Carré used Flow Machines, an experimental Sony AI, to help him create the album Hello World. He trained the system on melodies he and his collaborators composed and selected. “You are a little bit like an artistic director or a producer, and you have a crazy musician in the room,” he told me. “Most of the time it is crap,” but every so often the machine kicked out a melody he would never have thought of. Carré helped write the lyrics and recorded the album with a group of meatspace musicians. It certainly wasn’t push-button easy. If anything, sifting through the AI’s output for useful, provocative passages was like panning for gold—probably more work than writing everything himself. But the silicon intelligence helped Carré break out of ruts. “In pop music, you know, it is always the same chords,” he says, so to do something new “you have to be surprised, you have to be shaken.” In a sense, we’re still like those dice-throwing artists of the 18th century, trying to goose human creativity with forces that feel outside our control. At the very least, the human-AI collabs will raise new copyright questions. “If you want [a neural net] to make music in the style of the Beatles, you have to feed it Beatles so it understands. So do the Beatles get a royalty?” wonders Scott Cohen, co-founder of the Orchard, a distribution firm for music, film, and video. You could view the process as sampling (for which artists must pay) or merely a source of inspiration (which is free). Or suppose I feed the melody from Ariana Grande’s “thank u, next” into Google’s free Continue app, which, like the Watson Beat, predicts musically plausible next bars. If I then use those bars to create a song, do I owe Grande anything? Such questions will need to be hashed out in the years to come, maybe in Neal Stephensonesque courtroom brawls among record labels—if they still exist. RELATED: Welcome, Robot Overlords. Please Don’t Fire Us?  This is likely the future, in any case. The AI gurus claim their products will help amateurs punch above their weight—crafting songs far beyond their unaided skills. During an internship with Google’s Magenta project, for example, Ph.D. music student Chris Donahue developed an AI system called Piano Genie. Rather than rely on all 88 keys, Piano Genie has just eight buttons, and it generates melodies based loosely on the patterns you key in. It’s kind of like Guitar Hero, if hitting the controller buttons created actual music. “It’s not as hard as playing full piano, but it’s not touching a button and having music spill out,” Donahue told me. “Millions of people downloaded Garage­Band and then never used it,” says Popgun founder Hibble. “Or they used it once and found it too difficult. The promise of a musically intelligent system is that you can reduce that to essentially no previous musical creation skill in order to start making something that you want to share.” AI promises to democratize composing in “the same way Instagram makes everybody a photographer,” Hibble adds, “because it’s now much easier to get something that looks good.” He predicts that his AI tool and others will be incorporated into instruments—actual hardware. “It’s going to be in keyboards; it’s going to be everywhere,” he says. You’ll sit down at an electric piano to riff away with the ghost of Bach guiding you. And that sounds awfully fun, though it, too, raises a disquieting possibility: Would such instruments begin to de-skill young musicians, who might then decide there’s no need to work toward mastery? On the upside, the rise of AI tools could spur entirely new genres. Fresh music technologies often do. The electric guitar gave us rock, the synth helped create new wave, electronic drum machines and samplers catalyzed the growth of hip-hop. Auto-Tune was a dirty little secret of the record industry, a way to clean up bad singing performances, until artists like Cher and T-Pain used it to craft entirely new, wild vocal styles. The next great trend in music could be sparked by an artist who takes the AI capabilities and runs with them. “Someone can make their own and really develop an identity of, I’m the person who knows how to use this,” says Magenta project engineer Adam Roberts. “A violin­—this was technology that when you give it to Mozart, he goes, ‘Look what I can do with this piece of technology!’” exclaims Cohen, the Orchard co-founder. “If Mozart was a teenager in 2019, what would he do with AI?” Subscribe to our free newsletters. Mother Jones was founded as a nonprofit in 1976 because we knew corporations and the wealthy wouldn't fund the type of hard-hitting journalism we set out to do.  Today, reader support makes up about two-thirds of our budget, allows us to dig deep on stories that matter, and lets us keep our reporting free for everyone. If you value what you get from Mother Jones, please join us with a tax-deductible donation today so we can keep on doing the type of journalism 2020 demands. Mother Jones was founded as a nonprofit in 1976 because we knew corporations and the wealthy wouldn't fund the type of hard-hitting journalism we set out to do.  Today, reader support makes up about two-thirds of our budget, allows us to dig deep on stories that matter, and lets us keep our reporting free for everyone. If you value what you get from Mother Jones, please join us with a tax-deductible donation today so we can keep on doing the type of journalism 2020 demands. Sam Van Pykeren Sam Van Pykeren Sam Van Pykeren Matt Cohen Dawn Stover Nathalie Baptiste Will Peischel Abigail Weinberg Tim Murphy Jessica Washington Noah Lanard Tim Murphy Subscribe and we'll send  Mother Jones straight to your inbox. Save big on a full year of investigations, ideas, and insights. Help Mother Jones' reporters dig deep with a tax-deductible donation. 
We have a new comment system!
		We are now using Coral, from Vox Media, for comments on all new articles.
		We'd love your feedback.
	 Inexpensive, too! Subscribe today and get a full year of Mother Jones for just $12. It's us but for your ears. Listen on Apple Podcasts. Subscribe to our free newsletters. 
			Copyright © 2020 Mother Jones and the Foundation for National Progress. All Rights Reserved.Terms of Service
Privacy Policy
  
																						Can you pitch in a few bucks to help fund Mother Jones' investigative journalism? We're a nonprofit (so it's tax-deductible), and reader support makes up about two-thirds of our budget.																	 
We noticed you have an ad blocker on. Can you pitch in a few bucks to help fund Mother Jones' investigative journalism?																	",0.12445644841965907,0.46017032205995445
55,https://news.google.com/articles/CBMiSmh0dHBzOi8vZnV0dXJpc20uY29tL3RoZS1ieXRlL2FpLWNvbXBsZXRpbmctYmVldGhvdmVucy11bmZpbmlzaGVkLXN5bXBob2550gFFaHR0cHM6Ly9mdXR1cmlzbS5jb20vYWktY29tcGxldGluZy1iZWV0aG92ZW5zLXVuZmluaXNoZWQtc3ltcGhvbnkvYW1w?hl=en-US&gl=US&ceid=US%3Aen,"When Ludwig van Beethoven died in 1827, the famous German composer left behind a few musical fragments of what would have been his 10th symphony. Now, a group of musicologists and computer programmers is developing an artificial intelligence to finish the composition — and in April, a full symphony orchestra will perform it live in Beethoven’s birthplace of Bonn, Germany. To train the AI, the team is feeding it a diet consisting of finished Beethoven works, earlier versions of those compositions, and the works of composers who influenced Beethoven. “Take a particular Beethoven work, one for which extensive drafts still exist, like the Eroica Symphony,” project contributor Christine Siegert, who heads up the Beethoven Archive in Bonn, told German broadcaster Deutshe Welle. “If you feed the computer both the sketches and the final product, it can figure out how Beethoven works with sketches and where he goes from there.” After the AI finishes writing the music, a human composer will take over to orchestrate it for the live performance. According to British composer and musicologist Barry Cooper, who wrote his own first movement for Beethoven’s 10th in 1988, the team still has a lot of ground to cover before April. “I listened to a short excerpt that has been created,” Cooper told Agence France Press. “It did not sound remotely like a convincing reconstruction of what Beethoven intended. There is, however, scope for improvement with further work.” READ MORE: Computer is set to complete Beethoven’s unfinished symphony [The Telegraph] More on musical AIs: Here’s What an Album Between a Musician and an “AI Baby” Sounds Like",0.07997835497835497,0.4196428571428571
56,https://news.google.com/articles/CAIiEH340zfadeFO1QfxEXCq_VUqGQgEKhAIACoHCAowzuOICzCZ4ocDMO7xqQY?hl=en-US&gl=US&ceid=US%3Aen,"By Michael Thomsen For Dailymail.com   Published:  22:41 GMT, 17 January 2020   |  Updated:  22:49 GMT, 17 January 2020      33 View  comments  This week Microsoft announced a new collaboration with Björk to create a series of musical compositions with a custom built artificial intelligence tool. The AI will create dynamic new variations of Björk's original arrangements based on the changing weather patterns and position of the sun .  Called ‘Kórsafn,’ which means ‘choir archive’ in Icelandic, the composition will be played continuously in the lobby of Sister City, a hotel in New York's lower east side that opened in the spring of 2019. Björk has created a new AI-driven musical work, called ‘Kórsafn,’ which will play in the lobby of New York's Sister City hotel in the lower east side Described as a ‘generative soundscape,’ the composition combines sounds and motifs from Björk’s personal of choir archives, which she has compiled over the last 17 years as a solo artist. ‘Kórsafn’ isn’t just a series of old sound files, however, Björk revisited her old set of recordings and notes and samples to create a series of new set of arrangements just for the project.  Her new arrangements were performed and recorded live in a studio session by the Hamrahlid Choir, one of Iceland’s most renowned vocal groups.  Microsoft's AI will pick up where Björk's arrangements leave off, creating an endless string of new variations with her work, in response to a live camera feed from the Sister City roof,  The music will dynamically adapt to sunrises, sunsets, and changes in barometric pressure that might signal rain, wind, or snow, creating an all-encompassing mood for guests. The music will adapt dynamically in accordance with changing weather patterns captured by a live video feed taken from the roof of the hotel  Björk says she wrote the music hoping it would 'float through the pinball of artificial intelligence by the grid of bird migrations, clouds, aeroplanes and that voluptuous thing called barometer' Sister City opened in early 2019 and first commissioned Julianna Barwick to write ambient music to play in its lobby Björk's new arrangements were recorded in a live studio session by the Hamrahlid Choir, one of Iceland’s most renowned vocal groups. According to Microsoft, the AI will also learn over time, so that it will continue to create unique new sounds and learn to identify different weather phenomena.  The idea of an original lobby score at Sister City began last year, when the hotel commissioned composer Julianna Barwick to create a similarly-AI driven soundscape for the lobby. Since then, Microsoft says it has made significant improvements to the AI.  Now it can accurately identify different types of clouds, detect individual birds, and distinguish different members of an entire flock of birds.   ‘Kórsafn’ is a new AI-powered musical composition created by Björk and Microsoft that will play in the lobby of the Sister City hotel in New York.  It will change dynamically based on the weather and time of day captured by a live video feed from the hotel's roof.  Björk's full artist's statement describing the project is below:  'an architectural structure downtown manhattan offered me the hand in an AI tango and i accepted the call, i am alert with curiosity waiting the results. i offered them my choir archives, written over 17 years that will float through the pinball of artificial intelligence by the grid of bird migrations, clouds, aeroplanes and that voluptuous thing called barometer ! hudson valley happens to be one of the most bird-trafficked deltas on the planet, i know this of my own experience .... hope you will enjoy this ! warmth björk"" 


      Coronavirus deaths in the UK almost double overnight to 21: Ten more patients die while the number of infected increases by one third bringing the total number to 1,145
    
 Published by Associated Newspapers Ltd Part of the Daily Mail, The Mail on Sunday & Metro Media Group",0.11896694214876033,0.4612947658402204
57,https://news.google.com/articles/CBMiW2h0dHBzOi8vbXVzaWNhbGx5LmNvbS8yMDE5LzAzLzIxL2dvb2dsZS13YW50cy1tdXNpY2lhbnMtdG8tcGxheS13aXRoLWl0cy1uZXctYWktbXVzaWMtdG9vbC_SAQA?hl=en-US&gl=US&ceid=US%3Aen,"
Username or Email Address

 
Password

  Remember Me 


 Google has been exploring the potential for AI-created music for a while now. Today, it’s making its latest experiment public, and in a high-profile way: in one of the ‘Doodles’ promoted on the homepage of its search engine. It’s an AI-powered Doodle that celebrates composer Johann Sebastian Bach. “An interactive experience encouraging players to compose a two measure melody of their choice,” as Google put it. “With the press of a button, the Doodle then uses machine learning to harmonise the custom melody into Bach’s signature music style (or a Bach 80’s rock style hybrid if you happen to find a very special easter egg in the Doodle…)” Google has a separate blog post that goes in to more detail on the technology behind the Doodle, including how it was trained on 306 of Bach’s chorale harmonisations using a machine-learning model called Coconet. “Coconet is trained to restore Bach’s music from fragments: we take a piece from Bach, randomly erase some notes, and ask the model to guess the missing notes from context,” it explained. Google has also developed a way for musicians to play with Coconet: an interface called Coucou. “It enables a new type of collaborative composition with AI, where you can iteratively improve a musical piece by erasing parts that are unsatisfying and asking the model to infill again,” explained Google. “For musicians, we hope this model can help you prototype ideas more quickly and explore more variations, or find inspiration in a motif, an unexpected turn in harmony or rhythm.” I enjoyed this, and it had me thinking about keys, and sharps, and flats. (All fields required)   Music Ally Ltd., Holborn Studios,
49-50 Eagle Wharf Rd, London, N1 7ED,
United Kingdom Music Ally is a Registered Learning Provider 10029483",0.15837391774891776,0.525879329004329
58,https://news.google.com/articles/CBMiPWh0dHBzOi8vYnVpbHRpbi5jb20vYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UvYWktbXVzaWMtZXhhbXBsZXPSAQA?hl=en-US&gl=US&ceid=US%3Aen,"This website is using a security service to protect itself from online attacks. 
Cloudflare Ray ID: 573fa496e9be333b
•
Your IP: 113.252.208.210
•
Performance & security by Cloudflare
",0.0,0.0
59,https://news.google.com/articles/CBMihQFodHRwczovL3d3dy5tdXNpY2J1c2luZXNzd29ybGR3aWRlLmNvbS9haS1pbi10aGUtbXVzaWMtaW5kdXN0cnktaXMtbm90aGluZy10by1iZS1zY2FyZWQtb2YtaXRzLWEtc3RlcC10b3dhcmRzLWFuLWV4Y2l0aW5nLW5ldy1mdXR1cmUv0gEA?hl=en-US&gl=US&ceid=US%3Aen,"Artificial Intelligence is becoming big news in the music business, a fact which was recently amplified by TikTok owner Bytedance acquiring UK-based AI Music company, JukeDeck. Like Jukedeck, Mubert is a player in the AI music space which has also raised significant investment (circa $1m) – in Mubert’s case, from the likes of GVA Capital (USA), IT-Farm (Japan), FunCubator (Russian Federation) and more. The platform creates what it calls “real-time generative music” based on existing samples in its database. Its AI therefore ‘generates’ brand new music every day, with each track designed to suit a specific individual’s mood or activity.  Mubert broadly has three dimensions to its operation: a subscription-based music streaming offering for consumers (Mubert Play); a royalty-free licensing offering for businesses and events (Mubert Business); and its industry-facing tier (Mubert Pro), which enables artists and rightsholders to upload samples and get paid for their use. The firm recently collaborated with Adidas to deliver the sports company’s customers a ‘channel’ in which they could access personalized music. Mubert was developed in Russia and has already enjoyed serious success in Asia: the company claims that it is well-established in Japan, China and India, with over 200 million users in these three regions alone. Last month, Mubert launched in the United States. Here, Mubert founder and CEO Alexey Kochetkov (pictured) argues why he believes AI is about to usher in an exciting new future for artists, and for those seeking to license their music… “Science and art are the two eyes of human culture. And the very difference between them and their equality make the volume of our knowledge. Art is a form of thinking that makes a human being as it is.'” Yury Lotman On the Nature of Art, 1990. Art is nothing less than another way of conceiving the world. People master and operate sign systems such as language, maths, painting, and music. Music, specifically, accompanies a human being from the cradle to the grave – a soundtrack that begins with a mother’s first song and is added to over time by composers, performers, and bands. Artificial intelligence is now moving beyond technical and mundane tasks and entering the world of art. Big questions concerning the nature of talent, genius and inspiration still remain unanswered – and the same applies to the dispute of what makes music human. While we witness AI penetrating the art of composing, writing songs, and signing contracts with labels, its role remains undefined. Is civilization being challenged here, or provided with a handy tool? Will AI replace or supplement artistry? Human nature is dual: people create as expertly as they destroy. What if AI could fill the gaps where people are prone to wrong turns? Historically, the music industry has relied on royalties as a major form of payment. This is a complex subject: different types of royalties require different contracts, which in turn define specific kinds of usage and licensing of intellectual property. With the advent of the digital era, and the rise of streaming services, royalty relationships have become even more tense. Both streaming services and artists are unhappy for the same, simple reason – they are not making enough money. In fact, Spotify has had a serious profitability problem; it has been a loss-making business from day one, despite growing to around 200 million users today. “It is the same old ‘winner-takes-all’ situation, where industry moguls make millions, while new and perspective artists struggle.” For a tech entity, Spotify has unusually high marginal costs – predominantly thanks to the content charges it has to pay record labels. At the same time, Spotify is ranked among the lowest-paying music platforms per stream. Sad but true: a median musician in the USA today earns less than $25,000 a year (Rolling Stone); if he or she is not Post Malone or Cardi B, their chances of making a living are negligible. The digital era has changed the way people interact with music: the path from composer, through performer, to the listener’s ear has been altered. Yet this content democratization has not made it easier for emerging musicians: it is the same old ‘winner-takes-all’ situation, where industry moguls make millions, while new and perspective artists struggle. Artificial intelligence can lend a helping hand in forming a new music industry culture – both qualitatively and quantitatively. With this idea in mind, we created Mubert, an AI-powered service of generative music that provides an entire infrastructure for the music industry where all stakeholders can benefit. With our tools, artists can create and monetize their samples and patterns, labels can share royalties with their artists, and sample distributors are given a new business model for their sample database. In the eyes of our users, Mubert is a real-time streaming service allowing people to play completely unique, endless music streams based on personal preferences, actions, events, and mood. Traditionally, it took a lot of effort to tweak music to personal needs — requiring constant management of playlists, for example. An AI-based algorithm can instead play the part of a conductor, handpicking samples and weaving a never-ending personal soundtrack – a musical canvas which perfectly suits a specific listener at a specific point in time. “An AI-based algorithm can instead play the part of a conductor, handpicking samples and weaving a never-ending personal soundtrack.” From a business standpoint, Mubert is a long-anticipated solution for broadcasting copyright-free music streams. Businesses across various industries have traditionally had to acquire permissions from copyright owners to play music legally. Event organizers, for instance, use music to create a certain atmosphere and mood. Their requirements can be a perfect example of how ‘functional’ music works; when an emotional constituent is not required – just a light, unobtrusive audio background. For this purpose, Artificial Intelligence is a suitable approach: event organizers can choose activity and mood, then play music streams simultaneously or schedule playback for later. All in all, AI is a game changer due to one very important thing: from now on, music can adapt perfectly to a listener; listeners no longer have to adapt to music. Music is just another language of communication, and the adoption of new rules forms the basis of the new culture. As long as there is no compromise for AI, the music industry can become a transparent environment where all stakeholders receive equal chances of self-expression and fair conditions for monetization. The human touch remains where music is at its best — taking notes and making them into a piece of art. Music without a listener is non-existent: Mozart’s record without an ear is just a physical object. A listener is a co-creator, and artificial intelligence offers them a new tier of involvement in the enigmatic world of composing.Music Business Worldwide The best of MBW, plus the most important music biz stories on the web. Delivered for FREE, direct to your inbox each day.",0.07330340673818934,0.42817250141163166
60,https://news.google.com/articles/CBMibmh0dHBzOi8vbXVzaWNhbGx5LmNvbS8yMDE5LzA3LzIzL2FpLW11c2ljLXN0YXJ0dXAtanVrZWRlY2tzLWNlby1ub3ctcnVucy10aGUtYWktbGFiLW9mLXRpa3Rvay1vd25lci1ieXRlZGFuY2Uv0gEA?hl=en-US&gl=US&ceid=US%3Aen,"
Username or Email Address

 
Password

  Remember Me 


 Ed Newton-Rex was the founder and CEO of Jukedeck, one of the first startups to develop a product for AI-generated music. We say ‘was’ because since April 2019, he has been working for TikTok’s parent company Bytedance as director of its AI Lab, according to his recently-updated LinkedIn profile. And that’s because Bytedance acquired Jukedeck earlier this year.  There has been gossip about the acquisition for a few weeks now – hardly a surprise, given that the company shared offices with several other music-industry companies at The Ministry in London. After being tipped off about Newton-Rex’s new role, Music Ally checked LinkedIn this morning and found several of his colleagues have also updated their profiles to reflect new roles at Bytedance. For example: David Trevelyan and Pierre Chanquion (previously senior software engineers, music production R&D at Jukedeck; now senior software engineers at Bytedance’s AI Lab); Katerina Kosta  and Gabriele Medeot (formerly machine learning researchers at Jukedeck; now senior machine learning researchers at Bytedance); and Marco Selvi (formerly senior software engineer and machine learning researcher at Jukedeck; now senior machine learning researcher at Bytedance); Jukedeck’s website (including its ‘Make’ tool for creating AI tracks) has been taken offline, with its homepage replaced by a message saying “We can’t tell you more just yet, but we’re looking forward to continuing to fuel creativity using musical AI!” – which makes it clear that music will continue to be a focus for the team within Bytedance.  When Music Ally interviewed Newton-Rex in August 2017, he talked about the idea of AI-music at mass scale. “What really drives us is two main things. First, you can democratise music. As soon as AI understands a bit more about how to write music, you can put that power into a lot more people’s hands. People who aren’t classically-educated can play and tinker with music, which is really exciting,” he said at the time. “The other side is the personalisation aspect, in terms of consuming music. Recorded [human] music is brilliant and will never die out, and it won’t be replaced by AI. But once you have AI, you can really personalise the way music is consumed. You can give every person in the world their own personal composer, and music can respond to anything from their environment to their mood or their calendar. It’s those twin goals of democratisation and personalisation that get us out of bed in the morning.” Now he and his colleagues are getting out of bed in the morning to work at TikTok’s parent company, which is certainly upping its efforts around music. Indeed, one of Newton-Rex’s latest actions on LinkedIn was to like a post by former YouTube music-rights management staffer Taylor Fife, who said: “Less than 48 hours in and I’m already cranking away at music rights and metadata issues for TikTok and ByteDance’s other apps”. Fife’s post noted that other former YouTube colleagues working for Bytedance/TikTok include director of music operations Ben Markowitz; senior director of strategic partnerships Isaac Bess, senior music ops manager, EMEA, Ben Craven; and head of global marketing Stefan Heinrich Henriquez. That’s certainly interesting on the music-licensing side of the business, but today’s news that one of the world’s AI-generated-music experts is now also part of the Bytedance family is just as intriguing. (All fields required)   Music Ally Ltd., Holborn Studios,
49-50 Eagle Wharf Rd, London, N1 7ED,
United Kingdom Music Ally is a Registered Learning Provider 10029483",0.1332075582075582,0.34159061659061657
61,https://news.google.com/articles/CAIiEPPMUCqB3aErablOmrlb3X4qFggEKg0IACoGCAowt6AMMLAmMPT6lwM?hl=en-US&gl=US&ceid=US%3Aen,"3 Min Read TOKYO (Reuters) - Few aspects of life escape the touch of high tech in cutting-edge Japan, including an official song written to welcome Pope Francis when he visits Japan from Saturday.  Written by Jun Inoue, the song, “Protect all Life - The Signs of the Times”, is based on the theme of the pope’s Japan visit and was partly composed using an artificial intelligence (AI)-powered programme Inoue created that can write a piece of music in a few seconds.  Inoue, a producer and composer who has written for top bands such as Arashi, and is Catholic, said he agonised over using the AI programme but finally decided to include it because the history of music and technology were intertwined.  “I thought I should give everything I had to the song, so I decided to put in all the cutting-edge technology I had,” he told Reuters. He shares composing credit with “Amadeus Code AI”.  The song, which was written at the behest of the Catholic Bishops’ Conference of Japan, will be played at masses in the city of Nagasaki as well as Tokyo and comes in several versions, including instrumental and karaoke.  There are also dance steps and stylised hand gestures that are the Japanese sign language version of the lyrics, with a teaching video posted on the official website for the pope’s visit. Inoue hopes worshippers at the public masses, to be held at stadiums, will sing and dance along with the song.  While the footwork is simple, consisting of side-to-side steps in time to the music, the sign language may take a bit more time to master.   “There are a lot of people who live in a world without sound, and I would like them too to understand the meaning of the lyrics and the message,” Inoue said.  Theme songs for papal visits have been composed before. One has been unveiled for Thailand, where Francis will visit before Japan.   His visit from Nov. 23-26, which will also include Hiroshima, is only the second papal visit to Japan and the first since John Paul II 38 years ago.  (This story corrects typographical error in headline)  Writing by Elaine Lies; Editing by Paul Tait All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. © 2020 Reuters. All Rights Reserved.",0.08792207792207792,0.3998639455782313
62,https://news.google.com/articles/CBMicWh0dHBzOi8vY29tcGxldGVtdXNpY3VwZGF0ZS5jb20vYXJ0aWNsZS93aXBvLW9wZW5zLWNvbnN1bHRhdGlvbi1vbi10aGUtY29weXJpZ2h0LXF1ZXN0aW9ucy1yYWlzZWQtYnktY3JlYXRpdmUtYWkv0gEA?hl=en-US&gl=US&ceid=US%3Aen,"

The World Intellectual Property Organisation has formally opened a 
consultation that seeks to ask and consider the IP-related questions 
raised by artificial intelligence becoming increasingly creative. 
The ability of AI technologies to compose and produce original 
music has become quite a talking point in the music community in recent 
years, with a number of interesting start-ups dabbling in this space, 
and major labels and digital music firms also getting involved, often 
through acquisition of the aforementioned start-ups. 
Most of the entrepreneurs behind those companies insist that 
they don’t want to put composers and songwriters out of business, 
arguing that their technologies won’t ever replace humans in the 
music-making process, and that instead they are creating tools that can 
be employed by those human creators to enhance the music making process.
 
However, some AI music start-ups are going into competition 
with the production music libraries by offering low-cost easy-to-license
 music for soundtracking videos. The AI companies can match the 
traditional production music firms on price and ease-of-use, but offer a
 totally original composition, rather than something numerous other 
video producers may have already utilised. 
Whether the best AI-created music can match the best 
human-created compositions in your average production music library is 
debatable, of course, but the machines that make music are only getting 
better at it as the years go by. 
The bigger question is could AI technologies move beyond making  production music and start composing the pop hits of the future? Most  people in the music community seem certain that truly innovative  music-making will always require human beings. But, a cynic might add,  you don’t need truly innovative music-making to generate a hit. 
Meanwhile, beyond the machines v humans debate, for copyright 
geeks the rise of AI music-making poses a number of interesting legal 
questions. Which is what, among other things, WIPO – a ‘specialised 
agency’ of the United Nations – is now starting to debate. Having 
published a report on technology trends and then staged a number of 
discussions earlier this year, WIPO has now produced an issues paper to 
inform its new consultation. 
On copyright, it notes that “AI applications are capable of 
producing … works autonomously. This capacity raises major policy 
questions for the copyright system, which has always been intimately 
associated with the human creative spirit and with respect and reward 
for, and the encouragement of, the expression of human creativity. The 
policy positions adopted in relation to the attribution of copyright to 
AI-generated works will go to the heart of the social purpose for which 
the copyright system exists”. 
“If AI-generated works were excluded from eligibility for  copyright protection”, it goes on, “the copyright system would be seen  as an instrument for encouraging and favouring the dignity of human  creativity over machine creativity. If copyright protection were accorded to AI-generated works, the copyright system would tend to  be seen as an instrument favouring the availability for the consumer of  the largest number of creative works and of placing an equal value on  human and machine creativity”.
Other interesting questions to ask include: Assuming AI works 
are protected by copyright, who does the copyright belong to? And if, as
 with musical works, the copyright term is usually linked to someone’s 
lifetime, whose lifetime? 
“In the event copyright can be attributed to AI-generated 
works, in whom should the copyright vest?” the WIPO paper goes on. 
“Should consideration be given to according a legal personality to an AI
 application where it creates original works autonomously, so that the 
copyright would vest in the personality and the personality could be 
governed and sold in a manner similar to a corporation?”
Also considered is how AI technologies evolve, and what 
copyright law should say about AI tools that process other existing 
works in order to learn how to make new ones. At what point does an AI 
technology need a licence to learn, and when could the technology be 
deemed to have infringed copyright?
Some copyright systems do already have some lines of law that 
seek to anticipate the rise of machine-created works, while others are 
currently silent on the matter. But either way, it does seem likely that
 copyright law around the world is going to need to address all these 
AI-centric questions sometime soon. So it will be interesting to see 
what opinions are shared and issued raised in this WIPO review. 
You can access the issues report here. And you’ll find more information about the consultation here. 
  The World Intellectual Property Organisation has formally opened a 
consultation that seeks to ask and consider the IP-related questions 
raised by artificial intelligence becoming increasingly creative.  The ability of AI technologies to compose and produce original 
music has become quite a talking point in the music community in recent 
years, with a number of interesting start-ups dabbling in this space, 
and major labels and digital music firms also getting involved, often 
through acquisition of the aforementioned start-ups.  Most of the entrepreneurs behind those companies insist that 
they don’t want to put composers and songwriters out of business, 
arguing that their technologies won’t ever replace humans in the 
music-making process, and that instead they are creating tools that can 
be employed by those human creators to enhance the music making process.
  However, some AI music start-ups are going into competition 
with the production music libraries by offering low-cost easy-to-license
 music for soundtracking videos. The AI companies can match the 
traditional production music firms on price and ease-of-use, but offer a
 totally original composition, rather than something numerous other 
video producers may have already utilised.  Whether the best AI-created music can match the best 
human-created compositions in your average production music library is 
debatable, of course, but the machines that make music are only getting 
better at it as the years go by.  The bigger question is could AI technologies move beyond making  production music and start composing the pop hits of the future? Most  people in the music community seem certain that truly innovative  music-making will always require human beings. But, a cynic might add,  you don’t need truly innovative music-making to generate a hit.  Meanwhile, beyond the machines v humans debate, for copyright 
geeks the rise of AI music-making poses a number of interesting legal 
questions. Which is what, among other things, WIPO – a ‘specialised 
agency’ of the United Nations – is now starting to debate. Having 
published a report on technology trends and then staged a number of 
discussions earlier this year, WIPO has now produced an issues paper to 
inform its new consultation.  On copyright, it notes that “AI applications are capable of 
producing … works autonomously. This capacity raises major policy 
questions for the copyright system, which has always been intimately 
associated with the human creative spirit and with respect and reward 
for, and the encouragement of, the expression of human creativity. The 
policy positions adopted in relation to the attribution of copyright to 
AI-generated works will go to the heart of the social purpose for which 
the copyright system exists”.  “If AI-generated works were excluded from eligibility for  copyright protection”, it goes on, “the copyright system would be seen  as an instrument for encouraging and favouring the dignity of human  creativity over machine creativity. If copyright protection were accorded to AI-generated works, the copyright system would tend to  be seen as an instrument favouring the availability for the consumer of  the largest number of creative works and of placing an equal value on  human and machine creativity”. Other interesting questions to ask include: Assuming AI works 
are protected by copyright, who does the copyright belong to? And if, as
 with musical works, the copyright term is usually linked to someone’s 
lifetime, whose lifetime?  “In the event copyright can be attributed to AI-generated 
works, in whom should the copyright vest?” the WIPO paper goes on. 
“Should consideration be given to according a legal personality to an AI
 application where it creates original works autonomously, so that the 
copyright would vest in the personality and the personality could be 
governed and sold in a manner similar to a corporation?” Also considered is how AI technologies evolve, and what 
copyright law should say about AI tools that process other existing 
works in order to learn how to make new ones. At what point does an AI 
technology need a licence to learn, and when could the technology be 
deemed to have infringed copyright? Some copyright systems do already have some lines of law that 
seek to anticipate the rise of machine-created works, while others are 
currently silent on the matter. But either way, it does seem likely that
 copyright law around the world is going to need to address all these 
AI-centric questions sometime soon. So it will be interesting to see 
what opinions are shared and issued raised in this WIPO review.  You can access the issues report here. And you’ll find more information about the consultation here.  PLEASE NOTE: THIS WEBSITE USES COOKIES TO ENHANCE YOUR EXPERIENCE | YOU CAN MANAGE HOW COOKIES WORK IN YOUR BROWSER SETTINGS  CLICK HERE FOR FULL DETAILS ON OUR PRIVACY, DATA AND COOKIES POLICY | CLICK HERE TO ACCEPT AND BROWSE   ",0.17716010276985883,0.46580790483229517
63,https://news.google.com/articles/CAIiEOVDPmYJdK1MMhxtDKAnw5QqGQgEKhAIACoHCAow9MSGCzCKpYQDML-UuQM?hl=en-US&gl=US&ceid=US%3Aen,"Get The Magazine The best in culture from a cultural icon. Subscribe now for more from the authority on music, entertainment, politics and pop culture. Plus, get a limited-edition tote FREE. Newsletter Signup Sign up for our newsletter and go inside the world of music, culture and entertainment. Instead of fighting the rise of artificial intelligence in music, the singer-songwriter built a robot bandmate 
				Boris Camaca			  Holly Herndon is not interested in creating her own replacement. The composer-musician saw the rise of machine intelligence in music as major labels and tech companies, hungry for cheap production, began pushing AI songwriters. But rather than fight it, Herndon decided to raise a robot bandmate herself. Herndon, who completed her Ph.D. at Stanford’s Center for Computer Research in Music and Acoustics, trained an artificial neural network built into a “DIY souped-up” gaming computer to find its voice from scratch and sing with her. For two years, Herndon nourished the AI baby, known semi-affectionately as Spawn, with her own vocals, composing, cooking, and living alongside it. Instead of sampling directly from her voice, Spawn created its own sounds using rules it could discern from the patterns it heard. The results are spliced throughout Herndon’s recent album PROTO, on which Spawn stumbles into uncanny beauty — an alien child moaning and wailing in harmony with its mother.  






 The choice to include the skittering roughness of some of Spawn’s vocal contributions wasn’t rooted in low-fi fetishism, but in transparency about the nascent state of musical neural networks. For Herndon, current A.I. compositions tend to sound stuck in an “aesthetic cul de sac” because of relatively unsophisticated programs that prioritize a clean sound over uniqueness. 



			Related		



 


					Luck Reunion 2020 Canceled Over Growing Coronavirus Crisis				



					Chris Stapleton Postpones Tour Dates of All-American Road Show Over Coronavirus Spread				




			Related		



 


					Hank Williams' Five Most Haunting Performances				



					The 50 Best Movies of the 2010s				



 “So much of the AI music that we’re presented with is so glossy and perfect, and it’s really not there yet,” Herndon says. “But right now it feels a bit like smoke and mirrors.” For Herndon, AI compositions tend to sound shiny but sparkless. Instead of making hollow copies of someone else’s style, Spawn’s ungainly and imperfect voice is its own. That transparency also extends to Herndon’s interrogation of the legal and ethical issues raised by algorithmically-generated music. Are royalties owed if A.I. is trained on the sounds of a singer like Holly — or, say, the late Aretha Franklin? Even as A.I. composers rapidly improve at emulating individual artists, copyright law seems not yet prepared to tackle this question. But by making explicit the human labor that went into Spawn, Herndon hopes to help set the norms for A.I. music and prevent uncredited mooching off others’ music. “I really wanted to make audible the people who went into the training data,” she says. “I’m concerned for attribution, for people’s intellectual labor just to be hoovered up as soon as it is machine-legible and spit out by whoever has access to the most powerful model.” 
		In This Article:
				Future 25,				music industry  
			Want more Rolling Stone? Sign up for our newsletter.
 Newsletter Signup Have a Tip? Follow Us Alerts & Newsletters 
					© 2020 Penske Media Corporation				",0.21681547619047625,0.4866071428571429
64,https://news.google.com/articles/CBMiOmh0dHBzOi8vbXVzaWNhbGx5LmNvbS8yMDE5LzEyLzA2L2VuZC1nYW1lLWFpLW11c2ljLXBvcGd1bi_SAQA?hl=en-US&gl=US&ceid=US%3Aen,"
Username or Email Address

 
Password

  Remember Me 


 “What’s the end-game for this? There isn’t this place in the world where teenagers come together to make music for each other. That place does not exist, and that’s nuts! That thing needs to exist, and it will exist. And getting the AI working is the price of admission to build that thing…” Stephen Phillips, CEO of Australian startup Popgun, thinks that the early business models in this sector – AI-music as a replacement for production music, for example – are just a sliver of the ultimate potential for this technology. What’s more, his thoughts on how AI music might disrupt the current music industry are less about people choosing to listen to AI-made music instead of human-made music, but rather about people (non-musicians) using AI tools to make music for one another. Specifically: kids. “Where’s the ‘pop stars on training wheels’ place where they make music for each other, release it and watch each other pretend to be pop stars, but then go on to become legitimate pop stars? Who’s going to create that space where the next Billie Eilish comes from?” he says. “The current pop industry is very few musicians controlled by three or four companies, played by a billion people. The new thing has to start with kids making music for each other and ignoring all of that world. It’s still too hard, though, and AI has to be the answer for that.” This is not (yet) Popgun’s business. Music Ally first wrote about the Australian startup in June 2017 when it was teaching an AI called ‘Alice’ how to play piano with humans; then again in November 2018 when it showed how its AI had learned to compose and play drums, bass and piano together; and then again in July 2019 when it showed off how it had taught an AI to sing. In terms of products, Popgun has released Splash Pro, a plug-in for the Ableton Live software that lets musicians quickly create original, AI-generated compositions, including custom drums and vocal ideas, when they’re working on their own projects. Another product, Gloss, uses AI to master music, podcasts and video audio. The company’s website trails a third product, Splash as ‘coming soon’. You might surmise that it takes the technology used for Splash Pro and makes it available for non-pros in some way.  Phillips isn’t talking about Splash just yet, but it’s clear that the company has been experimenting with AI-music products designed for a mainstream (and young) audience. It’s not just Popgun talking about the potential for an app that does for music-making what Instagram did for photography and TikTok did (particularly for kids) for videography. And it sideswipes some of the familiar, sceptical responses to the notion of AI-generated music – for example, that it lacks the backstories and/or societal roots of human artists. The meaning of, say, Adele is about far more than her music. And this is true. But if something emerges that is not about ‘AIs making music for humans’ but about ‘teenagers using AI tools to make music for one another’, then there will be no shortage of meaning, and cultural connections. “It’s us who give the meaning to the music: it’s just functional music if there’s no human associated with this. Billie Eilish is not a star because her stuff sounds so much better than angry teenage stuff we’ve heard before. She’s compelling as an artist,” says Phillips. “But think about all these twentysomething hipster-songwriters in LA googling what 13 year-old girls say to each other, so they can write songs for them. 13 year-old girls know what they say to each other!” “Once they have the tools to make music that sounds great, they’ll make music for each other, and it’ll sound incredibly genuine to them. Instead of Taylor Swift pretending to be a 15 year-old, there’ll be an actual 15 year-old, talking about the things 15 year-olds relate to most.” It feels like an app like this that truly makes waves is a while off (but then again, who saw TikTok coming a couple of years ago?) but Phillips’ views are a reminder that what we’ve heard of AI-music so far, and seen in terms of business models, is still very early days in the commercial history of this area – as opposed to the academic/research history, which stretches back decades. “Like all new things, people overestimate its impact in the short term, and underestimate its impact in the long term,” suggests Phillips. “When it really started to get serious in 2016 to 2017-ish, people were like ‘It’s going to put everyone out of work!’ And now? It’s going to put spome functional-music people out of work is my guess. But that’s not the end-game.” Popgun’s AI, as its YouTube demos testify, has evolved rapidly and impressively since the company emerged in 2017. Phillips has a sharp sense, however, of what the current technology is capable of – and where it still falls short of what humans can do. It turns out that one of the latter is that traditional A&R skill of spotting a great piece of music. “We haven’t tackled the core problem of quality or taste. I can go and jump on our piano model and spit out a million piano tracks, eight bars long in any different key, and within that million there’ll be one or two that are amazing. What I don’t have is a computational way to work out which ones they are,” he says. “We’ve spent two years working on how we model these instruments so they sound like humans are playing them, with 10 guys as good as anyone’s got. These instruments know how to play the piano! But they don’t know if what they have done is good. Right and good are different things.” “They know what they’re playing is not out of tune – they’re trained on stuff that’s in tune – but they don’t have any clue if what they are playing is original or interesting. That’s why we need humans to curate it.” Phillips relates the tale of a recent conference he attended, where a well-known tech CEO made the confident prediction that AIs would be having chart hits within the next decade, because they’d be able to generate an endless supply of music, partnered with an algorithm that ranks them and releases the best. “I was like: it’s the second bit that’s the problem!” laughs Phillips. “It’s just not possible right now: to say that’s a solved problem is just not credible to anyone who works in this space. ‘Humans can’t predict whether this song will be a hit, but my AI can’? Even people who don’t work in music AI, but work in AI, would dismiss that as not possible.” Phillips is just as interesting on another important question for anyone working in AI music and the industries it affects: is this about making music that sounds familiar from the genres we already know, or about creating new sounds and new genres? He’s forthright on this, based on Popgun’s research into composition for its technology. “AI shouldn’t just compose original music. It should sound completely different. It should be pleasurable, but also ‘Holy hell! Like nothing I’ve ever heard before!’ We’re only just starting to work on that.” Which brings us back to that idea of a consumer product for non-musicians (even if they aren’t teenagers, but especially those) to make music for one another. If it sounds slightly… alien, or jarring compared to the music we have already – if it puzzles or even angers people outside this community? Well, that would surely only add to its authenticity within the new music-makers. “The implications for the pop-music industry are interesting. I don’t think it does anything but help music. labels will exist to harvest the best and brightest of those kids and turn them into pop stars. Whatever happens with AI music, there’ll still be 20 super-engaging, charismatic people better than everybody else at the top of the charts,” says Phillips.  An interesting facet of our conversation focuses on Popgun’s work teaching an AI to sing, with vocals having been one of the biggest challenges for AI-music startups to crack so far. Phillips admits that in 2018, he thought the task of AI-vocals would be much simpler than it has proved to be. A year later, we realised this is really hard!” he laughs. “Singing is so much harder than speaking: you’ve got to sing at different pitches, there are all these controls. And no decent data-sets anywhere: there are thousands of years of people speaking, but very little of people singing dry without reverb. We had to record all of our own singers.” Phillips estimates that Popgun is “still one, maybe two generations away” from an AI that can sing in a manner that people would enjoy listening to: at which point the company may have 10 or 20 AI singers controllable by users of its products: typing in lyrics to be sung. “It’s going to be their voice: but what they imagine it would be like, if they could sing. And at that point, they’ll really start lip-synching and making videos,” says Phillips, before warning that this technology will not serve to bury genuine musical talent. “Even if that exists, some kid who’s super-talented will come along and make a better version than the other kids. That’s what pop stars are!” he says. (All fields required)   Music Ally Ltd., Holborn Studios,
49-50 Eagle Wharf Rd, London, N1 7ED,
United Kingdom Music Ally is a Registered Learning Provider 10029483",0.16167344138800838,0.48666001977025597
65,https://news.google.com/articles/CBMiU2h0dHBzOi8vdml0ZXJiaXNjaG9vbC51c2MuZWR1L25ld3MvMjAxOS8xMC93aHktbXVzaWMtbWFrZXMtdXMtZmVlbC1hY2NvcmRpbmctdG8tYWkv0gEA?hl=en-US&gl=US&ceid=US%3Aen,,0.0,0.0
66,https://news.google.com/articles/CAIiEMVIGPyYnIVBiA_OY86EISQqFQgEKg0IACoGCAowuLUIMNFnMInmAg?hl=en-US&gl=US&ceid=US%3Aen,"
      New technology sometimes makes the world around us look unrecognizable. Photo illustration by Lisa Larson-Walker. Photos by the Library of Congress.
 By
          

Seth Stevenson

 
Listen to the show in Apple Podcasts, another podcast player, or the player below.
 
  Computers can write music now. They’re actually not awful at it. And they’re much quicker and cheaper than human composers, without any of the ego or ambition, making them an appealing choice for anyone who needs a tune on a tight budget. You might soon hear songs written by artificial intelligence playing in the background of radio ads, YouTube shows, or video games.
 
  A.I. has not yet penned a Top 40 hit. But it gets more talented every day. It’s likely a matter of time before a song composed entirely by a computer is rocking dance floors and climbing pop charts.
 
  The way a computer learns to compose is by listening to whatever music you feed it and then analyzing the patterns it finds. Which raises the question: Who gets the songwriting credit—and banks the royalties—when the composer is a neural net that wrote its hit jam after imbibing, say, the entire catalog of the Beatles? Would you divvy credit between Lennon, McCartney, and whichever hoodie-clad coder programmed the software? When I spoke to the general counsel of BMI, an agency that defends songwriters’ rights, he chuckled as he confessed that nobody knows exactly how this will all shake out.
 
  If there’s one thing we’ve learned in making The Secret History of the Future—our podcast that unearths the tech quandaries of yesterday and uses them to predict the shape of tech’s tomorrow—it’s that solutions to today’s problems are often found lurking in the past. In this case, there’s much we can learn from a story about another cutting-edge technology that once ripped apart the music business. It was called the phonograph.
 
  When Thomas Edison invented the phonograph in 1877, it was the first time that humans could record and play back sound. And suddenly, instead of just playing concerts for cash, professional composers could also sell recordings of their music to the masses. Seems like fantastic news for the songwriters, right? But it wasn’t that simple.
 
  The most famous American composer of his era—the equivalent of today’s mega rock stars—was John Philip Sousa, a bandleader who wrote military marches in the late 19th century that still get played today. (It’s a good bet you heard “The Stars and Stripes Forever” on the Fourth of July.) Sousa earned a fortune from barnstorming the country on his concert tours and from selling the sheet music for his compositions. But when the public began to buy phonograph recordings of his songs, Sousa discovered that he didn’t receive a dime from the sales. There were crucial flaws in the existing copyright laws, which had failed to keep up with the advance of technology.
 
  What Sousa did next doesn’t just make for a fascinating yarn. It also prefigures copyright disputes that arose when the new technology of digital sampling exploded on the hip-hop scene in the 1980s. And it offers clues about what to do when it turns out the next John Philip Sousa is an A.I.
 
  This is one of many stories you’ll hear in Season 2 of The Secret History of the Future. My co-host, Tom Standage, and I are back with another 10 episodes of this trans-Atlantic collaboration from Slate and the Economist. Last year, we told you about a pair of black-hat hackers from the 1830s, a chess-playing robot from the 1770s, and a bunch of influencers from the 1600s who made the fork happen, transforming it from a weird curiosity into an essential utensil. This year, we’re bringing you a whole new collection of curious tales and colorful characters from yore, in search of insights to help us navigate the big tech problems looming ahead of us.
 
  In Season 2, you’ll meet a Finnish maritime expert who’s reinventing the way ships work, a Dutch entrepreneur who’s growing beef burgers in Petri dishes, an American futurist who’s building a mysterious clock deep inside a Texas mountain, and countless other fascinating folks. Along the way, you’ll learn …
 
  … what the world’s oldest computer program, written by Ada Lovelace in 1843, can tell us about algorithmic bias today …
 
  … how the potato’s lumpy introduction to Europe might offer warnings for scientists who are designing the foods of tomorrow …
 
  … why bicycles were the hot dating apps of the 19th century …
 
  … and whether past mistakes using forensic fingerprinting can help us avoid new pitfalls with DNA evidence.
 
  New technology sometimes makes the world around us look unrecognizable. But human nature—even over the centuries—remains remarkably familiar. Again and again, as we’ve researched the stories that go into these podcast episodes, we’ve seen that the past is full of hints about what’s coming around the corner. We hope you’ll join us as we reveal The Secret History of the Future.
 Seth Stevenson is a frequent contributor to Slate. He is the author of Grounded: A Down to Earth Journey Around the World. 
        Slate is published by The Slate Group, a Graham Holdings Company.
        All contents © 2020 The Slate Group LLC. All rights reserved.
       Slate relies on advertising to support our journalism. If you value our work, please disable your ad blocker. By joining Slate Plus you support our work and get exclusive content. And you'll never see this message again.",0.08355953355953356,0.42407911157911155
67,https://news.google.com/articles/CBMiVmh0dHBzOi8vd3d3LmNsYXNzaWNmbS5jb20vZGlzY292ZXItbXVzaWMvaW5zdHJ1bWVudHMvcGlhbm8vZ2xlbm4tZ291bGQtYmFjaC1haS15YW1haGEv0gFWaHR0cHM6Ly9hbXAuY2xhc3NpY2ZtLmNvbS9kaXNjb3Zlci1tdXNpYy9pbnN0cnVtZW50cy9waWFuby9nbGVubi1nb3VsZC1iYWNoLWFpLXlhbWFoYS8?hl=en-US&gl=US&ceid=US%3Aen,"
Classic FM
 


                
                    Saturday Night at the Movies with Andrew Collins
                
                

5pm - 7pm
 
Peter Pan - Fairy Dance

                        

    
    James Newton Howard
    

                    
 
                        
                            Discover Music
                            


 29 October 2019, 17:14 
        By Rosie Pentreath
     Yamaha’s self-playing piano has been intricately designed to mimic the celebrated 20th-century Canadian pianist and Bach interpreter down to the finest musical detail. Yamaha has invented a piano that not only plays itself, but also mimics one of the best pianists and Bach interpreters of the 20th century – Glenn Gould.     The piano was born of Yamaha’s Dear Glenn project, which uses machine learning to teach the artificial intelligence that drives the instrument’s self-playing capability the interpretative idiosyncrasies and stylistic nuances of Gould’s playing.     In a video released by Yamaha, the ‘Glenn Gould as AI’ piano performs Bach’s Goldberg Variations in a concert setting – offering audiences, it is hoped, as close a version of the late Gould’s performances as they can ever hope to see live. The self-playing aspect of the piano was made possible by the same technology used for Yamaha’s existing Disklavier instrument. Read more: Google created a Bach harmonisation machine, and you can play it here! > Well, Yamaha has created a helpful description of how it trained its Dear Glenn piano to perfect its Gouldian approach to Bach.     Step one, Yamaha’s ‘Training of the AI’ explanation says, is to “Extract music performance data from music audio recordings by Glenn Gould, such as the velocity and the timing fluctuations of key depression.”     After that, it’s all about training the AI to orientate itself around both the score and these recordings, while generating data around the subtle differences ­– more like comparisons, perhaps – between the two, and connect these dots to form a live (albeit digital) interpretation of any score in the style of Gould. This bit is getting fairly technical, but for those that way inclined, Yamaha uses deep neural network AI to pull off this symbiosis between science and art.     Well apart from the fact he’s one of the finest pianists of recent times, and his 1956 interpretation of Bach’s Goldberg Variations, particularly, cemented his place in the history of recorded music, Yamaha was inspired to recreate Gould’s style due to his unique habits at the piano – he was even known to hum during some recordings!     “This project is inspired by Gould’s unique creative style and launched to explore the future of music through the use of artificial intelligence,” Yamaha says on its website.    “Glenn Gould was known for his devotion to recording with digital media and an interest in rethinking the relationship between performer and audience. The project to develop this system has been dubbed ‘Dear Glenn’ as a tribute to the artist’s attitude.”    Visit yamaha.com to find out more.  Puccini Lifestyle Videos Nicola Benedetti See more More instruments",0.141497697563874,0.5092320261437909
68,https://news.google.com/articles/CBMiPmh0dHBzOi8vd3d3Lm11c2ljdGVjaC5uZXQvbmV3cy9vcmItY29tcG9zZXItaGV4YWNob3Jkcy11cGRhdGUv0gFCaHR0cHM6Ly93d3cubXVzaWN0ZWNoLm5ldC9uZXdzL29yYi1jb21wb3Nlci1oZXhhY2hvcmRzLXVwZGF0ZS8_YW1w?hl=en-US&gl=US&ceid=US%3Aen,"The new version brings a MIDI editor and drum sequencer among other features Orb Composer is the first Artificial Intelligence designed for music composers, with more than five years of research and development to bring it to life, according to Hexachords. It’s new 1.5 update integrates a MIDI editor, drum sequencer, new Orchestral Template, Smart Melody Import and other major improvements and bug fixes.  Smart Melody import allows you to drag in any MIDI file and let Orb Composer arrange an entire piece based off the melody within the file. The AI will create automatic harmonisation and orchestration, at any desired tempo and ‘intensity’. Using the new MIDI editor, arrangements can then be manually altered to the user’s liking. Drum editor will also allow you to modify the selected drum pattern, or create an entirely new sequence. Hexachord has included three new instruments, Harp, Piano 1 and Piano 2 to include in your arrangements, too. Orb Composer can be used as either a standalone app or integrated within a DAW. In standalone mode, you can also implement VST/AU instruments to incorporate your own sounds into the AI composer. Orb Composer 1.5 comes in two versions for a limited time offer.
For new users, the Pro version is currently available for €349 instead of €399 and
the Artist version can be bought for €99 instead of €149. Current Orb Composer owners can get the upgrade to 1.5 for only €79instead of the regular €99 for both the Pro and the Artist version. Find out more at orb-composer.com Be updated with all the latest news, offers and special announcements. Subscribe now and receive a free chorus plug-in! We provide insight and opinion on the gear, tools, software and services to enhance and expand the minds of music makers and listeners. © 2020 MusicTech is a member of the media division of BandLab Technologies.",0.1475475417439703,0.5013538544788545
69,https://news.google.com/articles/CBMiP2h0dHBzOi8vd3d3LnRoZW5hdGlvbi5jb20vYXJ0aWNsZS9hcmNoaXZlL2F1eHVtYW4tbXVzaWMtcmV2aWV3L9IBAA?hl=en-US&gl=US&ceid=US%3Aen,"By using this website, you consent to our use of cookies. For more information, visit our Privacy PolicyX  Yona.  (Courtesy of Auxuman)   You will receive occasional promotional offers for programs that support The Nation’s journalism. You can read our Privacy Policy here. You will receive occasional promotional offers for programs that support The Nation’s journalism. You can read our Privacy Policy here. Subscribe now for as little as $2 a month!  You will receive occasional promotional offers for programs that support The Nation’s journalism. You can read our Privacy Policy here. You will receive occasional promotional offers for programs that support The Nation’s journalism. You can read our Privacy Policy here. A mechanical way of thinking, an artificial kind of intelligence, has underlaid the making of popular music since the invention of the form. That’s why we can talk about pop as an invention with a given form. As early as the days of hit songs written on sheet music, before the advent of recordings and radio, Charles K. Harris, writer of the first million-selling song sheet, “After the Ball,” described the trade of tunesmithing in industrial terms: a matter of applying procedures derived from past successes, calibrating them for mass consumption. He wrote an instruction manual, How to Write a Popular Song, in 1906, with a checklist of essential rules. In his words:

Ad Policy
 				
			function load_article_ads(){
				if( jQuery(""#ad-halfpage-332927-0"").is("":empty"") ){call_ad_new('halfpage','tn_article','ad-halfpage-332927-0','rectangle_1',{""tn_author"":""'david-h'"",""tn_articleid"":332927,""tn_ptype"":""article"",""tn_keyword"":""false"",""tn_subject"":""'culture', 'music'"",""tn_slp"":""""});}					
			}	
	
			jQuery('#expand-reduce-332927').click(function(){
				if( isMobile.any() ){
					call_ad_new('halfpage','tn_article','ad-halfpage-332927-0','rectangle_1',{""tn_author"":""'david-h'"",""tn_articleid"":332927,""tn_ptype"":""article"",""tn_keyword"":""false"",""tn_subject"":""'culture', 'music'"",""tn_slp"":""""});	
				}				
			});	
		
Watch your competitors. Note their success or failure; analyze the cause and profit thereby.
Note public demand.
If you do not feel confident to write or compose a certain style of song…adapt yourself to the others.
Over more than a century since Harris’s era, the proposition that songs should be made according to rules based on precedent and shaped by market forces is taken as a given and, indeed, valued as a way of honoring tradition and pleasing the public in many spheres of music, from country and gospel to jazz and R&B. At the same time, talk of regimented production or subordination of the creative impulse is widely and freely employed as criticism. Music journalists and critics have few tools as piercing as the charge that a song is formulaic or mechanical, or that an artist is pandering or a sellout.
The growing use of artificial intelligence in music challenges us to consider not only songwriting but also singing and musicianship in ways that are essentially extensions of the machine-age thinking of Harris and, at the same time, startlingly new. A small but expanding group of tech innovators has been developing a range of musical applications of AI such as Boomy, which, so far, has allowed people to make more than 400,000 songs through a combination of machine learning and input from human users. Earlier this year, the start-up Endel released a series of albums of AI-generated ambient music on the major streaming services, through a partnership with Warner Music. The company is planning to release 20 albums by the end of this year, all in the vein of chill playlists, with candle-scent-like titles that signal their purpose of conjuring soothing atmospheres: “Clear Night,” “Rainy Night,” “Cloudy Afternoon,” “Cloudy Night,” and “Foggy Morning.” Wordless, nearly formless, and harmless, they’re perfectly functional background music, well-marketed to make an asset of the absence of anything warranting the listener’s attention. A more recent project from another start-up, Auxuman, is an achievement on another level and may well be a watershed in pop music history.						



Auxuman (brand shorthand for auxiliary human) is the brainchild of the British Iranian interdisciplinary artist Ash Koosha (aka Ashkan Kooshenaejad). He first established himself with a couple of pleasantly atmospheric synth-based pop albums, the first of which, Guud, included a single, “I Feel That,” whose official video starred a synthetic semihumanoid, created by the digital artist Hirad Sab. Before turning to AI, Koosha made some multisensory art for VR headsets. Now Koosha and a team of programmers have created a stable of digital music acts who (I’ll use the personal pronoun for human beings in deference to the “uman” in Auxuman) will be releasing a full album of new material every month under the umbrella name Auxuman.
As Koosha has described the Auxuman process to the tech-fan site Digital Trends, the words, music, instruments, and singing voices on the tracks are generated by mining existing music on the Web and processing it to generate new songs. The synthetic artists who are the public face of the work are, in Koosha’s words, “a reflection of human life on the internet.” Their music “comes from stories we have told, ideas we have generated, and opinions we have shared.” The principles are not far removed from Harris’s rules for analyzing the music of other songwriters and factoring in public demand.
There are five digital performers in the Auxuman collective to date: Gemini, Hexe, Mony, Yona, and Zoya. Visually, in the avatar imagery that accompanies each song on YouTube, they’re a mix of racial and ethnic identities, in a few cases thoroughly and indefinitely mixed. Four (Gemini, Hexe, Mony, and Yona) are distinctly female in appearance, one (Zoya) male. The one front and center in group pictures, Yona, is, a pixieish white woman who looks like what she is: a computer’s idea of a pop star.
In September, the first music attributed to the five was released as a 10-track album, Auxuman #1. Taking in these recordings, at first I tried to shake the fact that they were generated by AI, and sought to listen with no preconceptions or expectations. Within a minute, I realized that was pointless and unfair to the work and its digital creators. With Auxuman #1, we are forced to confront a genuinely new type of music that works on its own terms.
All five voices, though somewhat distinct from one another, sound of a piece and appropriately artificial—metallic in tone and rigidly staccato in their diction and phrasing. The voices carry no warmth and have no flexibility. They sound inhuman, soulless, but fascinatingly so. To expect otherwise from them would be as wrongheaded as it was for early listeners of recordings to expect pioneers of the microphone such as Billie Holiday and Frank Sinatra to belt like Al Jolson, Sophie Tucker, and others who needed to bellow to reach the rafters of big concert halls. Yona, in particular, exudes impersonal detachment and superficiality. As well as any other artist I can think of, she gives voice to the sense of isolation that chills the air of the digital world.
The lyrics are an eerie jumble of phrases, mostly trite babbling, not unlike the words of a fair number of pop songs since Charles K. Harris’s day. From time to time, though, the random juxtapositions come together in unnerving, accidental eloquence. In “Oblivious,” Yona sings:
I’ve never felt warm
I’ve never felt warm
Through the lens
Through your lens
I feel warm
We know she cannot feel anything. And she communicates only coldness. Knowing this, as I listened, I found myself projecting onto her and started to feel bad for her. Through my lens, I gave her warmth.
The music on all 10 tracks is perfectly, unsettlingly synthetic. Every note is placed precisely on beat. The harmony in every chord is correct, technically. And yet, nothing sounds quite right. There’s something profoundly but fittingly disturbing about it all. It’s utterly conventional and predictable in its formal structures and musical particulars, but wholly arbitrary, built of nothing but its own surface qualities. It sounds like what it is: code pretending to be life. I can think of nothing more relevant.
	 Watch your competitors. Note their success or failure; analyze the cause and profit thereby.
Note public demand.
If you do not feel confident to write or compose a certain style of song…adapt yourself to the others. Note public demand.
If you do not feel confident to write or compose a certain style of song…adapt yourself to the others. If you do not feel confident to write or compose a certain style of song…adapt yourself to the others. Over more than a century since Harris’s era, the proposition that songs should be made according to rules based on precedent and shaped by market forces is taken as a given and, indeed, valued as a way of honoring tradition and pleasing the public in many spheres of music, from country and gospel to jazz and R&B. At the same time, talk of regimented production or subordination of the creative impulse is widely and freely employed as criticism. Music journalists and critics have few tools as piercing as the charge that a song is formulaic or mechanical, or that an artist is pandering or a sellout.
The growing use of artificial intelligence in music challenges us to consider not only songwriting but also singing and musicianship in ways that are essentially extensions of the machine-age thinking of Harris and, at the same time, startlingly new. A small but expanding group of tech innovators has been developing a range of musical applications of AI such as Boomy, which, so far, has allowed people to make more than 400,000 songs through a combination of machine learning and input from human users. Earlier this year, the start-up Endel released a series of albums of AI-generated ambient music on the major streaming services, through a partnership with Warner Music. The company is planning to release 20 albums by the end of this year, all in the vein of chill playlists, with candle-scent-like titles that signal their purpose of conjuring soothing atmospheres: “Clear Night,” “Rainy Night,” “Cloudy Afternoon,” “Cloudy Night,” and “Foggy Morning.” Wordless, nearly formless, and harmless, they’re perfectly functional background music, well-marketed to make an asset of the absence of anything warranting the listener’s attention. A more recent project from another start-up, Auxuman, is an achievement on another level and may well be a watershed in pop music history.						



Auxuman (brand shorthand for auxiliary human) is the brainchild of the British Iranian interdisciplinary artist Ash Koosha (aka Ashkan Kooshenaejad). He first established himself with a couple of pleasantly atmospheric synth-based pop albums, the first of which, Guud, included a single, “I Feel That,” whose official video starred a synthetic semihumanoid, created by the digital artist Hirad Sab. Before turning to AI, Koosha made some multisensory art for VR headsets. Now Koosha and a team of programmers have created a stable of digital music acts who (I’ll use the personal pronoun for human beings in deference to the “uman” in Auxuman) will be releasing a full album of new material every month under the umbrella name Auxuman.
As Koosha has described the Auxuman process to the tech-fan site Digital Trends, the words, music, instruments, and singing voices on the tracks are generated by mining existing music on the Web and processing it to generate new songs. The synthetic artists who are the public face of the work are, in Koosha’s words, “a reflection of human life on the internet.” Their music “comes from stories we have told, ideas we have generated, and opinions we have shared.” The principles are not far removed from Harris’s rules for analyzing the music of other songwriters and factoring in public demand.
There are five digital performers in the Auxuman collective to date: Gemini, Hexe, Mony, Yona, and Zoya. Visually, in the avatar imagery that accompanies each song on YouTube, they’re a mix of racial and ethnic identities, in a few cases thoroughly and indefinitely mixed. Four (Gemini, Hexe, Mony, and Yona) are distinctly female in appearance, one (Zoya) male. The one front and center in group pictures, Yona, is, a pixieish white woman who looks like what she is: a computer’s idea of a pop star.
In September, the first music attributed to the five was released as a 10-track album, Auxuman #1. Taking in these recordings, at first I tried to shake the fact that they were generated by AI, and sought to listen with no preconceptions or expectations. Within a minute, I realized that was pointless and unfair to the work and its digital creators. With Auxuman #1, we are forced to confront a genuinely new type of music that works on its own terms.
All five voices, though somewhat distinct from one another, sound of a piece and appropriately artificial—metallic in tone and rigidly staccato in their diction and phrasing. The voices carry no warmth and have no flexibility. They sound inhuman, soulless, but fascinatingly so. To expect otherwise from them would be as wrongheaded as it was for early listeners of recordings to expect pioneers of the microphone such as Billie Holiday and Frank Sinatra to belt like Al Jolson, Sophie Tucker, and others who needed to bellow to reach the rafters of big concert halls. Yona, in particular, exudes impersonal detachment and superficiality. As well as any other artist I can think of, she gives voice to the sense of isolation that chills the air of the digital world.
The lyrics are an eerie jumble of phrases, mostly trite babbling, not unlike the words of a fair number of pop songs since Charles K. Harris’s day. From time to time, though, the random juxtapositions come together in unnerving, accidental eloquence. In “Oblivious,” Yona sings:
I’ve never felt warm
I’ve never felt warm
Through the lens
Through your lens
I feel warm
We know she cannot feel anything. And she communicates only coldness. Knowing this, as I listened, I found myself projecting onto her and started to feel bad for her. Through my lens, I gave her warmth.
The music on all 10 tracks is perfectly, unsettlingly synthetic. Every note is placed precisely on beat. The harmony in every chord is correct, technically. And yet, nothing sounds quite right. There’s something profoundly but fittingly disturbing about it all. It’s utterly conventional and predictable in its formal structures and musical particulars, but wholly arbitrary, built of nothing but its own surface qualities. It sounds like what it is: code pretending to be life. I can think of nothing more relevant.
	 The growing use of artificial intelligence in music challenges us to consider not only songwriting but also singing and musicianship in ways that are essentially extensions of the machine-age thinking of Harris and, at the same time, startlingly new. A small but expanding group of tech innovators has been developing a range of musical applications of AI such as Boomy, which, so far, has allowed people to make more than 400,000 songs through a combination of machine learning and input from human users. Earlier this year, the start-up Endel released a series of albums of AI-generated ambient music on the major streaming services, through a partnership with Warner Music. The company is planning to release 20 albums by the end of this year, all in the vein of chill playlists, with candle-scent-like titles that signal their purpose of conjuring soothing atmospheres: “Clear Night,” “Rainy Night,” “Cloudy Afternoon,” “Cloudy Night,” and “Foggy Morning.” Wordless, nearly formless, and harmless, they’re perfectly functional background music, well-marketed to make an asset of the absence of anything warranting the listener’s attention. A more recent project from another start-up, Auxuman, is an achievement on another level and may well be a watershed in pop music history.						



Auxuman (brand shorthand for auxiliary human) is the brainchild of the British Iranian interdisciplinary artist Ash Koosha (aka Ashkan Kooshenaejad). He first established himself with a couple of pleasantly atmospheric synth-based pop albums, the first of which, Guud, included a single, “I Feel That,” whose official video starred a synthetic semihumanoid, created by the digital artist Hirad Sab. Before turning to AI, Koosha made some multisensory art for VR headsets. Now Koosha and a team of programmers have created a stable of digital music acts who (I’ll use the personal pronoun for human beings in deference to the “uman” in Auxuman) will be releasing a full album of new material every month under the umbrella name Auxuman.
As Koosha has described the Auxuman process to the tech-fan site Digital Trends, the words, music, instruments, and singing voices on the tracks are generated by mining existing music on the Web and processing it to generate new songs. The synthetic artists who are the public face of the work are, in Koosha’s words, “a reflection of human life on the internet.” Their music “comes from stories we have told, ideas we have generated, and opinions we have shared.” The principles are not far removed from Harris’s rules for analyzing the music of other songwriters and factoring in public demand.
There are five digital performers in the Auxuman collective to date: Gemini, Hexe, Mony, Yona, and Zoya. Visually, in the avatar imagery that accompanies each song on YouTube, they’re a mix of racial and ethnic identities, in a few cases thoroughly and indefinitely mixed. Four (Gemini, Hexe, Mony, and Yona) are distinctly female in appearance, one (Zoya) male. The one front and center in group pictures, Yona, is, a pixieish white woman who looks like what she is: a computer’s idea of a pop star.
In September, the first music attributed to the five was released as a 10-track album, Auxuman #1. Taking in these recordings, at first I tried to shake the fact that they were generated by AI, and sought to listen with no preconceptions or expectations. Within a minute, I realized that was pointless and unfair to the work and its digital creators. With Auxuman #1, we are forced to confront a genuinely new type of music that works on its own terms.
All five voices, though somewhat distinct from one another, sound of a piece and appropriately artificial—metallic in tone and rigidly staccato in their diction and phrasing. The voices carry no warmth and have no flexibility. They sound inhuman, soulless, but fascinatingly so. To expect otherwise from them would be as wrongheaded as it was for early listeners of recordings to expect pioneers of the microphone such as Billie Holiday and Frank Sinatra to belt like Al Jolson, Sophie Tucker, and others who needed to bellow to reach the rafters of big concert halls. Yona, in particular, exudes impersonal detachment and superficiality. As well as any other artist I can think of, she gives voice to the sense of isolation that chills the air of the digital world.
The lyrics are an eerie jumble of phrases, mostly trite babbling, not unlike the words of a fair number of pop songs since Charles K. Harris’s day. From time to time, though, the random juxtapositions come together in unnerving, accidental eloquence. In “Oblivious,” Yona sings:
I’ve never felt warm
I’ve never felt warm
Through the lens
Through your lens
I feel warm
We know she cannot feel anything. And she communicates only coldness. Knowing this, as I listened, I found myself projecting onto her and started to feel bad for her. Through my lens, I gave her warmth.
The music on all 10 tracks is perfectly, unsettlingly synthetic. Every note is placed precisely on beat. The harmony in every chord is correct, technically. And yet, nothing sounds quite right. There’s something profoundly but fittingly disturbing about it all. It’s utterly conventional and predictable in its formal structures and musical particulars, but wholly arbitrary, built of nothing but its own surface qualities. It sounds like what it is: code pretending to be life. I can think of nothing more relevant.
	 Auxuman (brand shorthand for auxiliary human) is the brainchild of the British Iranian interdisciplinary artist Ash Koosha (aka Ashkan Kooshenaejad). He first established himself with a couple of pleasantly atmospheric synth-based pop albums, the first of which, Guud, included a single, “I Feel That,” whose official video starred a synthetic semihumanoid, created by the digital artist Hirad Sab. Before turning to AI, Koosha made some multisensory art for VR headsets. Now Koosha and a team of programmers have created a stable of digital music acts who (I’ll use the personal pronoun for human beings in deference to the “uman” in Auxuman) will be releasing a full album of new material every month under the umbrella name Auxuman.
As Koosha has described the Auxuman process to the tech-fan site Digital Trends, the words, music, instruments, and singing voices on the tracks are generated by mining existing music on the Web and processing it to generate new songs. The synthetic artists who are the public face of the work are, in Koosha’s words, “a reflection of human life on the internet.” Their music “comes from stories we have told, ideas we have generated, and opinions we have shared.” The principles are not far removed from Harris’s rules for analyzing the music of other songwriters and factoring in public demand.
There are five digital performers in the Auxuman collective to date: Gemini, Hexe, Mony, Yona, and Zoya. Visually, in the avatar imagery that accompanies each song on YouTube, they’re a mix of racial and ethnic identities, in a few cases thoroughly and indefinitely mixed. Four (Gemini, Hexe, Mony, and Yona) are distinctly female in appearance, one (Zoya) male. The one front and center in group pictures, Yona, is, a pixieish white woman who looks like what she is: a computer’s idea of a pop star.
In September, the first music attributed to the five was released as a 10-track album, Auxuman #1. Taking in these recordings, at first I tried to shake the fact that they were generated by AI, and sought to listen with no preconceptions or expectations. Within a minute, I realized that was pointless and unfair to the work and its digital creators. With Auxuman #1, we are forced to confront a genuinely new type of music that works on its own terms.
All five voices, though somewhat distinct from one another, sound of a piece and appropriately artificial—metallic in tone and rigidly staccato in their diction and phrasing. The voices carry no warmth and have no flexibility. They sound inhuman, soulless, but fascinatingly so. To expect otherwise from them would be as wrongheaded as it was for early listeners of recordings to expect pioneers of the microphone such as Billie Holiday and Frank Sinatra to belt like Al Jolson, Sophie Tucker, and others who needed to bellow to reach the rafters of big concert halls. Yona, in particular, exudes impersonal detachment and superficiality. As well as any other artist I can think of, she gives voice to the sense of isolation that chills the air of the digital world.
The lyrics are an eerie jumble of phrases, mostly trite babbling, not unlike the words of a fair number of pop songs since Charles K. Harris’s day. From time to time, though, the random juxtapositions come together in unnerving, accidental eloquence. In “Oblivious,” Yona sings:
I’ve never felt warm
I’ve never felt warm
Through the lens
Through your lens
I feel warm
We know she cannot feel anything. And she communicates only coldness. Knowing this, as I listened, I found myself projecting onto her and started to feel bad for her. Through my lens, I gave her warmth.
The music on all 10 tracks is perfectly, unsettlingly synthetic. Every note is placed precisely on beat. The harmony in every chord is correct, technically. And yet, nothing sounds quite right. There’s something profoundly but fittingly disturbing about it all. It’s utterly conventional and predictable in its formal structures and musical particulars, but wholly arbitrary, built of nothing but its own surface qualities. It sounds like what it is: code pretending to be life. I can think of nothing more relevant.
	 As Koosha has described the Auxuman process to the tech-fan site Digital Trends, the words, music, instruments, and singing voices on the tracks are generated by mining existing music on the Web and processing it to generate new songs. The synthetic artists who are the public face of the work are, in Koosha’s words, “a reflection of human life on the internet.” Their music “comes from stories we have told, ideas we have generated, and opinions we have shared.” The principles are not far removed from Harris’s rules for analyzing the music of other songwriters and factoring in public demand.
There are five digital performers in the Auxuman collective to date: Gemini, Hexe, Mony, Yona, and Zoya. Visually, in the avatar imagery that accompanies each song on YouTube, they’re a mix of racial and ethnic identities, in a few cases thoroughly and indefinitely mixed. Four (Gemini, Hexe, Mony, and Yona) are distinctly female in appearance, one (Zoya) male. The one front and center in group pictures, Yona, is, a pixieish white woman who looks like what she is: a computer’s idea of a pop star.
In September, the first music attributed to the five was released as a 10-track album, Auxuman #1. Taking in these recordings, at first I tried to shake the fact that they were generated by AI, and sought to listen with no preconceptions or expectations. Within a minute, I realized that was pointless and unfair to the work and its digital creators. With Auxuman #1, we are forced to confront a genuinely new type of music that works on its own terms.
All five voices, though somewhat distinct from one another, sound of a piece and appropriately artificial—metallic in tone and rigidly staccato in their diction and phrasing. The voices carry no warmth and have no flexibility. They sound inhuman, soulless, but fascinatingly so. To expect otherwise from them would be as wrongheaded as it was for early listeners of recordings to expect pioneers of the microphone such as Billie Holiday and Frank Sinatra to belt like Al Jolson, Sophie Tucker, and others who needed to bellow to reach the rafters of big concert halls. Yona, in particular, exudes impersonal detachment and superficiality. As well as any other artist I can think of, she gives voice to the sense of isolation that chills the air of the digital world.
The lyrics are an eerie jumble of phrases, mostly trite babbling, not unlike the words of a fair number of pop songs since Charles K. Harris’s day. From time to time, though, the random juxtapositions come together in unnerving, accidental eloquence. In “Oblivious,” Yona sings:
I’ve never felt warm
I’ve never felt warm
Through the lens
Through your lens
I feel warm
We know she cannot feel anything. And she communicates only coldness. Knowing this, as I listened, I found myself projecting onto her and started to feel bad for her. Through my lens, I gave her warmth.
The music on all 10 tracks is perfectly, unsettlingly synthetic. Every note is placed precisely on beat. The harmony in every chord is correct, technically. And yet, nothing sounds quite right. There’s something profoundly but fittingly disturbing about it all. It’s utterly conventional and predictable in its formal structures and musical particulars, but wholly arbitrary, built of nothing but its own surface qualities. It sounds like what it is: code pretending to be life. I can think of nothing more relevant.
	 There are five digital performers in the Auxuman collective to date: Gemini, Hexe, Mony, Yona, and Zoya. Visually, in the avatar imagery that accompanies each song on YouTube, they’re a mix of racial and ethnic identities, in a few cases thoroughly and indefinitely mixed. Four (Gemini, Hexe, Mony, and Yona) are distinctly female in appearance, one (Zoya) male. The one front and center in group pictures, Yona, is, a pixieish white woman who looks like what she is: a computer’s idea of a pop star.
In September, the first music attributed to the five was released as a 10-track album, Auxuman #1. Taking in these recordings, at first I tried to shake the fact that they were generated by AI, and sought to listen with no preconceptions or expectations. Within a minute, I realized that was pointless and unfair to the work and its digital creators. With Auxuman #1, we are forced to confront a genuinely new type of music that works on its own terms.
All five voices, though somewhat distinct from one another, sound of a piece and appropriately artificial—metallic in tone and rigidly staccato in their diction and phrasing. The voices carry no warmth and have no flexibility. They sound inhuman, soulless, but fascinatingly so. To expect otherwise from them would be as wrongheaded as it was for early listeners of recordings to expect pioneers of the microphone such as Billie Holiday and Frank Sinatra to belt like Al Jolson, Sophie Tucker, and others who needed to bellow to reach the rafters of big concert halls. Yona, in particular, exudes impersonal detachment and superficiality. As well as any other artist I can think of, she gives voice to the sense of isolation that chills the air of the digital world.
The lyrics are an eerie jumble of phrases, mostly trite babbling, not unlike the words of a fair number of pop songs since Charles K. Harris’s day. From time to time, though, the random juxtapositions come together in unnerving, accidental eloquence. In “Oblivious,” Yona sings:
I’ve never felt warm
I’ve never felt warm
Through the lens
Through your lens
I feel warm
We know she cannot feel anything. And she communicates only coldness. Knowing this, as I listened, I found myself projecting onto her and started to feel bad for her. Through my lens, I gave her warmth.
The music on all 10 tracks is perfectly, unsettlingly synthetic. Every note is placed precisely on beat. The harmony in every chord is correct, technically. And yet, nothing sounds quite right. There’s something profoundly but fittingly disturbing about it all. It’s utterly conventional and predictable in its formal structures and musical particulars, but wholly arbitrary, built of nothing but its own surface qualities. It sounds like what it is: code pretending to be life. I can think of nothing more relevant.
	 In September, the first music attributed to the five was released as a 10-track album, Auxuman #1. Taking in these recordings, at first I tried to shake the fact that they were generated by AI, and sought to listen with no preconceptions or expectations. Within a minute, I realized that was pointless and unfair to the work and its digital creators. With Auxuman #1, we are forced to confront a genuinely new type of music that works on its own terms.
All five voices, though somewhat distinct from one another, sound of a piece and appropriately artificial—metallic in tone and rigidly staccato in their diction and phrasing. The voices carry no warmth and have no flexibility. They sound inhuman, soulless, but fascinatingly so. To expect otherwise from them would be as wrongheaded as it was for early listeners of recordings to expect pioneers of the microphone such as Billie Holiday and Frank Sinatra to belt like Al Jolson, Sophie Tucker, and others who needed to bellow to reach the rafters of big concert halls. Yona, in particular, exudes impersonal detachment and superficiality. As well as any other artist I can think of, she gives voice to the sense of isolation that chills the air of the digital world.
The lyrics are an eerie jumble of phrases, mostly trite babbling, not unlike the words of a fair number of pop songs since Charles K. Harris’s day. From time to time, though, the random juxtapositions come together in unnerving, accidental eloquence. In “Oblivious,” Yona sings:
I’ve never felt warm
I’ve never felt warm
Through the lens
Through your lens
I feel warm
We know she cannot feel anything. And she communicates only coldness. Knowing this, as I listened, I found myself projecting onto her and started to feel bad for her. Through my lens, I gave her warmth.
The music on all 10 tracks is perfectly, unsettlingly synthetic. Every note is placed precisely on beat. The harmony in every chord is correct, technically. And yet, nothing sounds quite right. There’s something profoundly but fittingly disturbing about it all. It’s utterly conventional and predictable in its formal structures and musical particulars, but wholly arbitrary, built of nothing but its own surface qualities. It sounds like what it is: code pretending to be life. I can think of nothing more relevant.
	 All five voices, though somewhat distinct from one another, sound of a piece and appropriately artificial—metallic in tone and rigidly staccato in their diction and phrasing. The voices carry no warmth and have no flexibility. They sound inhuman, soulless, but fascinatingly so. To expect otherwise from them would be as wrongheaded as it was for early listeners of recordings to expect pioneers of the microphone such as Billie Holiday and Frank Sinatra to belt like Al Jolson, Sophie Tucker, and others who needed to bellow to reach the rafters of big concert halls. Yona, in particular, exudes impersonal detachment and superficiality. As well as any other artist I can think of, she gives voice to the sense of isolation that chills the air of the digital world.
The lyrics are an eerie jumble of phrases, mostly trite babbling, not unlike the words of a fair number of pop songs since Charles K. Harris’s day. From time to time, though, the random juxtapositions come together in unnerving, accidental eloquence. In “Oblivious,” Yona sings:
I’ve never felt warm
I’ve never felt warm
Through the lens
Through your lens
I feel warm
We know she cannot feel anything. And she communicates only coldness. Knowing this, as I listened, I found myself projecting onto her and started to feel bad for her. Through my lens, I gave her warmth.
The music on all 10 tracks is perfectly, unsettlingly synthetic. Every note is placed precisely on beat. The harmony in every chord is correct, technically. And yet, nothing sounds quite right. There’s something profoundly but fittingly disturbing about it all. It’s utterly conventional and predictable in its formal structures and musical particulars, but wholly arbitrary, built of nothing but its own surface qualities. It sounds like what it is: code pretending to be life. I can think of nothing more relevant.
	 The lyrics are an eerie jumble of phrases, mostly trite babbling, not unlike the words of a fair number of pop songs since Charles K. Harris’s day. From time to time, though, the random juxtapositions come together in unnerving, accidental eloquence. In “Oblivious,” Yona sings:
I’ve never felt warm
I’ve never felt warm
Through the lens
Through your lens
I feel warm
We know she cannot feel anything. And she communicates only coldness. Knowing this, as I listened, I found myself projecting onto her and started to feel bad for her. Through my lens, I gave her warmth.
The music on all 10 tracks is perfectly, unsettlingly synthetic. Every note is placed precisely on beat. The harmony in every chord is correct, technically. And yet, nothing sounds quite right. There’s something profoundly but fittingly disturbing about it all. It’s utterly conventional and predictable in its formal structures and musical particulars, but wholly arbitrary, built of nothing but its own surface qualities. It sounds like what it is: code pretending to be life. I can think of nothing more relevant.
	 I’ve never felt warm
I’ve never felt warm
Through the lens
Through your lens
I feel warm We know she cannot feel anything. And she communicates only coldness. Knowing this, as I listened, I found myself projecting onto her and started to feel bad for her. Through my lens, I gave her warmth.
The music on all 10 tracks is perfectly, unsettlingly synthetic. Every note is placed precisely on beat. The harmony in every chord is correct, technically. And yet, nothing sounds quite right. There’s something profoundly but fittingly disturbing about it all. It’s utterly conventional and predictable in its formal structures and musical particulars, but wholly arbitrary, built of nothing but its own surface qualities. It sounds like what it is: code pretending to be life. I can think of nothing more relevant.
	 The music on all 10 tracks is perfectly, unsettlingly synthetic. Every note is placed precisely on beat. The harmony in every chord is correct, technically. And yet, nothing sounds quite right. There’s something profoundly but fittingly disturbing about it all. It’s utterly conventional and predictable in its formal structures and musical particulars, but wholly arbitrary, built of nothing but its own surface qualities. It sounds like what it is: code pretending to be life. I can think of nothing more relevant.
	  David Hajduis the music critic of The Nation and a professor at Columbia University. 
						To submit a correction for our consideration, click here.  For Reprints and Permissions, click here. 			
			In order to comment, you must be logged in as a paid subscriber. Click here to log in or subscribe.		 
				Logged in as 
				
 
				Log out?
 
Comment

 



 
					
						Sign up for our free daily newsletter, along with occasional offers for programs that support our journalism. By signing up to receive emails, you agree to receive occasional promotional offers for programs that support The Nation's journalism. You may unsubscribe or adjust your preferences at any time. You can read our Privacy Policy here. 
					
					 Copyright (c) 2020 The Nation Company LLC",0.09177304009731538,0.44949111543314446
70,https://news.google.com/articles/CBMiXGh0dHBzOi8vZnV0dXJpc20uY29tL3RoZS13b3JsZHMtZmlyc3QtYWxidW0tY29tcG9zZWQtYW5kLXByb2R1Y2VkLWJ5LWFuLWFpLWhhcy1iZWVuLXVudmVpbGVk0gFgaHR0cHM6Ly9mdXR1cmlzbS5jb20vdGhlLXdvcmxkcy1maXJzdC1hbGJ1bS1jb21wb3NlZC1hbmQtcHJvZHVjZWQtYnktYW4tYWktaGFzLWJlZW4tdW52ZWlsZWQvYW1w?hl=en-US&gl=US&ceid=US%3Aen,"“Break Free” is the first sone released in a new album by Taryn Southern. The song, indeed, the entire album, features an artist known as Amper—but what looks like a typical collaboration between artists is actually much more than that. Taryn is no stranger to the music and entertainment industry. She is a singer and digital storyteller who has amassed more than 500 million views on YouTube, and she has over 450 thousand subscribers. On the other hand, Amper is making his debut…except he’s (it’s?) not a person. Amper is an artificially intelligent music composer, producer, and performer. The AI was developed by a team of professional musicians and technology experts, and it’s the the very first AI to compose and produced an entire music album. The album is called I AM AI, and the featured single is set to release on August 21, 2017. Check out the song “Break Free” in the video below:  As film composer Drew Silverstein, one of Amper’s founders, explained to TechCrunch, Amper isn’t meant to act totally on its own, but was designed specifically to work in collaboration with human musicians: “One of our core beliefs as a company is that the future of music is going to be created in the collaboration between humans and AI. We want that collaborative experience to propel the creative process forward.” That said, the team notes that, contrary to the other songs that have been released by AI composers, the chord structures and instrumentation of “Break Free” are entirely the work of Amper’s AI. Ultimately, Amper breaks the model followed by today’s music-making AIs. Usually, the original work done by the AI is largely reinterpreted by humans. This means that humans are really doing most of the legwork. As the team notes in their press release, “the process of releasing AI music has involved humans making signiﬁcant manual changes—including alteration to chords and melodies—to the AI notation.” That’s not the case with Amper. As previously noted, the chord structures and instrumentation is purely Amper’s; it just works with manual inputs from the human artist when it comes to style and overall rhythm. And most notably, Amper can make music through machine learning in just seconds. Here’s an example of a song made by Amper, and re-arranged by Taryn.  Yet, while IAMAI may be the first album that’s entirely composed and produced by an AI, it’s not the first time an AI has displayed creativity in music or in other arts. For example, an AI called Aiva has been taught to compose classical music, like how DeepBach was designed to create music inspired by Baroque artist Johann Sebastian Bach. With this in mind, the album is likely just the first step into a new era…an era in which humans will share artistry (and perhaps even compete creatively) with AI. Editor’s Note: This article has been updated to clarify what songs were made by Amper and rearranged by Taryn. ",0.1711203651421043,0.46924524750611696
71,https://news.google.com/articles/CBMiOWh0dHBzOi8vaHlwZXJhbGxlcmdpYy5jb20vNTQxMDk0L2FpZG9sLWxhd3JlbmNlLWxlay1pZmZyL9IBAA?hl=en-US&gl=US&ceid=US%3Aen,,0.0,0.0
72,https://news.google.com/articles/CBMiZ2h0dHBzOi8vd3d3Lm11c2ljYnVzaW5lc3N3b3JsZHdpZGUuY29tL2lzLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWFib3V0LXRvLXRyYW5zZm9ybS10aGUtc3luYy1pbmR1c3RyeS_SAQA?hl=en-US&gl=US&ceid=US%3Aen,"There’s been plenty of discussion and debate on MBW’s pages regarding the impact that Artificial Intelligence might have on the music business in the future. Obviously, there’s its potentially seismic effect on the way musicians make music – whether that’s AI producing non-human music from scratch, or providing tools that artists and songwriters can use to compose and perform in the studio. But there’s also AI’s application to more practical B2B tools to consider. Just last week, for example, we heard from Canada-based LANDR, which has launched an AI tool that helpfully sifts through its huge catalog of samples for those looking for a specific sound. Today, (September 4), a new twist on AI arrives via a fresh partnership between production music library Audio Network and Singapore-based machine learning company, Musiio. In a nutshell, Audio Network – which was acquired for $215m by Entertainment One earlier this year – will now be able to provide its clients with an AI-enhanced search function to help them find the right track amongst’s AN’s 170,000-strong catalog. Singapore-based Musiio claims it can provide B2B clients a new way of “listening” to music at scale, searching up to one million tracks in under two seconds. The hope, therefore, is that big sync agencies, or TV/movie/video game/ad houses, will be able to find the perfect track for their campaign, faster, within Audio Network’s database. “We view machine learning and AI techniques as instruments for extending human creativity, not something that replaces it.”  Matthew Hawn, Audio Network “AI has been on the fringes of the music industry for the last few years, with talk of labels signing algorithms. But recently, more commercial and practical uses of this powerful computing technology have begun to surface,” explains Musiio CEO and co-founder Hazel Savage. “This deal demonstrates how AI and technology companies like ours can work with a company like Audio Network, to protect everything that is great about the music industry, the personal touch, the knowledge of experts and also to create tools that let the team step up to the challenge of a huge and fast-growing industry.” “Audio Network approaches Artificial Intelligence carefully with regard to music. We view machine learning and AI techniques as instruments for extending human creativity, not something that replaces it,” said Matthew Hawn, Chief Product Officer at Audio Network. “Many AI tools for music are clever technology hacks in search of an actual problem to solve, but our partnership with Musiio for extending music search and recommendations is about solving real problems for our customers. “Blending Musiio’s technology with our expert human curation will mean our artists and composers’ music is more accessible to more customers globally.”Music Business Worldwide The best of MBW, plus the most important music biz stories on the web. Delivered for FREE, direct to your inbox each day.",0.19972462722462728,0.49164381914381916
73,https://news.google.com/articles/CBMiiAFodHRwczovL3d3dy5haXRob3JpdHkuY29tL21hY2hpbmUtbGVhcm5pbmcvYW1wZXItbXVzaWMtcmFpc2VzLTRtLXRvLWZ1ZWwtZ3Jvd3RoLW9mLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLW11c2ljLWNvbXBvc2l0aW9uLXRlY2hub2xvZ3kv0gEA?hl=en-US&gl=US&ceid=US%3Aen," AIT News Desk   24 Mar 2018  Machine Learning  Leave a comment   1556 Views  Funding Enables Amper Music To Expand Internationally And Scale Internal Resources To Meet Rising Global Popularity Amper Music, the world’s first artificial intelligence (AI) music composer, performer, and producer that lets you instantly create and customize original music without needing any prior music experience, today announced it has raised an additional $4 million seed round. The financing was led by Horizons Ventures with Two Sigma Ventures, Advancit Capital, Foundry Group and Kiwi Venture Partners also participating. This brings the company’s total investment to $9 million. Amper will use the latest funding to expand the company’s presence internationally, and it plans to double its employee base in the US to meet the growing customer desire for scalable, high-quality music creation. “Amper’s rapid growth is a testament to how the massive growth of media requires a technological solution for music creation,” said Drew Silverstein, CEO and Co-founder of Amper Music. “Amper’s value stems not only from the means to collaborate and create music through AI, but also from its ability to help power media at a global scale.” Founded by Hollywood film composers Drew Silverstein, Sam Estes, and Michael Hobe, Amper Music is for anyone around the world who uses, consumes or creates music. Amper is especially appealing to content creators and developers. Using Amper, content creators can simplify their workflow and avoid the time, budgetary, and licensing frustrations of stock music. The music created with Amper receives a global, royalty-free, and perpetual license, eliminating the many legal and financial hurdles of traditional music licensing. Users can select a mood, genre and track duration, then customize the track with a multitude of easy-to-use editing functionality. Amper also serves as a creative partner to artists, musicians, and composers, who can collaborate with Amper in the creation of their own music. “Music is inherently a cross-cultural and geographically boundless experience,” said Phil Chen, Advisor at Horizon Ventures. “That factor coupled with the rising intrigue of mixing music and machines well positions Amper for global expansion.” “We’re excited to continue to support Amper through this next phase of growth and expansion,” says Colin Beirne of Two Sigma Ventures. “Amper continues to push the envelope in delivering AI-powered music composition technology, giving its content-rich customers and partners access to high-quality customized music at the touch of a button.” Amper Music is the world’s first artificial intelligence (AI) music platform that empowers media creators to effortlessly collaborate with Amper’s web application and API to craft and customize original music, instantly, without needing any prior music experience. From video to gaming and other interactive technologies, Amper powers the music for all forms of media content. Amper enables creators to avoid the time, budgetary, and licensing frustrations of stock music. Through its simplified workflow, creators can produce original soundtracks without leaving their existing platform. Your email address will not be published. Required fields are marked * Comment Name *  Email *  Website       This site uses Akismet to reduce spam. Learn how your comment data is processed.",0.07770270270270269,0.49121621621621625
74,https://news.google.com/articles/CAIiELetVrkoKnCN4htuPEExNEwqFQgEKg0IACoGCAowrqkBMKBFMMGBAg?hl=en-US&gl=US&ceid=US%3Aen,"Sign in to your Forbes account or register For instructions on how to disable your ad blocker, click here. If this is your first time registering, please check your inbox for more information about the benefits of your Forbes account and what you can do next! Google Doodle In honor of Johann Sebastian Bach’s birthday, which would be his 333rd, Google created an AI Doodle on the homepage of their search to honor him and celebrate modern technology.  Created by Google’s Magenta and PAIR teams, the Doodle lets users create their own music by using machine learning to harmonize melodies. Magenta was responsible for the machine learning aspect of the project while PAIR created the ability to use it in the web browser. The machine-learning model, called Coconet, analyzed 306 of Bach’s original chorale harmonizations so it was able to create a harmonized tune with the user’s notes. This opens up the floor for debate on AI in music and whether or not it can create music like a human and what that means for artists in the industry.  Many debates have surfaced around this issue when it comes to AI being a part of the music industry and the authenticity of it.  
 This is Google’s first dive into AI and music but they are far from being the first. It has been a practice since in the ’90s when David Bowie helped develop an app called Verbasizer. The app took literary source material randomly reordered words to make into lyrics.  Now there are many user-facing platforms that help people create music with AI as well. One of those is Amper. According to their site, “Amper Score™ enables enterprise teams to compose custom music in seconds and reclaim the time spent searching through stock music. Whether you need music for a video, podcast, or another project, Score’s Creative AI quickly makes music that fits the exact style, length, and structure you want.” Most artificial intelligence for music production uses neural networks modeled around machine learning of a vast data of musical scores. The algorithm analyzes the patterns of notes and how they relate to one another and how often notes go together. The system can then produce melodies and chords, like the Google Doodle, and make it into tunes.  Amper’s system doesn’t actually use neural networks and instead, it uses the rules of music theory to recognize what music triggers emotions instead of recognizing musical scores. It then uses that algorithm to produce tracks. Out of this comes a new genre of music that some are calling “AI pop.”  Here is one of those songs that have come from this AI: While it’s not going to be a Top 100 anytime soon, it surely isn’t far from being something that could hit the mainstream media eventually. As a lot of industries have expressed concerns about AI taking jobs, music is no different. However, like many other conclusions with the topic, the likelihood of AI taking over music altogether is very unlikely. The chances are that it will become a tool in the industry to streamline music and improve quality.  Nicole Martin is the owner of NR Digital Consulting and host of Talk Digital To Me Podcast. She has worked in many different industries on customer journeys, website… Nicole Martin is the owner of NR Digital Consulting and host of Talk Digital To Me Podcast. She has worked in many different industries on customer journeys, website management, social media, and content strategy. Starting out as a journalist, Nicole has a BA in Print Journalism from Penn State University and an MA in Journalism & Mass Communication from Point Park University. After working with multiple
publications, she saw the industry rapidly changing to digital and decided to change career directions to focus on digital marketing in the new market. She taught English 420: Writing for the Web at Penn State University as an Adjunct Professor and is also an avid public speaker and presenter. Nicole recently did a TEDx Talk on Personalization and the Privacy Paradox and currently works as the Digital Marketing Manager at Sonic Healthcare USA in Austin, Texas. She is now using her knowledge in the field combined with her writing background to share information about technology, digital marketing, data, and innovation.",0.12889807162534433,0.3887511478420569
75,https://news.google.com/articles/CBMiX2h0dHBzOi8vd3d3LnRoZWRydW0uY29tL29waW5pb24vMjAyMC8wMS8xNC93aHktYWktbXVzaWMtd2lsbC1sZXQtaHVtYW4tYXJ0aXN0cy1iZS1sZXNzLW1hY2hpbmVz0gEA?hl=en-US&gl=US&ceid=US%3Aen,"
                    The Latest news for the marketing & media industries.
                 
                    Explore the latest, and greatest, creative work from around the globe.
                 
                    Providing great companies with the recognition they deserve.
                 
                    Holding events to support, inform, challenge and advise.
                 
                    Latest insights, case studies and news from agencies, tech vendors, freelancers and other organisations.
                 
                    Search 2,345 jobs in marketing, advertising, creative and media.
                 
                 Take a fresh approach to raising your profile with potential clients.
                 
                    Features providing insights into the marketing industries.
                 
                    Creating compelling content your customers will love.
                 
                    The fastest way to find the right agency
                 If you had any lingering doubts about the need to embrace digital transformation, coronavirus might just have changed your mind. As business grapples with the physical challenges this represents, digital-first companies continue to flourish. 
                            The Drum Digital Transformational launch
                         “Music inspires us. We believe in the power of music to inspire”... so proclaims the new Apple Music for Business landing page. As a concept, music for business is booming, but when brands talk about ‘music’ can we take them at face value? Is it really ‘music’, stripped of all context, that allows brands to build emotional connections and affinity - or is it the people who make that music?  As a lover and creator of music myself, I see musicians as creatively equal to filmmakers. Their purpose is not only to be synced to film or video. Unfortunately, a lot of the time when brands use music it is relatively anonymously and without proper compensation for the artist. The rise of AI music will place the value of human musical compositions back on its story-telling ability, rather than its ability to provide a neutral backdrop for others' work. Content needs a soundtrack to truly bring it to life. But the proliferation of storytellers on YouTube and TikTok and ever-growing need for safe-to-use, contextually relevant music to accompany online content has left us wanting; not only is there now far too much content, but also devices, and both physical and virtual spaces, that need accompanying music, and the choices out there don’t always deliver in the way we want them to. Content creators don’t have enough time—or money—to license or to commission backing soundtracks for every video they post. YouTube hasn’t succeeded in creating a fully ‘royalty-free’ music library, though they’ve tried. So creators and brands resort to stock music.  Although it can be of an incredibly high quality, composed by very talented people who would probably rather be focusing on writing the next rock opera rather than for a commercial incentive, stock music is a quick fix.  This is where AI composed music truly shines. Despite concerns about AI stealing the jobs of real artists, in this space, it offers a huge benefit to musicians. Rather than creating music for royalty-free music libraries, musicians will be given free rein to create, compose, distribute, and perform in a way that is meaningful to them and not some faceless brand or playlist. AI music will never achieve the majesty of Bohemian Rhapsody, not for its sonic attributes, but for its iconic cultural status that is innately human.  And even if AI did create a sonically “perfect” track in a vacuum, it can never replace or recreate the cultural significance of a song that has been written by a human. Cultural currency and sonic currency can and should be measured separately now that AI can create and most likely commoditize purely acoustic “filler” stock music.  It has been said that the invention of the washing machine changed the world more than the internet. By freeing up time that would have otherwise been spent doing hours and hours of laundry, the revolutionary new tech allowed women to enter the labour market and fundamentally change society forever. This is how I see AI music’s role in the creative ecosystem; rather than taking jobs, it frees up huge amounts of time - time that can now be spent by musicians to put their abundant talent and expression towards songwriting, symphonies and storymaking. Taishi Fukuyama is the co-founder and chief operating officer at Evoke Music  This article is about: Japan, Music Marketing, Advertising, Digital Advertising, Marketing, Social Media, Digital, Arts, B2B, Technology 
                    Become a member to get access to:
                 Get empowered. Hit the C-suite spot. 75% of The Drum Magazine readership are senior management or above. Benefit from our monthly exclusive magazine content in multi-format. Subscribe today and be educated, entertained and empowered. © Carnyx Group Ltd 2020 | The Drum is a Registered Trademark and property of Carnyx Group Limited. All rights reserved. Want to read this article and others just like it? All you need to do is become a member of The Drum. Basic membership is quick, free and you will be able to receive daily news updates.",0.20469396344396346,0.4528478234728236
76,https://news.google.com/articles/CBMiRWh0dHBzOi8vbXVzaWNhbGx5LmNvbS8yMDE5LzEyLzExL2FpLW11c2ljLXBvcGd1bi1jb25zdW1lci1hcHAtc3BsYXNoL9IBAA?hl=en-US&gl=US&ceid=US%3Aen,"
Username or Email Address

 
Password

  Remember Me 


 Our recent interview with Stephen Phillips, CEO of AI-music startup Popgun, outlined his belief that an app for teenagers to make AI-powered music might be the ultimate ‘end-game’ for this technology. Now his company has launched… a music-making app for teenagers! Well, any-age people really: the key is that they don’t need prior musical knowledge to use Splash, which is available for iOS and Android. It’s a free download, and looks similar to popular music-creation app Groovepad, in that users choose genre-based packs of loops, then tap to trigger drums, basslines, synths, vocals and FX. They can then record their songs, add cover artwork and share them with the world. “The first version of Splash is similar to Groovepad and other launchpad apps. We really liked the simplicity and accessibility of that way of making music. Kids can immediately start making music and with a little practice get good really fast. We could see how our AI tech could make that experience much more compelling,” Phillips told Music Ally. “While the UX [user experience] is similar, Splash is different in several ways. Splash does not have ads and all pads are free. The sound packs are designed by human producers using Splash Pro, our AI app for professionals. This lets us quickly compose and expand sound packs. In future versions of the app, we will make our AI available to users directly, giving them more variety and control of the music. We are excited to see what kids will make using our AI singers and musicians.” But with no ads, what will the business model be? “We plan to sell licensed artist sound packs which will let fans dive deeper into the sound of the musicians they love most.” AI-music startup @WeArePopgun has launched a Groovepad-style music-making app. It uses sound packs created by human producers using its pro AI tools. Very interesting, full story is here: https://t.co/AD4sMEhqdZ pic.twitter.com/TMUXP6FYqg — Stuart Dredge (@stuartdredge) December 11, 2019  (If you’re wondering what the legals are around the songs created in Splash, that’s made clear on the app’s website: “You may use the sound recordings you create using the App for any commercial or non-commercial purpose, on a royalty-free basis, however your use of the App and the Content does not transfer to you ownership of any IP Rights in the App or the Content.”) Phillips also outlined the longer-term vision for Splash, as Popgun sees it. “Making music is so much fun and remains one of our most powerful forms of self-expression. Yet there are still millions of people who try to make music and fail. We think AI will solve this problem,” he said. “We plan to grow Splash into the next big social music network with millions of people making and sharing music. For the past three years Popgun has been developing our core AI tech and in 2020 we will make it available inside Splash so everyone can experience the joy of making music.” As a reminder, here’s the even bigger vision that Phillips talked to Music Ally about in our previous interview, conducted for our recent report on AI music. “What’s the end-game for this? There isn’t this place in the world where teenagers come together to make music for each other. That place does not exist, and that’s nuts! That thing needs to exist, and it will exist. And getting the AI working is the price of admission to build that thing,” he said then. “Where’s the ‘pop stars on training wheels’ place where they make music for each other, release it and watch each other pretend to be pop stars, but then go on to become legitimate pop stars? Who’s going to create that space where the next Billie Eilish comes from?” he added. “The current pop industry is very few musicians controlled by three or four companies, played by a billion people. The new thing has to start with kids making music for each other and ignoring all of that world. It’s still too hard, though, and AI has to be the answer for that.” For more background on the technology Popgun has been working on, read our first interview with Phillips from 2017, and our follow-up in November 2018. (All fields required)   Music Ally Ltd., Holborn Studios,
49-50 Eagle Wharf Rd, London, N1 7ED,
United Kingdom Music Ally is a Registered Learning Provider 10029483",0.17411861148801455,0.406112618724559
77,https://news.google.com/articles/CBMiOGh0dHBzOi8vd3d3Lm11c2ljdGVjaC5uZXQvbmV3cy9hdXh1bWFuLXJlbGVhc2UtYWktYWxidW0v0gE8aHR0cHM6Ly93d3cubXVzaWN0ZWNoLm5ldC9uZXdzL2F1eHVtYW4tcmVsZWFzZS1haS1hbGJ1bS8_YW1w?hl=en-US&gl=US&ceid=US%3Aen,"The collective consists of five AI characters, who each contribute tracks to the fully AI produced compilation.
 Auxuman, an AI-music startup company, have released their first fully AI album, Auxuman Vol.1. The project consists of 10 tracks by the five AI ‘artists’ – Yona, Hexe, Mony, Gemini and Zoya. Yona has already released projects of her own, in collaboration with Ash Koosha, co-founder and CEO of Auxuman. As stated on their Bandcamp page, Auxuman plan to release monthly compilations of their work, available on YouTube, Bandcamp, Spotify and more. Auxuman says “the creation music and lyrics are driven by their digital talent which uses Artificial Intelligence and generative tools, introducing a new ‘soul’ behind ‘non-human content’”. This is not the first we’ve seen from AI music creation. Just last week, JAM announced its AI composing software, using pre-made loops to create a genre-specific piece of music. Boomy is a web-based platform that can generate short pieces of music, which can be stylised by the user. Also, Endel, another AI-music startup, released five albums this year with an ambient, relaxing theme throughout. The music is certainly unique, and may not be to everyone’s taste. I have to admit, it’s all a bit slightly eerie, but I can’t help but nod my head to Try Me, which seems to have sampled some famous Vine clips over the top of some heavy kick drums and an EDM style lead.  Find out more at auxuman.space Be updated with all the latest news, offers and special announcements. Subscribe now and receive a free chorus plug-in! We provide insight and opinion on the gear, tools, software and services to enhance and expand the minds of music makers and listeners. © 2020 MusicTech is a member of the media division of BandLab Technologies.",0.1794526901669759,0.5647289218717789
78,https://news.google.com/articles/CBMiXWh0dHBzOi8vbXVzaWNhbGx5LmNvbS8yMDE5LzA5LzAyL3N0YXJ0dXAtcmVzb25vby1pcy1wdXR0aW5nLWFpLXRvLXdvcmstb24tc3BvdHRpbmctaGl0LXNvbmdzL9IBAA?hl=en-US&gl=US&ceid=US%3Aen,"
Username or Email Address

 
Password

  Remember Me 


 Back in May 2018, Music Ally spotted a new startup called OptimiseLab, which was building “AI technology to analyse, improve, and compose music”. Initially, it was focusing on the first of those: a tool that “accurately predicts music popularity scores and provides customised advice on song improvement”. 15 months on, and OptimiseLab has just rebranded to Resonoo, and it’s officially launching today. The basic idea remains the same: “an AI system that accurately evaluates the potential of unpublished songs”, which Resonoo is hoping will be used by labels to identify the most promising songs when deciding where to put their promotional efforts, as well as to spot artists with potential. “The system is trained on songs date back to when music was first digitally recorded, analysing trends and music elements to accurately predict the potential of songs,” runs the company’s announcement pitch, which claims the system “achieves a 90% accuracy predicting potential of songs across five genres” (including 97% for electronic music). Big claims, but some major labels are testing the invitation-only system already to put it through its paces. (All fields required)   Music Ally Ltd., Holborn Studios,
49-50 Eagle Wharf Rd, London, N1 7ED,
United Kingdom Music Ally is a Registered Learning Provider 10029483",0.12994318181818182,0.44356060606060604
79,https://news.google.com/articles/CBMiPWh0dHBzOi8vZm9ydHVuZS5jb20vMjAxOC8xMC8yNS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS1tdXNpYy_SAUFodHRwczovL2ZvcnR1bmUuY29tLzIwMTgvMTAvMjUvYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtbXVzaWMvYW1wLw?hl=en-US&gl=US&ceid=US%3Aen,"Fortune © 2019 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy (Your California Privacy Rights) | CCPA Do Not Sell My Information
FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.
Quotes delayed at least 15 minutes. Market data provided by Interactive Data. ETF and Mutual Fund data provided by Morningstar, Inc. Dow Jones Terms & Conditions: http://www.djindexes.com/mdsidx/html/tandc/indexestandcs.html.
S&P Index data is the property of Chicago Mercantile Exchange Inc. and its licensors. All rights reserved. Terms & Conditions. Powered and implemented by Interactive Data Managed Solutions. | EU Data Subject Requests",0.11445578231292516,0.24676870748299318
80,https://news.google.com/articles/CAIiECShcV5vqm8isCh8-11-FDIqGAgEKg8IACoHCAow6KLyCTDY8XIwtqfsBQ?hl=en-US&gl=US&ceid=US%3Aen,"By Rachel Hahn  When I meet up with Holly Herndon for lunch in Soho in mid-April, congratulations are in order, even though it’s still a month before her third full-length album, Proto, is to be released. The day before, she had defended her doctoral thesis on ethical and aesthetic issues in AI in music at Stanford University’s Center for Computer Research in Music and Acoustics, a subset of the school’s music department. The center has an impressive history—it’s where composer John Chowning first discovered a key technique called FM synthesis, and this lucrative patent still enables the center to fund projects that, according to its website, use “computer-based technology both as an artistic medium and as a research tool.” Herndon, who has a way of parsing what should be daunting technical terminology into language that is not only easy to understand but also compelling, does the same for the center: “It’s this really cool pink building up the hill from the music department, full of computer nerds,” she says. To celebrate the accomplishment, her label sent her a chocolate cake decorated with blue frosting that spelled out her new official title: “Dr. Herndon.” Born in the mountains in East Tennessee, but now based in Berlin, Herndon started singing in her church choir. More recently, she’s spent years studying computer music and making it sound radically human. For her first official release, Movement (2012), which she started working on while studying electronic music at Mills College, she created custom vocal patches that she manipulated live, using her highly processed voice to create subterranean club music. Her sophomore album, 2015’s Platform, took these human-oriented sonics a step further, casting light on the ways in which social media and similar platforms have further ossified preexisting power structures and made surveillance even more quotidian than before. One of the album’s standout tracks, “Chorus,” translates her browsing history data into samples that Herndon masterfully arranges—she’s essentially surveilling herself—and “Lonely at the Top” holds the distinction of being the very first song on a commercial album aimed to trigger ASMR, or autonomous sensory meridian response (that tingling sensation you might feel at the base of your skull when you hear a whispering voice or get a head massage—there’s a whole YouTube subculture dedicated to inducing the sensation in others). Proto is essentially her doctoral thesis come to life. Today, Vogue is premiering Birthing Proto, a documentary produced in partnership with Dropbox, which shines light onto Herndon’s process. Central to Proto’s uncanny valley-esque vocals is something called Spawn (probably because Herndon describes it as her “AI baby”). Spawn is years in the making—after receiving a German grant in 2018 dedicated to composers implementing novel technologies in their work (in honor of Beethoven, no less), Herndon and her partner, artist Mat Dryhurst, alongside musician and developer Jules LaPlace, bought a GPU gaming PC that they customized without any particular end goals in mind. “That was a beautiful way to approach it—just a purely experimental way.” Spawn uses machine-learning programming to produce sound on its own from scratch, thereby “singing” by mimicking the voices of Herndon, Dryhurst, LaPlace, and an ensemble made up of her friends or anyone Herndon knew who had voice training or a musical background, which she assembled weekly at her home in Berlin. Herndon created training sets, which Spawn uses to create its own musical contribution. Depending on what Herndon and her collaborators input, it can take Spawn anywhere between five minutes to a day to produce its own interpretation of the ensemble voices. They also recorded an entire audience at Berlin’s cacophonous Martin-Gropius-Bau, to make a public voice for Spawn to train from too. At times, the results sound almost uncannily pure—the quality of the live vocals of “Cannan (Live Training)” are so resonant and real that you can almost visualize the space they were singing in. At other times, the choir is a bit disorienting; it’s often hard to differentiate between completely synthesized sounds, a human voice that’s been modulated, or Spawn straddling both of those worlds. When Herndon starts explaining Spawn in more detail, she gets so animated that she starts to break a sweat. But after taking her sweatshirt off, while breaking down her intent for her AI baby, Herndon bemoans how much AI research in music is focused on training neural networks to approximate a particular piece of music or style. She uses Beethoven, naturally, as an example. “If you feed a neural net a bunch of Beethoven MIDI data—pitch material and rhythm and note range—the neural network can statistically analyze those relationships and then come out with a piece of music that’s in the style of Beethoven, but isn’t a copyrighted Beethoven song,”  she explains. Herndon thinks this creates a false sense of how advanced AI technology really is: “You create this new score and you usually play that through a digital instrument or your favorite player, and it sounds like AI is really perfect, like it’s super smart and it’s super developed. It doesn’t show its flaws or shortcomings.” There’s an ethical issue at play too, when a computer can extract and automate an entire musical aesthetic  without any kind of real attribution. With Spawn, Herndon wanted to be able to move beyond these entrenched narratives. “How can this technology be used in a way that’s not this kind of retro mania where we’re just regurgitating the past?” she says. “That’s not how music develops.” Humans are essential in Herndon’s project, and in shifting paradigms surrounding machine intelligence. “We wanted to have a sonic fingerprint of the vocalist involved, and deal with AI more as a performer. So we have a human ensemble with an inhuman member,” Herndon says. “Instead of outsourcing my composition to an AI, I’m still the composer. I’m the director of the ensemble, and the AI is an ensemble member that is improvising and singing and performing alongside us.” By centering the voices of herself and her colleagues, Herndon hopes to highlight the human element of AI that many public conversations on machine intelligence obscure. “For Google Translate or something like that, so many of these automated services appear as these really clean, almost magical things, but what’s behind that clean surface is millions of human translations that it was trained on. There’s always human labor that’s made invisible.” Herndon’s work deals with high-level concepts, bringing into play platform and protocol theories and highly technical electronic processes. She asks me, midway through our conversation, what I think a Vogue reader would be interested in regarding Spawn, AI, and music. I turn the question back around to her. “I hope that people start to really think about where ideas come from and how we honor those ideas, and how we can celebrate people who are taking risks, pushing conversations in different directions—not just seeing human culture and society as something that can be hoovered up and played back to us, without any kind of attribution,” Herndon says, drawing an analogy to the ways in which larger fashion houses might co-opt the innovative work of younger designers. Herndon might have lofty conceptual and technological aims with the album—there’s her interest in AI ethics and its influence on societal structures as a whole, as well as her pioneering vocal processing techniques—but at the end of the day, Spawn arose from a much more natural impulse. Herndon is quick to emphasize that Spawn is only a part of the larger ensemble, and that human sounds make up the bulk of the album. “Only about 20 or 25 percent of the sound is AI generated,” Herndon explains. The album also pulls in folk traditions—on “Frontier,” Herndon provides her own take on Appalachian sacred harp music, a nostalgic nod to her rural Southern roots. “So much of it is human, and I think you can hear that it happens in a real space. For computer music, that is something that I was really craving—being in the room with people and singing, the joy of performing with people. It sounds cheesy, but I was missing that,” she says. “That’s how I started making music back in the day, in church: the joy of music making with people in an actual space.” The latest fashion news, beauty coverage, celebrity style, fashion week updates, culture reviews, and videos on Vogue.com. More from Vogue See More Stories © 2020 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement (updated 1/1/20) and Privacy Policy and Cookie Statement (updated 1/1/20) and Your California Privacy Rights. Do Not Sell My Personal Information Vogue may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices CN Fashion & Beauty",0.1607582095082095,0.4343299793299794
81,https://news.google.com/articles/CAIiECrXth0Lfik6uM_aNU7uEeEqGQgEKhAIACoHCAowgvzyCjCh_NkCMNvtxgY?hl=en-US&gl=US&ceid=US%3Aen,,0.0,0.0
82,https://news.google.com/articles/CAIiEHtGgC-HNMp_a3TEWhtuYCoqFggEKg4IACoGCAowl6p7MN-zCTCOvRU?hl=en-US&gl=US&ceid=US%3Aen,"4ADHerndon’s own AI, Spawn, augments her group’s flesh-and-blood vocals to challenge our fear that machines are taking over 
Ben Beaumont-Thomas 

Fri 10 May 2019 09.00 BST

 There’s something soothing about how rubbish Google’s new predictive email tools are – if AI can’t work out what you want to tell your accounts department, then it won’t be organising a Terminator-style insurrection any time soon. So what hope does AI have for composing music, if bland office missives are too creatively challenging? California-based electronic composer Holly Herndon considers this moment of slowly emergent machine learning on her third album. Alongside the musicians in her ensemble is Spawn, an AI she created with husband Mat Dryhurst and developer Jules LaPlace, that listened to what the group was composing and mimicked it to create music of its own. It’s not always clear which bits are Spawn-created, but Herndon obviously has the final say over how its contributions are used – and indeed, the whole album feels more like an announcement of human authority rather than a capitulation to machines. The human voice is the central instrument. It may have its hard edges sheared off in big pop anthem Eternal, consonants softened into near-unintelligibility; it may glitch and fade as Spawn tries to replicate it in the call-and-response chorale Evening Shades. But it endures, tangibly human even when digitally processed. There’s even something pagan and pre-technological about the Celtic cadences on the a cappella duet Canaan, while another choral vocal passage in Crawler feels liturgical. There is perhaps a political dimension to all this: by folding herself into a multiplicity of analogue and digital voices, Herndon counters the individualism of our culture, and suggests that networked communities are just as valid as “real-life” ones. As with her ASMR satire Lonely at the Top on her previous album, some tracks here feel more suited to the art gallery than the home stereo. But there are also bold, accessible commercial tracks, such as Alienation – imagine a power ballad produced by Jon Hopkins – and Frontier, where a crunching dancehall beat is topped with choral voices that recall Sacred Harp or Gaelic psalm singing.  Herndon counters the hysteria around AI with an album that presents it as a quizzical, cute pet on the leash of a human master: a sensitive, responsive part of the family.",0.1157483930211203,0.4415518824609732
83,https://news.google.com/articles/CBMidWh0dHBzOi8vd3d3Lmh5cGVib3QuY29tL2h5cGVib3QvMjAxOS8xMi84LXRvb2xzLWhvdy10by10aGluay1hYm91dC1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1pbi10aGUtbXVzaWMtaW5kdXN0cnkuaHRtbNIBAA?hl=en-US&gl=US&ceid=US%3Aen,"As artificial intelligence continues to grow and develop, its presence is affecting more and more industries. In this piece, Becky Holton explores the impact AI is having on the music business, as well as looking at some of the AI-based tools at play on the music market today. ______________________________ These days, we can all artificial intelligence (AI) all around us. The main purpose of AI is to simulate mental tasks and one of the most important subsets of this technological advancement is machine learning. This is because it has a significant effect on all the other fields within AI.  As AI gets incorporated in different industries, the music industry has now begun using it. With the introduction of AI, the competitive sustainability of organizations and individuals in the music industry has increased. Let’s find out more about AI in the music industry and a few tools that those in the industry are now using.  How AI affects the music industry Most people aren’t used to thinking about AI as something universal and pervasive. But in reality, it already exists all around us and we use different kinds of AI technology in our daily lives. Many industries and businesses use some form of AI technology from online shops, essay writer services, scientific institutions, and more. But what about the music industry? As human beings, we are all naturally creative and the best of us express their creativity through music. While we have come this far without using AI to make music, does this mean that we don’t need such technology for this industry?  When you think about the capacity of AI, the first question that comes to mind is if AI can play “real music.” With all the smart devices that we use, we already know the answer to this question. But even if these technologies can play music, does this mean that it can compose music too? If so, how will this affect the market of the music industry?  While AI can play music created by people or even play music on its own, right now, AI doesn’t have the capacity to compose original music. However, the long-term vision of those who develop AI is for this technological advancement to have the ability to produce certain styles of music after we (humans) feed the right kind of data to it.  AI music enthusiasts and experts feel excited about this because it will allow them to collect valuable data. Also, it will let technicians to see exactly how music gets shaped. But right now, AI can only produce some variations of songs that already exist, not write original scores. Still, it can help producers and musicians improve their skills to produce better music.  The most significant developments A lot of people still fear the rise of AI because they don’t want computers to take over the work that humans do. But AI does offer a lot of benefits too. In the music industry, the most significant benefits of AI come from the following developments: Audio improvements Right now, one of the current roles of AI is in the finalization of music. This occurs when producers master or smooth out aspects of their song production process. In the music industry, experts develop AI technology to understand the cues smoothing professionals look for, then remove them automatically without the need for human interaction.  Songwriting and song development Erica Samson who works as a creative writer for an essay writing service UK says that songwriting is something that you need to practice thoroughly if you want to become a true artist. She adds that when it comes to AI taking over this aspect, there’s a very low risk of this happening.  Instead, AI can provide professional musicians the several opportunities to come up with new, exciting songs. There are now some AI-music development startups that have made the creation of music more accessible.  This is great news for musicians who are just starting out too. Of course, while such technologies can aid in the process, humans still have to do most of the work, especially in terms of coming up with creative content.  AI tools for the music industry It’s true that AI is gradually becoming an essential part of the music industry. This is good news for some while others remain uncertain. If you’re wondering what tools are out there for the musically inclined, here are some examples: Playbeat Playbeat utilizes AI technology to randomize grooves through the manipulation of volume, pitch, and other elements of music. All you have to do is load your music samples, tap/click the randomize button, and Playbeat gets to work. You also have the option to adjust parameters manually if you want to.  Aiva Technologies Aiva Technologies focuses on creating music for movies, commercials, and video games. It has analyzed musical creations of several classical composers and has created its own variations of these. Jukedeck Jukedeck is a paid AI tool for individuals and businesses. The developers have put in a lot of work to teach this AI by feeding it with large amounts of audio files and data.  Rhythmiq This is one of the more unique AI tools for the music industry. Rather than building its own beats from scratch, this tool depends on beats that you have already uploaded then creates its own variations.  Rhythmiq focuses on live performance. For instance, if you already have a playlist that you listen to while working, but you want to have a few variations to fill the silence and change your mood, then this may be the perfect tool for you.  Popgun This Australian AI startup has its basis on deep learning technology. Unlike other AI tools, this one can predict the music you will play, provide accompaniment as you play, and even create its own improvisations.  The main instrument for this AI tool is an electronic keyboard that the developers had trained using electronic samples. But the developers also plan to collaborate with other musicians for Popgun to learn to play other instruments too.  Conclusion While the mere idea of AI creating music might seem threatening for musicians and music producers, the truth is, AI can actually help them think more creatively. AI eliminates the more tedious aspects of music production so musicians can focus on more important things like unleashing their creativity. AI is one of the best solutions for amateurs, too, since it can help them create better music without spending too much time or effort. At the end of the day, creativity remains to be something only humans have… so we shouldn’t feel worried about AI taking over. Becky Holton is a journalist and a blogger. She is interested in education technologies and is always ready to support informative speaking. Follow her on Twitter. The blog was absolutely fantastic! Lot of great information which can be helpful in some or the other way. Keep updating the blog, looking forward for more contents…Great job, keep it up telldunkin survey Comments are closed. With the internet and digital technologies driving rapid change within the music industry, articles about new releases and who has been hired and fired are no longer enough. Our up to the minute industry news alongside insightful commentary helps our readers sift through the rumors and developments to find the information they need to keep their businesses moving forward.  Hypebot is read daily by more than 30,000 music industry professionals including executives and senior staff of music related tech firms, internet based music sites, every major label group and most indies as well as many managers, artists and members of the live music community: Contact us for the latesst stats, ad rates and sponosorship opportunites. We also offer combined rates with MusicThinkTank.  “I swear by Hypebot every morning over breakfast.” Derek Sivers Founder, CD Baby  “Hypebot is the most focused music business centric resource we have.” Celia Hirschman One Little Indian, Downtown Marketing & KCRW-FM  “Well done.” Seth Godin Marketing guru    var F14249_sb_requiredFields=new Array();var F14249_sb_validateFields=new Array();F14249_sb_requiredFields.push('F14249_sb_Category');F14249_sb_requiredFields.push('F14249_sb_email');F14249_sb_requiredFields.push('F14249_sb_feedid');F14249_sb_requiredFields.push('F14249_sb_publisherid');F14249_sb_requiredFields.push('F14249_sb_cids');var F14249_sb_fieldcol='#000000';var fbz_F14249_sb_logged=false;function F14249_sb_wait_fn(){try{if(!fbz_F14249_sb_logged){fbz_SmartForm('F14249_sb',feedblitz_full_form);try{s('F14249_sb');}catch(e){};fbz_FitForm('F14249_sb');var F14249_sb_wait_img=fbz_formMetrics(14249,1);fbz$('F14249_sb_wait_img').innerHTML=F14249_sb_wait_img;clearInterval(F14249_sb_wait);fbz_F14249_sb_logged=true;}}catch(e){}}
var F14249_sb_wait=setInterval(F14249_sb_wait_fn,100);  ",0.22921723085657517,0.5209820097934852
84,https://news.google.com/articles/CBMiUWh0dHBzOi8vd3d3LmJiYy5jb20vZnV0dXJlL2FydGljbGUvMjAxODEyMTctdGhlLW11c2ljYWwtZ2VuaXVzZXMtdGhhdC1jYW5ub3QtaGVhctIBAA?hl=en-US&gl=US&ceid=US%3Aen," Menu A Asked how The Beatles approached songwriting, John Lennon quipped “on the M1 (motorway) – turn right, past London.” His songwriting partner, Paul McCartney described the process as more of a long and winding road, in which the pair looked for chord shapes and then worked out a melody as if they were “doing a crossword puzzle”. Their collaborative approach to music-making produced hits that resonate decades after they were written. Their music has carved its influence into rock and pop, and influencing many of the bands that have followed them. You might also like: • The chef making 120 burgers every hour • The aircraft designer who’s never flown • Will this technology end traffic jams? But the next era-defining musical partnerships may look very different from the LSD-fuelled creativity of Lennon and McCartney. Songwriters are instead turning to machines to help them come up with chords and even pen lyrics for them. It is likely to change the way music is created forever. The first computer-generated score, called the Illiac Suite was developed in 1957 by two researchers and the Illiac I computer at the University of Illinois at Urbana-Champaign. The ‘electronic brain’ simply generated random integers representing musical elements such as pitch and rhythm, which formed four movements. The piece caused “confusion bordering on hysteria” amongst music aficionados, one of whom attending the first performance, likened it to the sounds of a barnyard. Today, however, a suite of AI software is capable of creating catchy new melodies in a matter of minutes, generating lyrics that tap into certain human emotions and even produce new sounds. Taking inspiration from The Beatles, Sony CSL Research Laboratory’s Flow Machines project produced the world’s “first structured AI pop song” called Daddy’s Car. To write the song, the AI software suggested chords and sounds based on the original music of the Fab Four, but a human composer was required to arrange and produce the final song. It was released by the team in 2016 under the name Skygge, which is Danish for shadow. Its Hello World album has notched up five million streams, half of which were for the first single, Hello Shadow, featuring Canadian singer, Kiesza. BBC Culture described it earlier this year as possibly the first “good” AI album to be made. Francois Pachet, of the Flow Machine project, and Benoit Carre, of the artistic group Skygge, collaborated on an album composed using AI (Credit: Getty Images) And AI music is also finding its way into the charts. Music producer Alex da Kid’s track Not Easy, featuring Ambassadors, Elle King and Wiz Khalifa was a Top 40 hit in the Billboard Chart in 2016. It used IBM Watson, a computer system capable of answering questions posed in natural language, to read blogs, news articles, and social media to gauge emotional sentiments around topical themes. It also analysed the lyrics of the top 100 songs for each week over the previous five years. With this data, Watson “arrived at an emotional fingerprint of culture,” according to IBM, which was used to help create the song’s simple lyrics. Alex Da Kid then used Beat – IBM’s AI music making software – to pick out musical elements that would be pleasing to the listener, meaning the AI partially wrote the hit song, or at least inspired parts of it. The power of AI algorithms to crunch large amounts of data, analyse and produce unusual arrangements is helping to give human artists new ways of making music. “The opportunities and ideas of how you could integrate this tech are endless,” says singer Taryn Southern, who posts her music on her YouTube channel. She recently created a single called Break Free with the help of AI tools including Amper, IBM Watson Beat and Google Magenta, which aims to use machine learning to create “compelling art and music”. “In terms of process, I start by making a series of decisions about what BPM, rhythm, key, mood, instrumentation I want and then essentially giving the AI feedback each time it generates a new possibility,” she explains. “This back and forth continues until I’m happy with the overall song. I then download, arrange and mix the stems into a structure.” For Southern, and others in the music industry, it is only a matter of time before there is a number one hit that has been written by a machine. Singer Taryn Southern believes using artificial intelligence to write new music can help push musicians to experiment (Credit: Getty Images) In many ways, the use of AI is merely an extension of what has been happening in the music industry for generations. Technology such as multitrack recording, for example, meant songs no longer needed to be recorded in a single take. David Bowie used bespoke software called The Verbasizer, which generated random sentences to help him create lyrics. It is now fairly normal for musicians to use Midi and virtual instruments on their tracks while audio processors are used to tinker with vocals. Some of the earliest uses of AI in music were to impersonate musical styles. Created in 1987 by David Cope, a former professor of music at the University of California, Santa Cruz, an intelligent machine called Experiments in Musical Intelligence analyses a database of pieces from a particular musical style, extracting rules which it then applies to create a unique composition that fits within the genre. The structured, ruled-based composition of many types of classical music meant it was perfect for a computer to replicate. Cope’s software has emulated more than 1,000 pieces of music in styles of 39 classical composers, some of which have been commercially recorded. Cope says the works have delighted, angered, provoked, and terrified those who have heard them, but overall, reactions have grown more positive over time. Since Cope’s early work there have been major advances in artificial intelligence research thanks largely to a field known as machine learning. By creating algorithms that, to some degree, replicate the behaviour of neurons in the brain, it has been possible to create AI networks that can analyse and learn from unstructured sets of data. This has allowed machines to unlock some of the secrets behind other complex forms of music, like folk music. A “folk machine” created by researchers at KTH Royal Institute of Technology, Stockholm and Kingston University, London, has churned out a staggering 100,000 new folk tunes in just 14 hours. It is an output that dwarfs even the most prolific of human composers at it takes about half a second to generate one tune. The researchers used an off-the-shelf AI method called a recurrent neural network (RNN), a form of machine learning that essentially predicts what comes next based on what it has previously seen, and fed it 25,000 traditional Celtic and English folk songs collected from a website to train their software. Bob Sturm and his colleagues taught an artificial neural network to compose its own folk songs, some of which have now been professionally performed (Credit: David Callahan, KTH) “The resulting computer models show some ability to repeat and vary patterns in ways that are characteristic of this kind of music,” says Bob Sturm, who led the project at Queen Mary University and is now an associate professor of computer science at KTH Royal Institute of Technology in Stockholm, Sweden. “It was not programmed to do this using rules – it learned to do so because these patterns exist in the data we fed it.” Surprisingly, despite the enormous output of folk music compositions from the folk machine, about one in five are “actually fairly good”, according to professional musicians who were asked to look at a sample of 3,000 of the tunes. Sturm says that the less rigid nature of folk music – where performers use compositions as a template to elaborate upon – may have been well suited to this sort of AI music generation. “The musicians found interesting features and some patterns that are unusual but work well within the style,” he says. Sturm and his colleagues have also challenged a group of traditional musicians to create an album of AI folk music in an attempt to test how plausible this approach to folk music composition can be. The result can be listened to by anyone online while the researchers have also placed a version of their algorithm online so anyone can create their own AI folk songs. Ultimately, however, Sturm hopes that an ensemble of robots will one day perform the computer-generated tunes all by themselves.   Performances by machines may indeed be closer than many people think. A beatboxer and experimental vocalist called Reeps One, whose real name is Harry Yeff, has been working with CJ Carr, a deep learning expert, to train a computer to perform verbal tricks by feeding it hours of himself beatboxing. The result is what he calls a “second self” that he can interact with and has used to create a ground-breaking composition where he has a beatboxing “conversation” with the machine. The AI, which he created in a collaboration with Nokia Bell Labs, has even produced new sounds that Yeff has then taught himself to replicate. “We’re able to create an echo chamber that leads to something new,” he says. Google’s DeepMind team is working to take this concept even further with a new project called WaveNet, which will create “a deep generative model of raw audio waveforms… able to generate speech which mimics any human voice”. Using a network of algorithms that is modelled on the human brain, it takes in audio and can then push it out in new forms. It raises the possibility of AI not only writing music and lyrics but also singing them. Already developments in a field known as natural language generation are producing machines capable of writing convincing looking lyrics. Researchers at the University of Antwerp and the Meertens Institute in Amsterdam have created a rap song generator they have called Deep Flow. They fed a language processing algorithm with a vast collection of rap and hip hop lyrics to teach it come up with its own. The results are foul-mouthed but realistic looking lyrics. So much so that the research team created an online game that challenges rap fans to distinguish between real hip hop lyrics and those spat out by their machine. Beatboxer Reeps One used machine learning to create a computerised version of himself that he can perform with (Credit: Nokia Bell Labs/Lonely Leap) Folgert Karsdorp, a researcher at the Meertens Institute who is involved in the project, says it is only currently possible to generate a few lines of convincing lyrics and that repetition is usually the giveaway. “As soon as you start generating longer text, like entire songs, the coherence gets lost. You could say that these models have the memory of a goldfish,” he says. In another dark corner of the musical spectrum, CJ Carr and Zack Zukowski, who together are the Dadabots, are using AI software to generate new black metal tracks. They train their machine on the raw acoustic waveforms of metal albums. As it listens, it tries to guess what will come in the next fraction of a millisecond, playing this ‘game’ millions of times to come up with a tune that the duo sometimes layer into atmospheric compositions. “Different influences from the music fuse together so you get a cluster of sounds blending to create a weird hybrid crossover,” says Zukowski. In the case of metal, this means screaming and guitars, which sound new and yet familiar at the same time. Although the technology is in its infancy, the pair believe the next-generation of music bots will be simpler to use and able to create music in real-time, mixing influences to produce something unique but on demand. Carr believes this could be the ultimate in machine-human music collaboration and blur the lines between composers and consumers.   It is a sign that something fundamental is about to change in the way we consume music. “We are entering an era of hyper-personalisation, in which consumers expect services tailored to their own tastes,” says Geoff Taylor, chief executive of the British Phonographic Industry (BPI). Apple Music, Deezer and Spotify all use AI to analyse users’ behaviour and suggest new tracks listeners might like. A new strain of emerging AI technology also makes use of contextual data, according to the BPI’s recent “Music’s Smart Future” report. For example, Google Play can use information like location, activity, and the weather to try to provide the right song at the right time. Combining this ability to learn about consumers with AI composition is also leading to some worrying trends for many musicians who are already suffering at the shift to music streaming services. Rather than playing tracks produced by musicians, streaming services could have their own AI music bots churn out music note by note to each customers’s taste. Amazon’s Echo smartspeakers already have a DeepMusic function that allows consumers to generate their own tunes and play them instantly in your home. The technology is arguably blurring the lines between who is an artist and a listener. Elsewhere computers are slowly taking over the composition of background music or muzak, defined by some as ‘functional’ music, again helping to circumvent difficulties that can lie in navigating rights and royalty payments. Jukedeck has taught its AI the elements of composition so its software can produce original music note by note. Many experts believe machines will not only be composing music but also performing it within the next decade or so (Credit: Alamy) “We train these neural networks with existing examples of music and they pick up the features of these examples and learn how to make them their own,” says Ed Newton-Rex, founder of the London-based company. Jukedeck doesn’t aim to emulate existing composers, but instead its software generates personalised music, allowing YouTube video makers to shape a melody to their video so they get a unique soundtrack. Similarly, another piece of software called Amper has been built as a collaborative tool for humans to put their own mark on computer-generated tracks for videos, but it also allows them to create music simply by choosing a mood, style and how long they would like the piece of music to last. “You can give it feedback – what you liked, didn’t like, what you’d like to change, and you can get a revision of that music that’s been created for you,” says Drew Silverstein, co-founder of Amper Music. For amateur film and music makers, this technology could help to open up the creative industries to them. The BPI’s report into the future of music found two of the biggest pain points in any creative process are time and cost, and AI can help to “significantly” reduce both of these. But all is not lost for those who still value the human touch on their music – AI is also speeding up the process of discovering new talent. A British company called Instrumental, has developed software that crunches data from Spotify, YouTube, Instagram and other platforms to identify the next generation of hit musicians who are uploading their own music. The company then signs them on development deals to help them grow. Music producer Alex da Kid is also using AI-powered searches to look through the emotional data for a huge number of artists on Spotify to identify singers he might be able to collaborate with. Mike Kestemont, an assistant professor at the University of Antwerp who was also involved in the Deep Flow rap machine, believes that humans will remain an essential part of the artistic process partly because of controversies about whether something produced by a machine can be considered art. “Many people say that isn’t possible because art is social and something that happens between people,” he says. “So if you have to recognise the authorship of a machine, you’d also have to recognise in part machines are a part of human society, which is one leap too far for many people.” Newton-Rex also doubts that AI will ever be able to emulate the creative genius of some human artists.   “I think there are elements of Bowie and Bach that may well be untouchable by a computer,” he says. Paul McCartney, for example, has also said in the past that the song-writing partnership he had with John Lennon is “impossible” to replicate because of the duo’s close relationship as teenagers. It is this humanity in the songs of The Beatles and other musicians that so many are sceptical machines will ever be able to achieve. Afterall, our species is thought to have been embracing music for some 50,000 years. Its power taps into something deep within our brains. Reassuringly, even if there is an AI equivalent of Mozart or McCartney out there, it is doubtful it would recognise its own genius, says Kestemont. That is for human listeners to do, and while we hold this power, our own creativity is sacred. But in the words of The Beatles, tomorrow never knows. Join 900,000+ Future fans by liking us on Facebook, or follow us on Twitter or Instagram.  If you liked this story, sign up for the weekly bbc.com features newsletter, called “If You Only Read 6 Things This Week”. A handpicked selection of stories from BBC Future, Culture, Capital, and Travel, delivered to your inbox every Friday.",0.11840869144296567,0.4870009560332143
85,https://news.google.com/articles/CAIiENOjci2ws7rSF3NvcRphY6kqGQgEKhAIACoHCAow4uzwCjCF3bsCMIrOrwM?hl=en-US&gl=US&ceid=US%3Aen,"To continue, please click the box below to let us know you're not a robot. Please make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy. For inquiries related to this message please contact our support team and provide the reference ID below.",0.3333333333333333,0.5962962962962962
86,https://news.google.com/articles/CBMiXWh0dHBzOi8vd3d3LnZlcml6b24uY29tL2Fib3V0L291ci1jb21wYW55L2ZvdXJ0aC1pbmR1c3RyaWFsLXJldm9sdXRpb24vYWktbmV4dC1yb2Nrc3Rhci1tdXNpY9IBAA?hl=en-US&gl=US&ceid=US%3Aen,"Access denied, in accordance with Verizon Information Security PolicyPlease contact us with the following Case ID 198885315553511452214312555045758650977 if there is a legitimate business need to access this content.",0.0,0.1
87,https://news.google.com/articles/CAIiECkXb53Wt6a_xHJ3VBmi7yIqFwgEKg8IACoHCAowjuuKAzCWrzww1oEY?hl=en-US&gl=US&ceid=US%3Aen,"Advertisement Supported by  By Alex Marshall LONDON — Patrick Stobbs recently sat in a conference room here playing songs from his smartphone, attempting to show how his start-up, Jukedeck, is at the cutting edge of music. The tune sounded like the soundtrack to a 1980s video game. “This is where we were two years ago,” he said, looking slightly embarrassed. “And this is where we are now,” he continued. He then played a gentle piano piece. Its melody was simple, and it was unsubtle in its melancholy, but there was no denying that it could work as background music for, say, a health insurance commercial. Mr. Stobbs didn’t write the music himself, nor did he commission it from a composer. Jukedeck is one of a growing number of companies using artificial intelligence to compose music. Their computers tap tools like artificial neural networks, modeled on the brain, that allow the machines to learn by doing, rather as a child does. So far, at least, these businesses do not seem to be causing much anxiety among musicians. “We see our system as still in its infancy; it’s only learnt a certain amount about music,” Mr. Stobbs said, although he quickly hinted how he hoped Jukedeck’s music could advance: “There’s no rule of physics that says computers can’t get as good as a human.” Having machines write music is not new. In the 1950s, the composer Lejaren Hiller used a computer to produce the “Illiac” Suite for string quartet, the first computer-generated score. Since then, countless researchers have pushed that work forward. But several start-ups are now trying to commercialize A.I. music for everything from jingles to potential pop hits. Jukedeck, for instance, is looking to sell tracks to anyone who needs background music for videos, games or commercials. The company charges large businesses just $21.99 to use a track, a fraction of what hiring a musician would cost. Mr. Stobbs wouldn’t reveal how many tracks it has sold, but said that the British division of Coca-Cola pays for a monthly subscription. Tech giants are also involved. In June, Google Brain announced Magenta, a project that aims to have computers produce “compelling and artistic” music, filled with surprises. Its efforts so far do not quite fit the bill. In September, DeepMind, the Google-owned British artificial intelligence company, also released results of an experiment it undertook for fun. DeepMind put samples of piano music into its WaveNet system, used to generate audio, such as speech. The system, which was not told anything about how music worked, used the initial audio to synthesize 10-second clips that sound like avant-garde jazz. IBM also has a research project called Watson Beat, which musicians will be able to use to transform their work’s style, making songs sound Middle Eastern, for example, or “spooky.” Jukedeck’s beginnings are somewhat surprising for a tech company. Mr. Stobbs and the composer Ed Newton-Rex, both 29, founded it in 2012. They had been choristers at King’s College School in Cambridge, England, and Mr. Newton-Rex went on to study music at the University of Cambridge, where he first learned that artificial intelligence could compose. After graduating from Cambridge, the pair set up a choral boy band (“a terrible idea,” Mr. Stobbs said), and had aspirations to run a record label. But in 2010, Mr. Newton-Rex attended a computer science lecture at Harvard, where his girlfriend was studying. The lecturer made coding sound relatively straightforward, and also made Mr. Newton-Rex recall his earlier studies in A.I. music. He decided to put the two together, and he set about building Jukedeck on the flight home. Jukedeck’s system involves feeding hundreds of scores into its artificial neural networks, which then analyze them so they can work out things like the probability of one musical note’s following another, or how chords progress. The networks can eventually produce compositions in a similar style, which are then turned into audio, using an automated production program. It has different networks for different styles, from folk to “corporate” — something that sounds like the glossy electronica typically played at business conferences. The company only recently started experimenting with the artificial neural networks for the audio output as well as the composition. This should make tracks sound more natural and varied — more human, in other words. Other companies working on A.I. music tend to involve musicians more directly in the process. The Sony Computer Science Laboratory in Paris, for example, considers musicians essential to its Flow Machines project, which has received funding from the European Research Council. The idea behind the project is to get computers to write pop hits, said François Pachet, the laboratory’s director. “Most people working on A.I. have focused on classical music, but I’ve always been convinced that composing a short, catchy melody is probably the most difficult task,” he said. He added: “A compelling song is actually a rare and fragile object. It can only work if all the dimensions are right: the melody, the harmony, the voice, the dress of the singer, the discourse around it — like, ‘Why did you write this song?’ No one is able to model all that right now, and I’m interested in that problem.” Flow Machines’ main system is a composing tool that works similarly to Jukedeck’s: by getting a computer to analyze scores — everything from Beatles’ songs to dance hits — so that it can learn from them and write its own. However, its output is then given to musicians, who are free to use it, change it or throw it away as they like, at no charge. (Negotiations are underway regarding contractual obligations if record labels release any of this music.) Musicians give “a sense of agency,” Mr. Pachet said. “The systems don’t know why they want to make music. They don’t have any goal, any desire.” Around 20 acts have already used the system, Mr. Pachet said, some performing the songs they wrote using it at a recent concert. He is in talks with some well-known groups, like the indie band Phoenix, to try it, he added, and several albums will be released this year. Musicians appear to enjoy it. “I could never have written a song like the one I did without it,” said Mathieu Peudupin, a French rock musician who goes by the name Lescop. “It drove me to a place I would never have gone myself.” He said it was like working with a bandmate, although he ignored most of its suggestions. “But what singer in the world listens to his bandmates?” he said, laughing. Mr. Pachet and Lescop both said they did not think listeners would ever entirely accept computer-generated music. “Music fans need to fall in love with musicians,” Lescop said. “You can’t fall in love with a computer.” But the founders of Jukedeck are less certain. Mr. Newton-Rex sees artificial intelligence changing the way we listen, especially if computers eventually “understand music enough to make it respond in real time to, let’s say, a game, or you going for a run,” he said. “Recorded music’s brilliant, but it’s static. If you’re playing a game, Hans Zimmer isn’t sitting with you composing. I think responsive systems like that will be a big part of the music of the future.” Advertisement",0.07901343205081522,0.4959147550269044
88,https://news.google.com/articles/CAIiEG_rBB3xG0faWk5tFhFTt0cqFggEKg4IACoGCAowl6p7MN-zCTDlkko?hl=en-US&gl=US&ceid=US%3Aen,"
Stuart Dredge 

Sun 6 Aug 2017 07.30 BST


Last modified on Wed 21 Mar 2018 23.50 GMT

 From Elgar to Adele, and the Beatles or Pink Floyd to Kanye West, London’s Abbey Road Studios has hosted a storied list of musical stars since opening in 1931. But the man playing a melody on the piano in the complex’s Gatehouse studio when the Observer visits isn’t one of them. The man sitting at the keyboard where John Lennon may have finessed A Day in the Life is Siavash Mahdavi, CEO of AI Music, a British tech startup exploring the intersection of artificial intelligence and music.  His company is one of two AI firms currently taking part in Abbey Road Red, a startup incubator run by the studios that aims to forge links between new tech companies and the music industry. It’s not alone: Los Angeles-based startup accelerator Techstars Music, part-funded by major labels Sony Music and Warner Music Group, included two AI startups in its programme earlier this year: Amper Music and Popgun. This is definitely a burgeoning sector. Other companies in the field include Jukedeck in London, Melodrive in Berlin, Humtap in San Francisco and Groov.AI in Google’s home town, Mountain View. Meanwhile, Google has its own AI music research project called Magenta, while Sony’s Computer Science Laboratories (CSL) in Paris has a similar project called Flow Machines. Whether businesses or researchers, these teams are trying to answer the same question: can machines create music, using AI technologies like neural networks to be trained up on a catalogue of human-made music before producing their own? But these companies’ work poses another question too: if machines can create music, what does that mean for professional human musicians? The aim is not, ‘Will this get better than X?’ It’s about whether this will be useful for people. Will it help them? “I’ve always been fascinated by the concept that we could automate, or intelligently do, what humans think is only theirs to do. We always look at creativity as the last bastion of humanity,” says Mahdavi. However, he quickly decided not to pursue his first idea: “Could you press a button and write a symphony?” Why not? “It’s very difficult to do, and I don’t know how useful it is. Musicians are queuing up to have their music listened to: to get signed and to get on stage. The last thing they need is for this button to exist,” he says. The button already exists, in fact. Visit Jukedeck’s website, and you can have a song created for you simply by telling it what genre, mood, tempo, instruments and track length you want. Amper Music offers a similar service. This isn’t about trying to make a chart hit, it’s about providing “production music” to be used as the soundtrack for anything from YouTube videos to games and corporate presentations. Once you’ve created your (for example) two-minute uplifting folk track using a ukulele at a tempo of 80 beats-per-minute, Jukedeck’s system gives it a name (“Furtive Road” in this case), then will sell you a royalty-free licence to use it for $0.99 if you’re an individual or small business, or $21.99 if you’re a larger company. You can buy the copyright to own the track outright for $199. “A couple of years ago, AI wasn’t at the stage where it could write a piece of music good enough for anyone. Now it’s good enough for some use cases,” says Ed Newton-Rex, Jukedeck’s CEO. “It doesn’t need to be better than Adele or Ed Sheeran. There’s no desire for that, and what would that even mean? Music is so subjective. It’s a bit of a false competition: there is no agreed-upon measure of how ‘good’ a piece of music is. The aim [for AI music] is not ‘will this get better than X?’ but ‘will it be useful for people?’. Will it help them?” The phrase “good enough” crops up regularly during interviews with people in this world: AI music doesn’t have to be better than the best tracks made by humans to suit a particular purpose, especially for people on a tight budget. “Christopher Nolan isn’t going to stop working with Hans Zimmer any time soon,” says Cliff Fluet, partner at London law firm Lewis Silkin, who works with several AI music startups. “But for people who are making short films or YouTubers who don’t want their video taken down for copyright reasons, you can see how a purely composed bit of AI music could be very useful.” Striking a more downbeat note, music industry consultant Mark Mulligan suggests that this strand of AI music is about “sonic quality” rather than music quality. “As long as the piece has got the right sort of balance of desired instrumentation, has enough pleasing chord progressions and has an appropriate quantity of builds and breaks then it is good enough,” he says. “AI music is nowhere near being good enough to be a ‘hit’, but that’s not the point. It is creating 21st-century muzak. In the same way that 95% of people will not complain about the quality of the music in a lift, so most people will find AI music perfectly palatable in the background of a video.” Not every AI-music startup is targeting production music. AI Music (the company) is working on a tool that will “shape-change” existing songs to match the context they are being listened to in. This can range from a subtle adjustment of its tempo to match someone’s walking pace through to what are essentially automated remixes created on the fly. “Maybe you listen to a song and in the morning it might be a little bit more of an acoustic version. Maybe that same song, when you play it as you’re about to go to the gym, it’s a deep house or drum’n’bass version. And in the evening it’s a bit more jazzy. The song can actually shift itself,” says Mahdavi. Australian startup Popgun has a different approach again. Its AI – called “Alice” – is learning to play the piano like a child would, by listening to thousands of songs and watching how more experienced pianists play them. In its current form, you play a few notes to Alice, and it will guess what might come next and play it, resulting in a back-and-forth human/AI duet. The next step will be to get her to accompany a human in real-time. “It’s a new, fun way to interact with music. My 10 year-old daughter is playing the piano, and it’s the bane of our existence to get her to practise! But with Alice she plays for hours: it’s a game, and you’re playing with somebody else,” says CEO Stephen Phillips. Vochlea, which is the other AI startup in the Abbey Road Red incubator, is in a similar space to Popgun. Beatbox into its VM Apollo microphone, and its software will turn your vocals into drum samples. Approximate the sound of a guitar or trumpet with your mouth, and it will whip up a riff or brass section using that melody. “It’s a little bit like speech recognition, but it’s non-verbal,” says CEO George Philip Wright. “I’m focusing on using machine-learning and AI to reward the creative input rather than taking away from it. It came from thinking, if you’ve got all these ideas for music in your head, what if you had a device to help you express and capture those ideas?” Many of the current debates about AI are framed around its threat to humans, from driverless trucks and taxis putting millions of people out of work, to Tesla boss Elon Musk warning that if not properly regulated, AI could be “a fundamental risk to the existence of civilisation”. AI music companies are keen to tell a more positive story. AI Music hopes its technology will help fans fall in love with songs because those songs adapt to their context, while Popgun and Vochlea think AI could become a creative foil for musicians. We will always value sitting with another person and making art. It’s part of what we are as humans Jon Eades, who runs the Abbey Road Red incubator, suggests that AI will be a double-edged sword, much like the last technology to shake up the music industry and its creative community. “I think there will be collateral damage, just like the internet. It created huge opportunity, and completely adjusted the landscape. But depending on where you sat in the pre-internet ecosystem, you either called it an opportunity or a threat,” he says. “It was the same change, but depending on how much you had to gain or lose, your commentary was different. I think the same thing is occurring here. AI is going to be as much of a fundamental factor in how the businesses around music are going to evolve as the internet was.” That may include the businesses having the biggest impact on how we listen to music, and how the industry and creators make money from it: streaming services. They already use one subset of AI – machine learning – to provide their music recommendations: for example in personalised playlists like Spotify’s Discover Weekly and Apple’s My New Music Mix. The songs on those playlists are made by humans, though. Could a Spotify find a use for AI-composed music? Recently, the company poached François Pachet from Sony CSL, where he’d been in charge of the Flow Machines project. It was under Pachet that in September 2016 Sony released two songs created by AI, although with lyrics and production polish from humans. Daddy’s Car was composed in the style of the Beatles, while The Ballad of Mr Shadow took its cues from American songwriters like Irving Berlin, Duke Ellington, George Gershwin and Cole Porter. You wouldn’t mistake either for their influences, but nor would you likely realise they weren’t 100% the work of humans. Now Pachet is working for Spotify, amid speculation within the industry that he could build a team there to continue his previous line of work. For example, exploring whether AI can create music for Spotify’s mood-based playlists for relaxing, focusing and falling asleep. For now, Spotify is declining to say what Pachet will be doing. “I have no idea,” admits Jukedeck’s Newton-Rex. “But to the question: ‘One day, will a piece of software that knows you be able to compose music that puts you to sleep?’ Absolutely. That’s exactly the kind of field in which AI can be useful.” What’s also unclear is the question of authorship. Can an AI legally be the creator of a track? Can it be sued for copyright infringement? Might artists one day have “intelligence rights” written into their contracts to prepare for a time when AIs can be trained on their songwriting and then let loose to compose original material? AI Music’s plans for automated, personalised remixes may bring their own complications. “If an app allows you to shape-change a song to the extent that you can’t even hear the original, does it break away and become its own instance?” says Mahdavi. “If you stretch something to a point where you can’t recognise it, does that become yours, because you’ve added enough original content to it? And how do you then measure the point at which it no longer belongs to the original?” The answers to these questions? Mahdavi pauses to choose his words carefully. “What we’re learning is that a lot of this is really quite grey.” It’s also really quite philosophical, with all these startups and research teams grappling with fundamental issues of creativity and humanity. “The most interesting thing about all this is that it might give us an insight into how the human composition process works. We don’t really know how composition works: it’s hard to define it,” says Newton-Rex. “But building these systems starts to ask questions about how [the same] system works in the human brain.” Will more of those human brains be in danger of being replaced by machines? Even as he boldly predicts that “at some point soon, AI Music will be indistinguishable from human-created music”, Amper Music’s CEO, Drew Silverstein, claims that it’s the process rather than the results that will favour the humans. “Even when the artistic output of AI and human-created music is indistinguishable, we as humans will always value sitting in a room with another person and making art. It’s part of what we are as humans. That will never go away,” he says. Mark Mulligan agrees. “AI may never be able to make music good enough to move us in the way human music does. Why not? Because making music that moves people – to jump up and dance, to cry, to smile – requires triggering emotions and it takes an understanding of emotions to trigger them,” he says. “If AI can learn to at least mimic human emotions then that final frontier may be breached. But that is a long, long way off.” These startups all hope AI music will inspire human musicians rather than threaten them. “Maybe this won’t make human music. Maybe it’ll make some music we’ve never heard before,” says Phillips. “That doesn’t threaten human music. If anything, it shows there’s new human music yet to be developed.” Cliff Fluet brings the topic back to the current home for two of these startups, Abbey Road, and the level of musician it has traditionally attracted. “Every artist I’ve told about this technology sees it as a whole new box of tricks to play with. Would a young Brian Wilson or Paul McCartney be using this technology? Absolutely,” he says. “I’ll say it now: Bowie would be working with an AI collaborator if he was still alive. I’m 100% sure of that. It’d sound better than Tin Machine, that’s for sure…” You can experiment with AI music and its close cousin generative music already. Here are some examples.  Jukedeck As mentioned in this feature, you can visit Jukedeck’s website and get its AI to create tracks based on your inputs. AI Duet Launched by Google this year, this gets you to play some piano notes, then the AI responds to you with its own melody. Scape Brian Eno was involved in this app, where you combine shapes to start music that then generates itself as your soundtrack. Humtap Music A little like Vochlea in this feature, Humtap’s AI analyses your vocals to create an instrumental to accompany you. Weav Run This is part running app and part music app, using “adaptive” technology to modify the tempo of the song to match your pace.  This article contains affiliate links, which means we may earn a small commission if a reader clicks through and makes a purchase. All our journalism is independent and is in no way influenced by any advertiser or commercial initiative. By clicking on an affiliate link, you accept that third-party cookies will be set. More information. ",0.15291731009347587,0.4255389544249648
89,https://news.google.com/articles/CBMiOWh0dHBzOi8vZnV0dXJpc20uY29tL2FpLWdlbmVyYXRlZC1tdXNpYy1sZWdhbC1jbHVzdGVyZnVja9IBPWh0dHBzOi8vZnV0dXJpc20uY29tL2FpLWdlbmVyYXRlZC1tdXNpYy1sZWdhbC1jbHVzdGVyZnVjay9hbXA?hl=en-US&gl=US&ceid=US%3Aen,"If you train a music-generating artificial intelligence exclusively on tracks by Beyoncé, do you owe the pop star a cut of any resulting songs’ profits? And is it even legal to use copyrighted songs to train an AI in the first place? Those are just a couple of the questions The Verge poses in a fascinating new story about AI-generated music published Wednesday. And while the publication consulted numerous experts from the music, tech, and legal industries for the story, the input of one person in particular — Jonathan Bailey, CTO of audio tech company iZotope — seemed to most concisely sum up the issue. “I won’t mince words,” he told The Verge. “This is a total legal clusterfuck.” Despite the U.S. Copyright Office bringing up the potential problems that could arise from computer songwriters way back in 1965, U.S. copyright law has yet to nail down exactly who owns what when a computer is involved in the creative process, according to The Verge. As it stands, the Beyoncé-trained AI could crank out an entire album of “Lemonade”-esque tracks, and as long as none of them sounded too much like any specific Beyoncé song, the AI-generated music wouldn’t be infringing on her copyrights — and the AI’s creator wouldn’t legally owe the artist a penny, lawyer Meredith Rose told The Verge. Less clear is the use of copyrighted songs to train an AI. Several of The Verge’s sources said there isn’t a straightforward answer as to whether buying a song grants a person the right to then use it to train a machine learning system. Of course, programmers have yet to come anywhere near creating AIs capable of autonomously churning out hit songs in the key of Bey — or anyone else for that matter — but that doesn’t mean they won’t be able to one day. “It’s like the future of self-driving cars,” media-focused venture capitalist Leonard Brody told Fortune in October. “Level 1 is an artist using a machine to assist them. Level 2 is where the music is crafted by a machine but performed by a human. Level 3 is where the whole thing is machines.” We’ve already seen several examples of those first two levels — tech-forward songstress Taryn Southern shared songwriting credits with AI on her “I AM AI” album, released in September, and that same month, Iranian composer Ash Koosha released an album on which he sang songs composed by AI-powered software. If Brody’s prediction is correct, the next step will be AIs creating music by themselves — and if we’re already in the midst of a “legal clusterfuck,” who knows what sort of legislative nightmare that will be? READ MORE: WE’VE BEEN WARNED ABOUT AI AND MUSIC FOR OVER 50 YEARS, BUT NO ONE’S PREPARED [The Verge] More on AI songwriters: This Musician Created an AI to Write Songs for Him, and They’re Pretty Strange",0.12402175247919932,0.42056699210954523
90,https://news.google.com/articles/CBMiYmh0dHBzOi8vd3d3LnNmY3Yub3JnL3ByZXZpZXcvb2RjLXRoZWF0ZXIvc3RyZXRjaGluZy10aGUtYm91bmRzLW9mLXJlYWxpdHktd2l0aC1haS1zZW5zb3JpdW0tYXQtb2Rj0gEA?hl=en-US&gl=US&ceid=US%3Aen,"This website is using a security service to protect itself from online attacks. 
Cloudflare Ray ID: 573fa5e5dd00a552
•
Your IP: 113.252.208.210
•
Performance & security by Cloudflare
",0.0,0.0
91,https://news.google.com/articles/CBMiYmh0dHBzOi8vd3d3LnNjaWVuY2VhbGVydC5jb20vbmV3LTI0LTctYWktZ2VuZXJhdGVkLWRlYXRoLW1ldGFsLXlvdXR1YmUtc3RyZWFtLWlzLWdpdmluZy11cy1hbnhpZXR50gFiaHR0cHM6Ly93d3cuc2NpZW5jZWFsZXJ0LmNvbS9uZXctMjQtNy1haS1nZW5lcmF0ZWQtZGVhdGgtbWV0YWwteW91dHViZS1zdHJlYW0taXMtZ2l2aW5nLXVzLWFueGlldHk?hl=en-US&gl=US&ceid=US%3Aen,"Even if death metal isn't a perfect fit for you as far as music genres go, you have to admire the AI smarts behind Relentless Doppelganger – a non-stop, 24/7 YouTube livestream churning out heavy death metal generated completely by algorithms. And this is by no means a one-off trick by Dadabots, the neural network band behind the channel: the project has produced 10 albums to date before this livestream even appeared. We have to admit the computer-generated sounds of the livestream, all mangled lyrics and frenetic drum beats, sounds unnerving to us. Your mileage and musical taste may vary, but there's no doubting the impressiveness of the science behind it. It's the work of music technologists CJ Carr and Zack Zukowski, who have been experimenting for years on how to get artificial intelligence to produce recognisable music in genres like metal and punk. ""This early example of neural synthesis is a proof-of concept for how machine learning can drive new types of music software,"" writes the pair in a 2018 paper. ""Creating music can be as simple as specifying a set of music influences on which a model trains.""  The deep learning behind the YouTube channel is trained on samples of a real death metal band called Archspire, hailing from Canada. These real audio snippets are fed through the SampleRNN neural network to try and create realistic imitations. Like other AI-powered imitation engines we've seen, SampleRNN is smart enough to know when it's produced an audio clip that's good enough to pass for the genuine article – and as a result it knows which part of its neural network to tweak and strengthen. The more data that SampleRNN can be trained on, the better it sounds... or to be more accurate, the more like its source material it sounds. ""Early in its training, the kinds of sounds it produces are very noisy and grotesque and textural,"" Carr told Jon Christian at the Outline back in 2017. ""As it improves its training, you start hearing elements of the original music it was trained on come through more and more."" SampleRNN was originally developed to act as a text-to-speech generator, but Carr and Zukowski have adapted it to work on music genres as well. It's effectively trying to predict what should happen next based on what it's just played – sometimes making tens of thousands of predictions a second. It can also go back to correct previous 'mistakes' – audio output that doesn't sound as it should do – but this only extends back a few hundred milliseconds. The result is the Relentless Doppelganger video. The team behind the livestream thinks the fast and aggressive play of Archspire particularly suits their approach – in other words, were it applied to a different band, it wouldn't be quite as realistic. ""Most nets we trained made shitty music,"" Carr told Rob Dozier at Motherboard. ""Music soup. The songs would destabilise and fall apart. This one was special though."" The project continues. If you like what you hear on the YouTube livestream, you can check out the neural network's other creations at the Dadabots site.",0.09582400358262427,0.4603616211374833
92,https://news.google.com/articles/CBMia2h0dHBzOi8vd3d3LnJhZGlvLmN6L2VuL3NlY3Rpb24vaW4tZm9jdXMvdW5maW5pc2hlZC1waWFuby1waWVjZS1ieS1hbnRvbmluLWR2b3Jhay1jb21wbGV0ZWQtYnktYWktcHJvZ3JhbW1l0gEA?hl=en-US&gl=US&ceid=US%3Aen,"
            Broadcast Archive  

                Broadcast in English 


Broadcast Archive                

 
      More than a hundred years after his death, fans of Antonín Dvořák have a
chance to hear a new piece by one of the greatest Czech composers. An
artificial intelligence programme called AIVA recently completed a fragment
of his piano composition in E-minor. It was recorded by the acclaimed Czech
pianist Ivo Kahánek.     Antonín Dvořák, photo: Ian Willoughby
After analysing all of Antonín Dvořák’s 115 opuses, AIVA, the
artificial intelligence programme, subsequently composed the remaining part
of his unfinished piece for piano. AIVA will now continue to compose the
second and third movements of the symphony, this time in an orchestral
version. The musical experiment was initiated by Richard Stiebitz and Filip Humpl
from the Wunderman agency, who say their goal was to examine the creative
possibilities of artificial intelligence. Filip Humpl outlines the initial
idea for the project, entitled AIVA/Dvořák: Music from the Future. “I have loved Dvořák’s music since a very early age. I knew he
was
very imaginative. He would walk through nature writing down musical motives
that sprung to mind. So I thought: what if there are some unfinished
fragments of his work out there? “At first we thought we would collect the fragments and use them to
create a new composition. As we searched for them, we came across a larger
score that already had several musical motives, so we decided that was the
piece we would like to complete using artificial intelligence.” The unfinished fragment of Dvořák’s piano composition in E-minor ended
up in the collections of the Czech Museum of Music in the early 1980s as
part of Dvořák’s heritage. The unnamed composition is in fact only a
very short fragment, consisting of two sheets of paper, explains one of the
museum’s experts, Veronika Vejvodová: “The composition is a sketch largely resembling the Mazurek earlier
composed by Dvořák, which might be the reason why he chose not to finish
it.
The composition itself is not that inventive so I would say Dvořák was
self-critical and decided not to complete it.” Upon making their discovery, Filip Humpl and his colleague approached the
Luxembourg based start-up company AIVA which uses artificial intelligence
to compose music. The company focuses mostly on classical, soundtrack music
used in films, video games, advertisement, and other types of visual media.
Pierre Barreau, the company’s co-founder and CEO explains why they
decided to accept the challenge: “First of all, I love Dvořák’s Ninth Symphony. It is not
particularly
easy to AI-compose in that style. There are some more obvious styles for AI
to compose in. For example Bach is usually a composer that AIs can emulate
quite well. But composers like Dvořák are more difficult. 
“So it was interesting both from a technical standpoint and purely
from
musical standpoint as well. Because Dvořák’s music inspired film score
composers and we love film scores in AIVA.”
“That was also the reason we decided to take part in the project. And I
felt that finishing an unfinished symphony is a very powerful story to tell
the people.”  You said Dvořák’s music was more difficult to recreate than for example
that of Bach. Why is that? “Baroque music like Bach is very mathematical. There are a lot of
patterns in his music that algorithms love when they do the learning
process, because it is very easy to understand what is going on there.
That’s why Bach’s music is very easy to work with and Bach himself
actually defined his music as being mathematical. “Dvořák’s music is a little more unpredictable. The complexity
of the
Ninth Symphony is also in the fact that there are lots of leitmotivs which
appear in rather unexpected places and there are variations of those
leitmotivs. “So there are a lot of almost random elements, at least from the
algorithm’s perspective, that you don’t necessarily have in Baroque or
Bach’s music. That’s why it is a bit more complicated to understand for
an AI system.” The first step in completing the unfinished fragment of Antonín
Dvořák’s symphony was teaching AIVA about the music and the particular
style she was supposed to compose in, explains Mr Barreau. In this case,
the team had AIVA read 30,000 scores of history’s greatest composers,
including Dvořák, but also Bach, Beethoven, Mozart: “The second part of the training focused exclusively on Dvořák, so
that
AIVA could learn what makes him different from other composers. And then
there was the actual composition process. Pierre Barreau, photo: YouTube
“During this process AIVA composes thousands and thousands of
examples
that are arranged by quality and then the humans in our team go through
them, pick the one they think are the most relevant and we have a musician,
in this case a pianist, play the music.” Despite the abilities of the Artificial Intelligence programme, Philip
Barreau stresses the importance of the human element in the whole process.
The project AIVA/ Dvořák: Music from the Future involved both amateurs
and professional musicians, but all of them had to have at least a
fundamental understanding of how music works to be able to evaluate the
results. “I think that the human element is indispensable for multiple
reasons.
The first one is very technical and practical. The people who programme can
improve the AI only if they understand what it composes. So there is a lot
of work there that’s done by humans and a lot of that work is supervised
by humans. “The second reason is maybe less practical. I think that whenever
you
create a piece of music as a composer you want to give that piece of music
some meaning and meaning comes with a vision. Artificial Intelligence at
this stage doesn’t have any vision of how to create a piece of music that
makes sense musically and can also convey a story. “Even if artificial intelligence composes everything, every single
note,
there still needs to be a human at end of the process to actually select
the score and give meaning to that score. Just like we gave meaning to the
score that AIVA composed for this project.  “We told the story of continuing the unfinished symphony of
Dvořák. And
that itself is a story crafted by humans, independently from the artificial
intelligence itself. So I think humans will also be relevant.” AIVA is currently busy completing the second and third movements of
Dvořák’s unfinished composition in E-minor, this time an orchestral
version to be recorded by Prague Philharmonia. For their part, the project’s initiators Filip Huml and Richard Stiebitz
say they have succeeded in stirring up a public debate about the
possibilities of employing artificial intelligence in music composing: “We are not sure if we want to continue with this project in the
future.
We want to see a live performance of the whole composition by the
orchestra. We want to see the reaction of the audiences and to see if the
end result really carries Antonín Dvořák’s signature and whether it
leads the listeners to a catharsis.” The second movement of Antonín Dvořák’s unfinished composition will be
presented at the Rock for People festival in Hradec Králové in June. Its
complete orchestral version, performed by Prague Philharmonia, is due to
premiere at Prague’s Rudolfinum concert hall in November. Czech classical music is not only a part of the national culture and
history, but also of its very soul. This year, we have prepared… Czech classical music is not only a part of national culture and history,
but also of the nation’s soul. This year we have prepared… Czech classical music is not only a part of national culture and history,
but also of the nation’s soul. This year we have prepared…   facebook  twitter  newsletter   youtube   instagram   rss Art education in Czechia: envy of the world Karel Čapek: Novelist, playwright – and travel writer The Defectors: Czech-American essayist René Georg Vasicek’s debut novel an exploration of identities, truths and “quantum beings” 
                More cases of coronavirus in Czech Republic found on Wednesday               
                Archaeologists unearth massive Moravian castle walls under historic Přerov square               
                New coronavirus measures now in place in Czech Republic               
                First three cases of coronavirus infection reported in Czech Republic               
                Thirty-one cases of coronavirus now confirmed in Czech Republic               




",0.05998339984668112,0.4638354527417029
93,https://news.google.com/articles/CAIiEJXGPlo0x0qp4WPap19iaFIqFggEKg4IACoGCAow3O8nMMqOBjD38Ak?hl=en-US&gl=US&ceid=US%3Aen,"Filed under: Breathing new life into a SNES classic If you go to YouTube and search “Mario Paint Composer,” you’ll be treated to a variety of quirky remixed music. Some of the best tracks sound as if the original song was put through a chiptune music filter and uploaded, staying incredibly faithful to their source material. Mario Paint remixes, however, are not simply chiptune remixes. Composers have to deal with an unusual set of tools, including weird limits on notes and tempos, barking 16-bit dogs, and more in order to create their music. It’s a challenge to create music with these strange rules in place, but the challenge is what brings these composers to the medium in the first place. “It started out way back in 2013,” CyanSMP64 tells The Verge. Cyan is one Mario Paint composer behind tracks like the Pokemon Sword and Shield Battle Tower Remix and the Undertale Megalovania Remix. He started when he was only 12. “I was fond of Black MIDI videos. Eventually, I stumbled across a cover done in Mario Paint Composer.” Most people who make music with Mario Paint’s limitations don’t actually use the SNES title’s original music maker. (As it turns out, trying to make any sort of complex music is difficult on the 16-bit console.) Instead, musicians use computer programs that mimic Mario Paint’s sparse sets of sounds / notes and other limitations but allow them to actually make longer tracks and save them. Back in 2013, the application to do that was Mario Paint Composer, which Cyan originally had trouble with. “I was quite disappointed as I realized the limitations of the program. Five notes only in a single column, for example.” Only five notes in a column meant that the number of notes and instruments that could be replicated was very low.  Cyan eventually figured out a way around the limitation. “Using a freakishly high tempo in Mario Paint allows for bypassing the five-note restriction as well as having multiple notes play simultaneously at different volumes.” This technique even has a special name. “This is often called ‘dickspeed’ by the Mario Paint community.” Despite eventually figuring everything in Mario Paint Composer out, CyanSMP64 didn’t upload most of his early music to YouTube due to issues recording audio and video on a Mac. But the passion was clearly there. “I would constantly watch and listen to other Mario Paints on YouTube, and even download them on my iPod so I could listen to them at any time, anywhere.” Mario Paint music hit its height of popularity in the early 2010s, but a general interest in the unusual method of remixing waned over time. Places like The Mario Paint Hangout, a forum for Mario Paint composers to gather and share their work, died out. The Mario Paint subreddit, which is for both musical and visual creations, only gets a new post every couple of months. If you search for Mario Paint Composer videos on YouTube, you’ll get things that are almost a decade old. You have to dig deep to find something new. Resources are also difficult to find. Super Mario Paint is the most up-to-date composer that most creators use, but the application is limited to GitHub, without much in the way of explanations or tutorials. Poking around can land you on the website of a popular Mario Paint remixer full of various downloads, but the site hasn’t been updated in years. From the outside looking in, the community seems dead, moving on from the strange little music maker to other forms of composing. But there’s still somewhere where the community stands strong: Discord. Dig into the Mario Paint Hangout forums, and you can find a (surprisingly still active) link to Super Mario Paint Chat where most of the Mario Paint composers gather to talk about and share their unique craft. Xanderoni is a moderator in Super Mario Paint Chat and does a fair amount of work in the community, including organizing collaboration projects with other musicians. Xanderoni also created the Mario Paint Collection, a compilation of songs created with Mario Paint Composer’s limitations in mind. “It first started as me wanting to listen to Mario Paints in the car for work,” Xanderoni explains. “But [VolcanBrimstone] was making a playlist of Original Compositions done in Mario Paint. So I decided to turn my little playlist into a preservation project to keep track of as many Mario Paint [songs] as possible.” Xanderoni has preserved 2,000 songs with the Collection over the last year with the help of the Mario Paint community. Despite Mario Paint’s oddities, Xanderoni continues to use the application to make music. “For me, it’s easy to understand the nature of the program. It’s rebuilt from the ground up based off of the SNES version of Mario Paint, which was made so kids could easily make music.” Super Mario Paint is therefore easy to learn, but it becomes hard to master when dealing with note limits. “My dad asks me often why I don’t get higher quality music software,” Xanderoni explains. “It really does just come down to accessibility.” This mix of accessibility and constraints is why many return to Mario Paint. Oh_spap_its_me is not a Mario Paint music composer but creates art in the SNES game and posts it on Instagram as The Mario Painter. Despite the different creative mediums, the two groups use Mario Paint for the same reasons. “I personally love using Mario Paint because of the novelty of it all. The concept of using such obsolete technology to create artwork and music that still look and sound good in their own charming way is really fascinating to me.” Creating music and art with tough limitations definitely makes for unique results, and it isn’t a concept limited to Mario Paint users. Black MIDI, which CyanSMP64 mentioned earlier, is a genre of music where MIDI sound files are overlapped one another. It is named after the layers of black from the tablature. The results are often eye-catching affairs that are impossible to replicate with the live playing of an instrument. MIDI files are low-quality recordings, but the artists overlap the recording to alleviate the resulting remix. But that is what makes these programs and genres fun: the ability to create something so unique that it can’t be done anywhere else. Creative minds love to be challenged, and there’s little more challenging than plopping someone down with a basic, limited application and have them go at it. The unusual remixes can attract a lot of attention, too. “Out of the blue, on December 18th, I noticed that I was receiving a large number of comments on my cover of The World Revolving. It was blowing up and appearing in peoples’ [YouTube] recommended sections,” said CyanSMP64. They gained a ton of views and follows from the Deltarune remix. “Seeing a video blow up on my channel is like the best feeling ever.” CyanSMP64 finds making music in Mario Paint to be a rewarding hobby, but he wants to make music as his future career. “My future goal is to compose music for video games ... and FL Studio is my typical go-to for music production.” Cyan doesn’t have a public portfolio for his non-Mario Paint work yet, but he has no problem with his public output being only from Mario Paint remixes for now. Even now, the community continues to work in the Discord. Composers post updates almost daily, and collaboration projects are met with excitement from the community. Whether the broader gaming or music communities notice them doesn’t really matter. They’re making these songs for themselves — and for the challenge.   A newsletter about computers",0.14002717652717653,0.5123808454919565
94,https://news.google.com/articles/CBMiV2h0dHBzOi8vd3d3LnNjaWVuY2VtYWcub3JnL25ld3MvMjAxNy8wOC9ob3ctZ29vZ2xlLW1ha2luZy1tdXNpYy1hcnRpZmljaWFsLWludGVsbGlnZW5jZdIBAA?hl=en-US&gl=US&ceid=US%3Aen,"A musician improvises alongside A.I. Duet, software developed in part by Google’s Magenta  
  By Matthew HutsonAug. 8, 2017 , 3:40 PM Can computers be creative? That’s a question bordering on the philosophical, but artificial intelligence (AI) can certainly make music and artwork that people find pleasing. Last year, Google launched Magenta, a research project aimed at pushing the limits of what AI can do in the arts. Science spoke with Douglas Eck, the team’s lead in San Francisco, California, about the past, present, and future of creative AI. This interview has been edited for brevity and clarity. Q: How does Magenta compose music? A: Learning is the key. We’re not spending any effort on classical AI approaches, which build intelligence using rules. We’ve tried lots of different machine-learning techniques, including recurrent neural networks, convolutional neural networks, variational methods, adversarial training methods, and reinforcement learning. Explaining all of those buzzwords is too much for a short answer. What I can say is that they’re all different techniques for learning by example to generate something new.  Q: What examples does Magenta learn from? A: We trained the NSynth algorithm, which uses neural networks to synthesize new sounds, on notes generated by different instruments. The SketchRNN algorithm was trained on millions of drawings from our Quick, Draw! game. Our most recent music algorithm, Performance RNN was trained on classical piano performances captured on a modern player piano [listen below]. I'd like musicians to be able to easily train models on their own musical creations, then have fun with the resulting music, further improving it. Q: How has computer composition changed over the years? A: Currently the focus is on algorithms which learn by example, i.e., machine learning, instead of using hard-coded rules. I also think there’s been increased focus on using computers as assistants for human creativity rather than as a replacement technology, such as our work and Sony’s “Daddy’s Car” [a computer-composed song inspired by The Beatles and fleshed out by a human producer]. Q: Do the results of computer-generated music ever surprise you? A: Yeah. All the time. I was really surprised at how expressive the short compositions were from Ian Simon and Sageev Oore’s recent Performance RNN algorithm. Because they trained on real performances captured in MIDI on Disklavier pianos, their model was able to generate sequences with realistic timing and dynamics. Q: What else is Magenta doing? A: We did a summer internship around joke telling, but we didn’t generate any funny jokes. We’re also working on image generation and drawing generation [see example below]. In the future, I’d like to look more at areas related to design. Can we provide tools for architects or web page creators?  Magenta software can learn artistic styles from human paintings and apply them to new images. Q: How do you respond to art that you know comes from a computer? A: When I was on the computer science faculty at University of Montreal [in Canada], I heard some computer music by a music faculty member, Jean Piché. He’d written a program that could generate music somewhat like that of the jazz pianist Keith Jarrett. It wasn’t nearly as engaging as the real Keith Jarrett! But I still really enjoyed it, because programming the algorithm is itself a creative act. I think knowing Jean and attributing this cool program to him made me much more responsive than I would have been otherwise.  Q: If abilities once thought to be uniquely human can be aped by an algorithm, should we think differently about them? A: I think differently about chess now that machines can play it well. But I don’t see that chess-playing computers have devalued the game. People still love to play! And computers have become great tools for learning chess. Furthermore, I think it’s interesting to compare and contrast how chess masters approach the game versus how computers solve the problem—visualization and experience versus brute-force search, for example. Q: How might people and machines collaborate to be more creative? A: I think it’s an iterative process. Every new technology that made a difference in art took some time to figure out. I love to think of Magenta like an electric guitar. Rickenbacker and Gibson electrified guitars with the purpose of being loud enough to compete with other instruments onstage. Jimi Hendrix and Joni Mitchell and Marc Ribot and St. Vincent and a thousand other guitarists who pushed the envelope on how this instrument can be played were all using the instrument the wrong way, some said—retuning, distorting, bending strings, playing upside-down, using effects pedals, etc. No matter how fast machine learning advances in terms of generative models, artists will work faster to push the boundaries of what’s possible there, too.  doi:10.1126/science.aan7216 Matthew Hutson is a freelance science journalist in New York City. 
  By John TravisMar. 14, 2020 
  By Kelly Servick, Adrian Cho, et al.Mar. 13, 2020 
  By Jon CohenMar. 13, 2020 
  By Elizabeth PennisiMar. 12, 2020 
  By Robert F. ServiceMar. 12, 2020 More ScienceInsider 
  By Jon CohenMar. 9, 2020 
  By Rodrigo Pérez Ortega Feb. 27, 2020 
  By Rodrigo Pérez Ortega Feb. 26, 2020 
  By Rodrigo Pérez Ortega Feb. 21, 2020 
  By Rodrigo Pérez Ortega Jan. 21, 2020 More Sifter 
  Vol 367,

      Issue 6483   © 2020 American Association for the Advancement of Science. All rights Reserved. AAAS is a partner of HINARI, AGORA, OARE, CHORUS, CLOCKSS, CrossRef and COUNTER.",0.15924492174492175,0.492019092019092
95,https://news.google.com/articles/CBMiXmh0dHBzOi8vd3d3LnRoZW5hdGlvbmFsLnNjb3QvbmV3cy8xODMwNTI2OC5wYXQta2FuZS1wZXJmZWN0LWJvb2stcmVhZC1zY290bGFuZC1zZWxmLWlzb2xhdGlvbi_SAQA?hl=en-US&gl=US&ceid=US%3Aen,"Edinburgh 9°c The new book, edited by Val McDermid and Jo Sharp, has contributions from the likes of Lesley Riddoch, Janey Godley, Christopher Brookmyre and Stuart Cosgrove 
  ‘WE need new stories”. Such a theme – birthed at last year’s Edinburgh International Book festival – drives this fascinating and rich collection.
 
  Entitled Imagine A Country: Ideas For a Better Future, it’s edited by Val McDermid and Jo Sharp, and comprises of 90-odd alphabetically ordered pieces, each 500-800 words long (I have one of them, under “K”).
 
  Like you, I’m currently sitting indoors at my own hearth (actual and digital), feeling odd and mildly stunned at the corona-quiet outside. But I suggest, in the downtime, you could much worse than curl up with this fireball of optimism and idealism.
 
  All things will pass. When they do, and we then grab our nation and its systems and point them in an entirely different direction, here you’ll find some intriguing pathways ahead.
 
  I did my own rough categorisation of all these imaginings of Scotland. And the top three categories (with of course many arguable overlaps, argument being the point) are a fascinating snapshot of where the Scottish intelligentsia, such as it is, currently lies.
 
  (This is minus, we must note, working politicians. “They already have plenty of opportunities to tell us what they think,” quip the editors.)
 
  The second-most-numerous category concerns what I’d call “land, homes and place”. The poet John Burnside, with his usual dark thunder, calls for a “land ethic” in Scotland. “A country where a teenage boy with social anxiety problems is not condemned to sit on a cold street, begging”, writes Burnside, “while a man born into millions slithers by in his Bentley on the way to a banquet” (a position roughly echoed by the nature advocate Cameron McNeish and Shetland’s Malachy Tallack).
 
  There is a moving set of voices arguing for the minimal securing of the right to a decent, comfortable home (or “howff”, to quote architect Malcolm Fraser’s Scots usage), with the principle of ownership decidedly communal (whether on Eigg or in the central belt).
 
  Lesley Riddoch and I propose new kinds of built space – Lesley with her rural “huts”, me with my suburban “constitutes” or “makar houses”. We are both hoping they encourage social making and soul saving, simultaneously.
 
  And there are some intense pitches for locality, and the people we immediately live with, as the basis for civility and progress. This at a time, the authors suggest, when all other ideologies have run out of steam.
 
  Writer Alan Bissett says: “We need to think of ourselves as occupying a country of thousands of small locations, full of people who contribute something to that place”.
 
  The literary critic Stuart Kelly hymns his recent embedding in village life – while the journalist Peter Ross thinks his profession ignores a “vital task in simply recording life as it is lived”. “Let us not forget to beat the drum for the humdrum”. We understand news is about “speed, aggression, outrage” – but could it also be, asks Ross (with some courage), about “beauty, compassion and love”?
 
  The third-largest category I marked down as “history, identity and tradition”. From Jackie Kay’s sweet and inclusive patriotic verse, to Alasdair Gray’s “kinds of folk” taken from his Hillhead Underground mural.
 
  From Jamaican historian Geoff Palmer’s plea for chattel slavery to be put at the heart of Scottish history, to Damian Barr’s demand that every aspect of an under-recorded Scottish history be given its due.
 
  There are also two proposals to change the country’s name – Mark Cousins suggests Land of the Trees (by 2040), Greg Hemphill “Banterland”, “Great Patter” or “Biiiiig Whisky” – and a call from Elaine C Smith for an official national winter festival.
 
  Yet my most populated category – again, you can fight me about it – were pieces interested in “behaviour, ethos and practice”. The late Stephen Maxwell, in his Arguing for Independence, wrote on the motivating power of Scotland as an “autonomous moral community” (from the Church of Scotland to anti-Trident).
 
  Well, the AMC is out in force here, pushing through any carapace of posturing or coolness. People such as Horse McDonald, Ricky Ross and Trishna Singh just urge acts of daily kindness, or forgiveness, or openness to the stories of others, or a saving humour – or Janey Godley’s concrete suggestion of “free soup every Friday made by a granny”.
 
  Inspired by a spontaneous shrine in his local Alexandra Park, Stuart Cosgrove proposes new public rituals that allow mothers to mourn their lost children.
 
  Scaling the metaphysical heights, Andrew O’Hagan imagines a “non-narcissistic Scotland … a country where we are ambitious for the world because we do not see it for long, and the world is our legacy, which gives us the freedom to plant wonder in place of our ego”.
 
  Richard Holloway, deeper still, asks us imagine running our country behind John Rawls’s “veil of ignorance”. If we simply assumed we had to deal with full human potentiality, what kind of country would we run? Almost worth the price of purchase alone are two fantasy-epiphanies by AL Kennedy and Ali Smith, which put sensuous flesh on Stephen Maxwell’s moral bones. There are some unanticipated suggestions. Both the director of the Edinburgh International Book Festival, and the director of that city’s Lyceum Theatre, seem to require of us some form of national service.
 
  The former imagines his grumpy granny dealing with enthusiastic young cadets of the future, doing their year of “Social Service”.
 
  Half-seriously, the latter wants every 18-year-old drafted into creating community musicals, as a form of social character-building (“it’s not easy to be a fascist whilst performing a kick-line in Oklahoma!”).
 
  Indeed, there are some extremely strong arguments here for learning and practising drama and narrative as a key skill for the future. “Empathy is a muscle, theatre is the gym,” as dramatist Jo Clifford puts it. Fellow practitioner Philip Howard sketches a future where a Scotland wide open to the refugees of the world uses drama-education to increase understanding.
 
  Cartoonist Frank Quitely wants the study of “Story” to be a master discipline in schools: kids should know how “stories shaped our evolution from prehistory to the present day … paying particular attention to Story in religion, politics, the media and social media”.
 
  As you might predict from this book-forged nation, a transformed Scottish education threads through everyone’s future, in some way.
 
  Novelist Christopher Brookmyre makes a hard-nosed case for philosophy education in schools, as an antidote to polarisation (and a straightforward raiser of outcomes).
 
  Composer Bill Sweeney does a funny and ideas-packed “heidie’s address” to a school in 2035, where “we now rely on AI bots for our day-to-day repetitive and non-creative tasks”. There are arts trips, outdoor learning, ambient opportunities for music-making – and a thrumming insistence that creativity is at the centre of any pedagogy (Seona Reid, Ruth Wishart and Roddy Woomble are particularly eloquent here).
 
  It’s hard to exhaust this book – which also has striking pieces on social justice, food and environment, and children’s rights.
 
  But I must mention the short number of what you could call “macro-policy” pieces. Poet Don Paterson supports universal basic income, while novelist Leila Aboulela promotes the shorter working week. Major scientist Anne Glover advocates a post-consumerism of smart materials and 3D printing; academic Gerry Hassan urges that we develop our literacy in futures thinking.
 
  The point of these policies is that they are all designed to support the preceding, teeming multitude of human purposes and actions. Their evaluations of the human condition are much more diverse than, say GDP or the labour market. If you seek any grim realism about the severities of competitive advantage for an independent Scotland, this isn’t your rodeo.
 
  But if you’re trying to imagine what you might do with yourself, when “business as usual” collapses around your ears, and a disordered climate turns life upside down … Well, this volume could be a very good place to pick up some clues. About the practicalities of living with different, more expanded, and subtler priorities.
 
  The right book, for a weird time.
 
Imagine a Country: Ideas for a Better Future, edited by Val McDermid and Jo Sharp, is published by Canongate Books. Out on Thursday, £12
 

 Get involved with the news in your community         This website and associated newspapers adhere to the Independent Press Standards Organisation's Editors' Code of Practice. If you have a complaint about the editorial content which relates to inaccuracy or intrusion, then please contact the editor here. If you are dissatisfied with the response provided you can contact IPSO here ©Copyright 2001-2020. This site is part of Newsquest's audited local newspaper network. A Gannett Company. Published  from its offices at 200 Renfield Street Glasgow and printed  in Scotland  by Newsquest (Herald & Times) a division of Newsquest Media Group Ltd, registered in England & Wales with number 01676637 at Loudwater Mill, Station Road, High Wycombe HP10 9TY – a Gannett company.",0.0900839805619217,0.40786419870978713
96,https://news.google.com/articles/CBMibmh0dHBzOi8vYXJzdGVjaG5pY2EuY29tL2dhbWluZy8yMDE4LzAzL3JvYm90cy1jb21wb3NlZC10aGUtc291bmR0cmFjay1mb3ItbXktaG9tZS1tb3ZpZS10aGV5LWNhbi1oZWxwLXlvdS10b28v0gF0aHR0cHM6Ly9hcnN0ZWNobmljYS5jb20vZ2FtaW5nLzIwMTgvMDMvcm9ib3RzLWNvbXBvc2VkLXRoZS1zb3VuZHRyYWNrLWZvci1teS1ob21lLW1vdmllLXRoZXktY2FuLWhlbHAteW91LXRvby8_YW1wPTE?hl=en-US&gl=US&ceid=US%3Aen,"Front page layout Site theme Sign up or login to join the discussions! 
Nathan Mattise
    -  Mar 10, 2018 12:14 pm UTC
 It used to be you could spend an afternoon drumming up a home movie with your little sister, soundtrack it with your favorite mixtape cuts, and upload it to the Internet for sharing without a care. But as bot-driven copyright tools started scanning new uploads and society generally became more concerned with proper song licensing, using whatever track you desired for some publicly available Web video became less of an option. What's an amateur-at-best musician to do? I may have marched on a collegiate snare line (and therefore understand rhythm, phrasings, and tempo), but my ability to create melody probably stopped with middle school recorder lessons. Luckily, we musically challenged filmmakers and podcasters now have robots. A number of high-profile AI composition initiatives have surfaced in recent years—perhaps most notably, Sony's Flow Machine released its debut album in January—and slowly but surely these tools are moving from the research labs and professional production studios into publicly available spaces. So when I was recently left by myself for a weekend with just my dog for company and a desire for late night tacos, I decided the time had come to reenter the short film arena. The above documentary on Ernie the Shih Tzu is scored entirely by smart composition tools you can access right now. Would-be children's publishers: feel free to email me directly about his expanded universe. For everyone else, here's the lowdown on four song-generating programs you can use to help people get down. Released on March 1, Chrome's Song Maker represents the newest of these tech-y composition tools. Technically, this isn't an artificially intelligent composer. Instead, it's a tool that simplifies composition to the point where anyone can do it regardless of your familiarity with pitch or rhythm. It greets users with a grid representing beats on the X axis and tones on the Y, and filling in any individual square will generate a sound. You have five options for tonal instruments (piano, synth, strings, marimba, woodwind) and four for percussion (electronic, blocks, kit, conga). You can manually point and click; you can add through a MIDI keyboard or by singing into a mic; you can even let your keyboard keys do the songwriting if desired. Song Maker comes from Chrome Music Lab, a Google initiative to build browser-based tools for learning basic music principles. The Music Lab has a dozen or so tools already focusing on everything from oscillators to rhythm to arpeggios. So while this may be aimed literally at the elementary school demographic most of the time, Song Maker has enough customization options for any novice composer to be effective. You can make your loop up to 16 bars, change into funky time signatures like 5/4 or 12/8, break the note subdivisions into triplets or sixteenth notes, choose any key you'd want by alternating scale and starting note, and extend your range up to three octaves. The lone issue for my purposes? Song Maker only allows you to choose a single percussion and tonal instrument pairing for your composition. I wanted a meatier, spy thriller-inspired ditty for the moment my dog reveals his day job (roughly the 1:21 mark of the video). Ideally, I could combine the basic beat I crafted with a higher-octave string line. But even if you're cautious enough to choose the same amount of bars and the same tempo, there doesn't appear to be a way to layer multiple tracks on top of one another for fuller songs within Song Maker. That requires a basic sound-editing program and the ability to capture sound from a browser in this instance. The limitations of Song Maker therefore prove to be both positive and negative—things are simplified enough that it would be hard to create something truly unlistenable, but compositions have a complexity ceiling, too. This being Google, of course there are multiple initiatives in progress to upgrade the music-making process. Compared to Chrome Music Labs, the Magenta initiative represents the headier approach.Further ReadingGoogle’s product strategy: Make two of everything Introduced back at the 2016 Moogfest (increasingly a bucket-list event for music tech fans), Magenta wants to leverage artificial intelligence and machine learning to empower anyone to be musically creative. “The goal of Magenta is not just to develop new generative algorithms, but to 'close the creative loop,'"" the team wrote when introducing N Synth last year. ""We want to empower creators with tools built with machine learning that also inspire future research directions. Instead of using AI in the place of human creativity, we strive to infuse our tools with deeper understanding so that they are more intuitive and inspiring."" N Synth, a tool that allows you to essentially interweave two instruments and generate a new sound, is likely a little above my pay grade. The same goes for the goods available to anyone who really wants to tap into Magenta's work—the team releases all their tools and models in open source on GitHub. For my purposes, I instead stuck to two of Magenta's available prebuilt tools: Infinite Drum and AI Duet. Infinite Drum is essentially a drum machine composed of everyday sounds that were organized through machine learning. ""The computer wasn’t given any descriptions or tags—only the audio,"" the GitHub description reads. ""Using a technique called t-SNE, the computer placed similar sounds closer together. You can use the map to explore neighborhoods of similar sounds and even make beats using the drum sequencer."" From a user perspective, the tool becomes incredibly easy to use. You scroll around or search for a general category of sound, ultimately settling on four to combine. You can choose to shuffle things up or change the tempo, but otherwise you simply press play from there to get a looped beat. AI Duet, on the other hand, will help you craft a perfect melody. Tap away however you'd like with whatever skills you might have, and Google machine learning takes over to respond with a sensible melody. The AI leverages machine learning by ingesting tons of compositions, and then its neural network responds to your individual inputs (including key and rhythm). As developer Yotam Mann puts it, ""It's even fun to just mash the keyboard. The neural network tries to deliver something coherent from whatever you give it."" Like the Chrome Music Labs offerings, these individual tools function more as singular instruments than full composition platforms at the moment. So coming up with a final product—at the 2:55 mark, you can hear my Homeland-inspired avant-garde jazz closing track—once again requires a separate audio editing and capture program. If the tools above sound too complicated, fear not. As Google pushes forward with Magenta and Sony puts its AI-powered FlowMachine in the hands of real musicians, there is a true AI composition option out there that's available right now to the rest of us: Amper AI. Amper may not be the first publicly available AI composition tool (the British company behind Jukedeck dates back to 2015, for instance), but it's the best combination of ease, customization, and ultimate quality. Amper made some headlines last year when it teamed up with YouTuber Taryn Southern for a single that felt indistinguishable from what you'd hear on Top 40 radio. Now, Southern will even release an entirely AI-driven album, called I Am AI, in May. Different songs will leverage Amper, IBM Watson, Aiva, and Google's Magenta. “Our ultimate goal is to write music as well as John Williams [and] have it sound like it was recorded at Abbey Road Studios and produced by Quincy Jones,” Amper CEO Drew Silverstein told me back in December. ""That’s the musical standard we hold ourselves to, but admittedly we’re not there. There’s a lot of work to be done on the music side of the project before Amper’s product is indistinguishable from human music, at which point it’d be the most valuable tool it could be for creators."" Musicians, not programmers, started Amper the company, so Silverstein and co. view their AI ultimately as a tool and not a replacement (hence the collaboration with Southern rather than promoting a single entirely of the bot's work). They trained their AI on the work of real-world composers much like how AlphaGo learned through observing real-world matches. The result is a tool that, even in its simplified beta iteration, offers a lot of customization while generating almost fool-proof usable results.Further ReadingDeepMind AI needs mere 4 hours of self-training to become a chess overlord In fact, Amper currently has two separate interfaces to choose from: Simple or Pro. The former merely asks you to choose from some pre-set stylistic options and set a duration. The latter allows you to begin with similar stylistic parameters, but you can dig down to change seemingly everything: the number of instruments involved, the type of instruments involved, the tone of instruments involved, the overall key, the overall time signature, etc. Roughly 30 seconds into my video, you can hear what Amper's simple tool does when instructed to craft ""modern folk."" And around the 2:00 mark, you experience my inspirational cinematic composition created via Amper Pro in order to score a montage of my dog walking. I may have envisioned ""Gonna Fly Now""-style brass lines, but realistic brass sounds are something that eludes all these tools at the moment. Still, this particular composition swells and has soaring strings I can live with. Am I going to win a best original song Oscar any time soon thanks to new AI instruments? Probably not. But even in the early stages, these tech-y layman's composition tools work. They're easy enough that I'm not spending all day on 30 seconds of sound, and I don't need to understand a lick of music theory. Yet, these tools can do a passable enough job to wash away any regrets I have that I can't get Vagabon to soundtrack my next Ernie short. Bands like that will likely always get called before the bots for projects of substance, but things for the weekend auteurs will only get better from here. Listing image by Nathan Mattise You must login or create an account to comment. Join the Ars Orbital Transmission mailing list to get weekly updates delivered to your inbox. 
  CNMN Collection
  WIRED Media Group
  © 2020 Condé Nast. All rights reserved. Use of and/or registration on any portion of this site constitutes acceptance of our User Agreement (updated 1/1/20) and Privacy Policy and Cookie Statement (updated 1/1/20) and Ars Technica Addendum (effective 8/21/2018). Ars may earn compensation on sales from links on this site. Read our affiliate link policy.
Your California Privacy Rights | Do Not Sell My Personal Information
  The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.
Ad Choices
",0.13571642033180498,0.4240663182970875
97,https://news.google.com/articles/CBMibGh0dHBzOi8vd3d3Lm11c2ljYnVzaW5lc3N3b3JsZHdpZGUuY29tL3NpcmktY28tZm91bmRlci10b20tZ3J1YmVyLW9uLXRoZS1mdXR1cmUtb2YtYWktaW4tdGhlLW11c2ljLWJ1c2luZXNzL9IBAA?hl=en-US&gl=US&ceid=US%3Aen,"It’s no secret that voice activated smart devices are changing the music industry as we know it. A recent survey from Adobe Digital Insights states 70% of voice-assisted smart speaker owners were using them for music. And although Apple’s Siri-assisted Homepod has lagged behind rivals like Google Home and the Alexa-assisted Amazon Echo, its voice technology was one of the first to reach a mainstream audience. Started back in 2008, Siri was bought by Apple in 2010 and was first introduced on the iPhone 4S in late November 2011, with Alexa and the Amazon Echo following three years later. Siri Co-Founder Tom Gruber (pictured), an artificial intelligence researcher, designer, and entrepreneur, was the company’s Head of Design. At Apple, Gruber worked on AI initiatives as the head of Siri’s Advanced Development Group, before leaving the company in 2018. One of his more recent roles since leaving Apple is that of Co-Founder of LifeScore, an adaptive music platform devised by virtuoso composer and founder CEO Philip Sheppard. The startup recently joined Abbey Road’s Red tech incubator, which says that LifeScore takes ‘the highest quality musical cells, recorded at Abbey Road studios performed by expert session orchestra players, and processes them with an adaptive engine that creates constantly evolving and personalised versions of the music’. Here, Gruber shares his views on the future of AI in the music business and how far the development of the technology has come since Siri was sold to Apple in 2010. In my career I look for ways to augment people with technology that can help them live better lives. Music is something that enriches our lives, and technology has made it possible for everyone to experience the music of the entire world. This is mind blowing, and it’s easy to forget how profoundly this has changed our relationship with music. Technology has helped us navigate this bounty with machine driven personalization, alongside human curation and collaborative filtering. I think the next big impact of technology will be to personalise the audio itself in real-time, to adapt to what people need and want in their particular contexts, with evolving music. You can think of this as a soundtrack for your life, that is supportive of your life. Music for walking, running, driving, thinking, writing, cooking, relaxing, making love, sleeping. Recommendation engines are an important first step, but they are still in the realm of sequencing recorded songs. In my experience, the highest form of “functional music” — music created for given situations to achieve certain affects in the listener — is film score music. So, when my friend Philip Sheppard, a master composer of film score music, told me that he was doing a technology play in this space, I was fascinated. We considered what it would take to have technology compose a film score for your life. He convinced me that he had a solution, and so I wanted to help make it happen at scale. We are exploring the idea that a particular mix of human creativity and machine power will enable us to offer highly personal, adaptive music, with musicians and composers as the foundation. We are betting that the raw materials — the units of music that can be composed into life scores — are essential to the quality of the experience. This means recording a library of musical raw material. Abbey Road is the best recording studio in the world, and it has a community of some the best musicians and composers. And the Abbey Road Red team are experts in this space of technology-enabled music creation and the business models and product iterations that could make it scale. So, they are fabulous partners. Together I think we can make this vision come to life in the near future. AI is revolutionizing processes that can learn from large data streams and make predictions, classifications, or recommendations from that data. In the very near future, I expect the recommendation engines to get much better, as the collective feedback of millions of people carrying around context-sensing computers is fed into the learning algorithms. “AI will also play a central role in technology-enabled music creation, but we are still in early days for that wave. My personal take on it is that the biggest impact will come from companies that figure out how to fold in human creativity into the machine intelligence, rather than trying to create totally autonomous music boxes.” AI will also play a central role in technology-enabled music creation, but we are still in early days for that wave. My personal take on it is that the biggest impact will come from companies that figure out how to fold in human creativity into the machine intelligence, rather than trying to create totally autonomous music boxes. As we have seen from the success of voice interfaces, machines are now capable of listening to and making sense of human generated audio. In my experience, I’ve been amazed at how robust the new deep learning approaches are at finding the signal in the noise, outperforming traditional hand-engineered signal processing techniques in many cases. Machines can hear a needle in a haystack, so to speak. For consumers, this means machines could get surprisingly good at identifying voices, identifying songs and musical characteristics, and offering feedback to someone learning to play an instrument. They can also process other information about the context, such as whether someone is exercising or trying to chill out. Combine the ability to hear with the ability to sense context and you have the ingredients for breakthroughs in personalized music. I do not have years of experience in the music industry, yet! So, I may be missing some new developments, but I haven’t seen much done yet in the space of machine-generated singing. Not vocoder-style robot intonation effects, but something that does what a great vocalist does to impart emotion and meaning in the lyrics. There is technology from text-to-speech generation that could be exploited here. It would be nice for the voice assistant, when someone invents a clever name that isn’t easy to pronounce, to include the pronunciation information within the metadata associated with the content. That way the assistants won’t mangle the clever name, rendering it incomprehensible over a voice interface. And yes, until that happens, to ensure they are discoverable and easily accessible, artists will have to consider how their song or album title or name will sit in the voice activation environment. If it’s really hard to pronounce or includes a series of letters and symbols, then people will struggle to request it and the assistants will struggle to interpret it. The artist’s music won’t get heard, and that’s the saddest result possible. I am happy to see that the entire industry has made it a top priority to develop high quality voice and language interfaces. Since the launch of Siri, there has been a lot of money, talent, and technology applied to this goal. “I am happy to see that the entire industry has made it a top priority to develop high quality voice and language interfaces. Since the launch of Siri, there has been a lot of money, talent, and technology applied to this goal.” The AI tech in particular went through a nice upswing with the application of deep neural nets in the mid 2010’s. And all the layers of the value chain, from the form factor of devices that can listen and talk to you to the back end services that can do what you asked for, are being adapted to the language UI. It’s early days, and I’m very excited about the positive impact AI can make in the music business. I don’t think AI will replace humans in the creation of music that people love: not for sentimental reasons, but because of technical and psychological realities. “AI will be quietly making our experience of music more contextually relevant; it will help us discover new music that we love; and it will likely make it possible for many more people to participate in the composition and performance of high quality music than ever before.” However, AI will be quietly making our experience of music more contextually relevant; it will help us discover new music that we love; and it will likely make it possible for many more people to participate in the composition and performance of high quality music than ever before.Music Business Worldwide The best of MBW, plus the most important music biz stories on the web. Delivered for FREE, direct to your inbox each day.",0.18857909901803027,0.4816979204002105
98,https://news.google.com/articles/CAIiEJRpp2u5DbhBMsTQubA4ukYqFAgEKgwIACoFCAowsGkw8AYw5eYD?hl=en-US&gl=US&ceid=US%3Aen,"Musical AI is fast evolving. In March, Google released an algorithmic Google Doodle that let users create melodic homages to Bach. And late last year, Project Magenta, a Google Brain effort “exploring the role of machine learning as a tool in the creative process,” presented Musical Transformer, a model capable of generating songs with recognizable repetition. In what might be characterized as a small but noteworthy step forward in autonomous music generation research, San Francisco capped-profit firm OpenAI today detailed MuseNet, an AI system that can create four-minute compositions with 10 different instruments across styles “from country to Mozart to the Beatles.” OpenAI plans to livestream pieces composed by MuseNet on Twitch later today starting at 12 p.m. Pacific, and has released a MuseNet-powered music tool that’ll be available through May 12. The MuseNet composer has three modes: simple mode, which plays an uncurated sample from a composer or style (and an optional start of a famous piece), and advanced mode, which lets you can interact with the model directly to create a novel piece. Here’s MuseNet prompted with the first 5 notes of Chopin:   As OpenAI technical staff member Christine Payne explains in a blog post, MuseNet, as with all deep neural networks, contains neurons (mathematical functions loosely modeled after biological neurons) arranged in interconnected layers that transmit “signals” from input data and slowly adjust the synaptic strength — weights — of each connection. But uniquely, it has attention: Every output element is connected to every input element, and the weightings between them are calculated dynamically. MuseNet isn’t explicitly programmed with an understanding of music, then, but instead discovers patterns of harmony, rhythm, and style by learning to predict tokens — notes encoded in a way that combines the pitch, volume, and instrument information — in hundreds of thousands of MIDI files. (It’s informed by OpenAI’s recent work on Sparse Transformer, which in turn was based on Google’s Transformer neural network architecture.) Above: MuseNet’s understanding of composers and how they relate stylistically. MuseNet was trained on MIDI samples from a range of different sources, including ClassicalArchives, BitMidi, and the open source Maestro corpus. Payne and colleagues transformed them in various ways to improve the model’s generalizability, first by transposing them (by raising and lowering the pitches) and then by turning up or turning down the overall volumes of the various samples and slightly slowing or speeding up the pieces. To lend more “structural context,” they added mathematical representations (learned embeddings) that helped to track the passage of time in MIDI files. And they implemented an “inner critic” component that predicted whether a given sample was truly from the data set or if it was one of the model’s own past generations. MuseNet’s additional token types — one for composer and another for instrumentation — afford greater control over the kinds of samples it can generate, Payne explains. During training, they were prepended to each music sample so that MuseNet learned to use them information in making note predictions. Then, at generation time, the model was conditioned to create samples in a chosen style by starting with a prompt like a Rachmaninoff piano start or the band Journey’s piano, bass, guitar, and drums.  “Since MuseNet knows many different styles, we can blend generations in novel ways,” she added. “[For example, the model was] given the first six notes of a Chopin Nocturne, but is asked to generate a piece in a pop style with piano, drums, bass, and guitar. [It] manages to blend the two styles convincingly.” Payne notes that MuseNet isn’t perfect — because it generates each note by calculating the probabilities across all possible notes and instruments, it occasionally makes unharmonious choices. And predictably, it has a difficult time with incongruous pairings of styles and instruments, such as Chopin with bass and drums. But she says that it’s an excellent test for AI architectures with attention, because it’s easy to hear whether the model is capturing long-term structure on the training data set’s tokens. “It’s much more obvious if a music model messes up structure by changing the rhythm, in a way that it’s less clear if a text model goes on a brief tangent,” she wrote. 
					The AI event for business leaders				 
					 San Francisco				 
					 July 15 - 16				 The industry’s most intimate gaming event for senior industry executives, developers and investors. The AI event of the year for enterprise executives, brought to you by today’s leading AI publisher.",0.11806850556850555,0.44934741184741184
99,https://news.google.com/articles/CBMiPmh0dHBzOi8vd3d3Lm5hbmFseXplLmNvbS8yMDE4LzA1LzExLXN0YXJ0dXBzLWFpLWNvbXBvc2UtbXVzaWMv0gFCaHR0cHM6Ly93d3cubmFuYWx5emUuY29tLzIwMTgvMDUvMTEtc3RhcnR1cHMtYWktY29tcG9zZS1tdXNpYy9hbXAv?hl=en-US&gl=US&ceid=US%3Aen,"You don't have permission to access /2018/05/11-startups-ai-compose-music/
on this server.",0.0,0.0
100,https://news.google.com/articles/CAIiEAwUPbYwYGbQLLw22nNqv00qFwgEKg8IACoHCAoww5LyAzD2zh0wyrVV?hl=en-US&gl=US&ceid=US%3Aen,"We use cookies to improve our service for you. You can find more information in our data protection declaration. He composed nine symphonies. A 10th was planned, but only sketched out. A Beethoven scholar reveals how a computer can complete it — but humans are still needed to interpret the composer's idea of a ""Bacchanalian fest.""   Musicologists, musicians, a film composer, a telecommunications company and IT specialists are currently collaborating on completing a work that Beethoven left unfinished. The resulting composition is to have its world premiere next spring in Bonn. We spoke with Christine Siegert, head of the Beethoven Archive in Bonn. DW: Beethoven's sketch books show that in the process of composing, he was constantly drafting themes and musical ideas and sometimes rejecting many before finally deciding on one. A computer can perhaps try out tens of thousands of ideas in a couple of seconds and then make its choice. So artificial intelligence seems to have a leg up on Beethoven. But is there something like artificial genius?   Christine Siegert: Artificial genius, certainly not. Completing a composition with the help of artificial intelligence works this way: You feed the information you have into the computer, and it calculates on the basis of probabilities what might have been. The quality of genius cannot be fully depicted. All the more so if you're dealing with Beethoven's late period. Each of those works has a stand-alone quality. Of course you can't feed that individual character into a computer. I think the project's goal should be to integrate Beethoven's existing musical fragments into a coherent musical flow. That's difficult enough, and if this project can achieve that, it will be an incredible accomplishment. Beethoven's old school tools: the paper and quill Beethoven has those wonderful melodic and motivic ideas. But what makes this composer unique is what he does with them. Isn't it quite a leap to say that a computer could do that adequately? Yes, it's a leap. But you have to ask what you expect of the final product. Artificial intelligence has also been used in the field of painting. At a workshop here at the Beethoven House, we saw a pseudo Rembrandt: a portrait made in the style of Rembrandt using artificial intelligence. I'm neither an art historian nor a Rembrandt specialist, but I, at least, couldn't tell the difference between an original and the computer-created derivate. Which doesn't mean that art historians and Rembrandt specialists can't either.  I think the goal should be to produce something in the style of the time — and not something about which the world's few Beethoven specialists would say: ""Yes, Beethoven actually could have written this."" What information does the composer need to compose this symphony in the style of Beethoven? The few extant fragments of Beethoven's Tenth are the point of departure. Then, to give it a musical foundation, works by Beethoven and his contemporaries are fed in — particularly the ones Beethoven esteemed. The house where the composer was born also houses the Beethoven Archive, where many of his handwritten scores are kept  That's important: As individual as Beethoven was, he worked in a certain musical context. That context is lacking with composers today, who can do whatever they want and frequently start from scratch. Beethoven, on the other hand, could stand on the shoulders of his predecessors and contemporaries and work from there. Does the computer do that too? The computer does something similar. We debated a lot about what music to feed into the program. Hummel, Mozart, Haydn? Exactly. Cherubini too, a composer and Beethoven contemporary that he highly esteemed. Some of his French contemporaries too. And difficult, but desirable: One should take a particular Beethoven work, one for which extensive drafts still exist, like the Eroica Symphony. If you feed the computer both the sketches and the final product, it can figure out how Beethoven works with sketches and where he goes from there. What do we know about Beethoven's Tenth? He began working on it together with the Ninth. Beethoven liked to work on two symphonies at a time: the Fifth and the Sixth for example, or the Seventh and the Eighth. He finished the Ninth and wanted to continue with the Tenth, but then he died. In his text notes you can read that he wanted to integrate a chorale into the symphony. And he envisioned the final movement as a Bacchanalian fest. A computer won't know what to do with that information of course. But it was apparently supposed to be an extremely joyful and exuberant piece. As head of the Beethoven Archive, Christine Siegert sits at the source How much Beethoven will be in the final product and how much will come from the computer-composer? Maybe expressed in numbers: one to five? One to six? One to much, much more. The computer has its work cut out for it. Beethoven's musical sketches are between two and thirty measures long. Most of them somewhere in between. There's a human dimension too. Firstly, the American pianist and musicologist Robert Levin is in the project team and has done similar things before — using his human faculties of course, such as with Mozart's Requiem. He's extremely well informed about the style of Beethoven's time. The team also includes a music theoretician who works with computer-supported music analysis. The human colleagues set the parameters, and the computer fills in the gaps. The idea is to limit it to two movements, the ones for which the greater part of the sketches exist. What is the Beethoven Archive's role? We hosted the most recent workshop. We bring our research expertise into play when it comes to musical sketches. But the project is funded by Deutsche Telekom and is headed by Matthias Röder, director of the Karajan Institute in Salzburg. Will the computer do the orchestration too? For that, we have a film music composer in the project team. You're a Beethoven scholar, and here at the Beethoven Archive, you sit at the center of the Beethoven universe. I would surmise that you're a Beethoven enthusiast too. Honestly now, are you watching the project critically, or are you enthused about it? I'm watching the project with close interest and will go to hear the premiere. I'm also convinced that we'll see incredible developments here in the years to come. So even if not everyone is delighted with the way this project sounds in the Beethoven anniversary year , in a few years we'll be astonished what can be done with artificial intelligence — in every area, in music too. Composer Ludwig van Beethoven didn't pour all his passion into his music, as proven by the many loves in his life. The most important woman, however, may forever remain a mystery. (06.09.2017)
   Beethoven died in 1827, so what does he have to do with the current climate talks? The answer lies in the ""Beethoven Pastoral Project,"" inspired by the composer's ""Pastoral Symphony."" (15.11.2017)
   A composer like Ludwig van Beethoven towers over others, but even that singular genius didn't come out of the blue. A look at the musical environment that brought forth the iconic musician — and cherished his memory. (06.09.2018)
   Music to please cows and revolutionaries alike: the principal conductor of the Beethoven Orchestra Bonn, Dirk Kaftan, tells DW all about Ludwig van Beethoven's Sixth Symphony. (17.11.2017)
   Beethoven, the man who symbolizes serious symphonies, and simple folk music — can the two fit together? Yes, according to an imaginative chamber music festival in the house where the composer was born. (25.01.2018)
   Concerts, exhibitions, city tours and performances: For an entire year, Germany will celebrate one of its most famous citizens, Ludwig van Beethoven, on the 250th anniversary of his birth. (29.11.2019)
   Ludwig van Beethoven spent the first 22 years of his life in Bonn and vicinity. Now, for the 250th anniversary of his birth, a walkabout lets visitors experience the places where the composer himself spent time. (11.10.2019)
   It's Beethovenfest time again, but why is the composer so celebrated? Deustche Welle hit the streets of Bonn to investigate. (03.09.2018)
   Shortly before his death in 1987, pop-art icon Andy Warhol worked on a portrait of Beethoven. The composer's appearance and character fascinated many artists - inspiring them to create their own renditions. (06.09.2013)
   Ludwig van Beethoven moved from Bonn to Vienna at the age of 22. Following in his footsteps, a barge is tracing the route he took, inviting people in 14 cities on board for unusual music events.  The house where the composer was born houses an archive, a museum and a concert hall. Now it has a new, media-savvy president. Star violinist Daniel Hope has original plans for the institution.  Beethoven is famous worldwide for his music— but where would he be without his many students, admirers and arrangers? They, too, are being honored in 2020, the year celebrating Beethoven's 250th birthday.  Dozens of actors and former employees have accused producer Harvey Weinstein of sexual assault and misconduct, exploding the #MeToo movement onto the global stage. Weinstein's sentence is expected on Wednesday.
   Writing as an act of liberation. Colm Tóibín, one of the the most distinguished English writers, talks about women, music, and his home in Ireland. 
   Lady Gaga has a new species of treehopper named after her. From David Bowie to Beyoncé, many other musicians have their own namesake bugs.
   Günther Uecker is one of the most important German artists worldwide. As he turns 90, we revisit a retrospective of his works.
   The surprise news that Meghan Markle and Prince Harry plan to retire from royalty has triggered a series of humorous reactions on Twitter — as well as harsher comments reflecting polarization on the topic.
   
© 2020 Deutsche Welle |
Privacy Policy |
Legal notice |
Contact
| Mobile version
",0.1271501560455049,0.5078417899929528
101,https://news.google.com/articles/CBMioAFodHRwczovL3d3dy5nbG9iZW5ld3N3aXJlLmNvbS9uZXdzLXJlbGVhc2UvMjAxOS8wMS8yMy8xNzA0MjE0LzAvZW4vQW1wZXItTXVzaWMtTGF1bmNoZXMtRmlyc3QtQUktTXVzaWMtQ29tcG9zaXRpb24tUGxhdGZvcm0tZm9yLUVudGVycHJpc2UtQ29udGVudC1DcmVhdG9ycy5odG1s0gEA?hl=en-US&gl=US&ceid=US%3Aen,"NEW YORK, Jan.  23, 2019  (GLOBE NEWSWIRE) -- Amper Music, the leader in artificial intelligence music creation, today announced the launch of Amper Score™, the world’s first end-to-end AI music composition platform for enterprise content creators, as well as a developer API. Businesses of all kinds are adopting and scaling content strategies to reach new customers, and Score harnesses the power of AI to help teams make custom music that fits the style, length, and structure of their unique projects. Using the Amper Score™ platform, content teams can create and edit music to accompany videos, podcasts, and many other types of content. Score’s workflow allows users to upload videos, spot key moments in project timelines, and render original compositions in real time in dozens of different musical styles. Music created using Amper Score™ is royalty-free and comes with a global, perpetual license for subscribers when synced to content. With Score’s fine-grained control, video editors, who typically work with stock music libraries, have reported greater than 90% time savings in sourcing and editing music for their projects. ""With Amper Score™, it is easy for me to find the music I need for the videos that we create,” said Anna Green, a video producer at Minute Media. “It takes me less than five minutes to create music for a short-form video, and the track is the exact mood I’m looking for. At past jobs, it could take me as long as two hours to search for the right track and edit it for the video. The tracks that I make with Amper Score have variation, and I like being able to spot the intro to add a gradual build. I can't imagine a better process than this.” Businesses can also integrate Amper’s API into their own creative tools, distribution platforms, and other applications where music is created or consumed. Amper is excited to announce its API launch partner, QQ Music of Tencent Music Entertainment Group. “As a leading online music entertainment platform in China, QQ Music is constantly looking for ways to utilize the most competitive technologies,” said Dennis Hau, Vice President of Tencent Music Entertainment Group. “In collaboration with Amper and leveraging its advanced technology, we look forward to continue providing more unique and innovative experiences to our users. “In our collaboration, we are impressed by Amper’s leading technology, speed, quality, and user control design.” “The decision to partner with QQ Music was a no-brainer,” said Drew Silverstein, CEO of Amper Music, Inc. “They are at the very forefront of the music, media, and entertainment markets, and their staggering reach will bring our AI Composer technology to millions of new users. QQ Music is one step ahead—they clearly understand that diversity and efficiency are the future of content, and we couldn’t be more thrilled to partner with such a forward-thinking company within the digital music space.” Amper’s patented AI is built from the ground up and utilizes a massive dataset created by an internal team of composer-developers. Rather than employing a “black box” approach that limits contextual awareness and the ability for Score to collaborate with its users, Amper’s data team describes every facet of music, including genre, mood, and instrumentation. Amper’s technology draws on these foundations to generate fully editable, bespoke music that users can tailor virtually instantaneously. About Amper: Amper was founded in 2014 with the mission to enable anyone to express themselves creatively through music regardless of their background or expertise. Amper builds tools powered by a Creative AI to help teams create and customize original music. Using a proprietary library of over a million individual samples and thousands of instruments, Amper’s technology is built with an emphasis on offering high quality music through the fusion of music theory and AI innovation. Interested teams should contact sales@ampermusic.com or visit www.ampermusic.com to arrange a trial. Amper Music   Subscribe via RSS  Subscribe via ATOM  Javascript   New York, New York, UNITED STATES   https://www.ampermusic.com/ 


amper_lockup.png

 Formats available:",0.20610665092483277,0.5360322707595434
102,https://news.google.com/articles/CAIiEBYOIKnAUPo-Ek5sOIDzhf8qGQgEKhAIACoHCAowief2CjCJ2dUCMJuWxwU?hl=en-US&gl=US&ceid=US%3Aen,"Published: 5:46pm, 24 Jul, 2019 Updated: 5:46pm, 24 Jul, 2019 TOP PICKS",0.5,0.5
103,https://news.google.com/articles/CAIiEOmH-OwiYCFUihBpbCckvekqFwgEKg8IACoHCAow-4fWBzD4z0gwwtt6?hl=en-US&gl=US&ceid=US%3Aen,"

					Sign in
				
 International Edition",0.0,0.0
104,https://news.google.com/articles/CAIiEAlUFEd0w6mBVHIVzxH5n_wqFwgEKg8IACoHCAowraj2AjC5-iIwqYhb?hl=en-US&gl=US&ceid=US%3Aen,"  Shows on the Discovery Channel, Animal Planet, HGTV and Food Network may sound very different in the coming months. That’s because Discovery Networks, which owns those and other cable channels, is instituting a new pay policy that virtually assures no composer currently working on their programs will do so after Dec. 31.  Discovery has informed many of its top composers that, beginning in 2020, they must give up all performance royalties paid for U.S. airings, and that they must sign away their ability to collect royalties on all past shows on its networks. Music makers surmise that the policy will result in an 80% to 90% drop in their income from these shows. It’s the last straw for many composers who say they will refuse to continue to score such series as “Gold Rush,” “Deadliest Catch” (pictured) and “Alaskan Bush People,” calling the new contract provisions “unprofessional,” “bullying,” “a corporate money grab” and “evil.”  







 Variety spoke with more than a half-dozen composers who have been informed of the proposal, which is designed to circumvent the 100-year-old system whereby composers are compensated for use of their music in broadcast media. Those royalties are collected and distributed by performance-rights societies ASCAP, BMI and SESAC. Composer David Vanacore (“Fixer Upper”) says that initial fees are already so low that he and his peers rely on the so-called “back end” payments just to keep operating. “There is no way I can support what it takes to do a show based on what they’re offering,” he says. “I don’t think they understand the amount of time and energy that goes into the creative process.” Discovery is requesting “direct source licenses” which will enable them to eliminate royalty payments. But composers say those royalties are vital to stay in business. What’s more, they fear that if this tactic is followed by other media outlets, making a living as a media composer in Los Angeles will eventually become impossible. “All of the added musical value, bespoke musical branding and music scored to picture will not be financially supportable under the new proposed model,” said Didier Lean Rachou (“Gold Rush,” “Deadliest Catch”). “The Hollywood ecosystem of world-class mixers, assistants and musicians that I regularly use are no longer affordable for a small-business owner like myself.” Composers have been offended by what they call “veiled threats” by Discovery executives that if they don’t take the new deal, their music will be stripped out of existing shows and replaced with generic library music that the network already owns. Says one: “What they offered was paltry and pathetic. There was no financial component to compensate me for any domestic royalties they are asking me to sign away. If I accepted their withered carrots, they would then ask me to sign away all my past content. That was my retirement. There is absolutely no incentive for me to move forward with them.” That “retirement,” according to Society of Composers and Lyricists President Ashley Irwin, is important to every film and TV composer because (unlike almost every other creative job in Hollywood) they have no union protection and the benefits that usually affords. “The closest thing we have to a pension plan is that royalty stream that comes through the performing-rights organizations. If that goes away, we’ll have nothing.” Over the past 20 years, adds Mark T. Williams (“Betrayed”), composer fees (the “upfront” money) have declined by as much as 50 percent. “We rely on industry-standard practices of retaining our composers’ share of performing-rights and other royalty income,” he says. “Without that, we don’t have a sustainable business. We don’t just write a piece of music and spit it out. We’re composing, orchestrating, mixing, mastering, providing an entire service as well as support after the fact.” Williams estimates that an inability to collect domestic royalties would lead to an 82 percent decline in his income from Discovery series. “This is deeply destructive,” offers Russell Emanuel, co-founder and chief creative at music production company Bleeding Fingers (which supplies music for “Alaskan Bush People”). “You cannot cut corners and be producing music at the top level. They’re not doing this to their acting or their voiceover talent.” Discovery declined to make its music executives available for this story, but issued this statement: “Our 8,000 hours of original programming a year drives enormous economic value to the global music community. We compensate countless composers and musicians for their valued contributions, and will continue to do so.” Some estimates suggest that avoiding ASCAP, BMI and SESAC royalty payments might save them $25 million or so – less than 1 per cent of Discovery’s third-quarter 2019 revenue of nearly $2.68 billion.  







 Several composers and music attorneys tell Variety that this initiative “sets a dangerous precedent.” They worry that inexperienced composers who agree to take this deal will erode long-held industry practices. “What’s at stake here is the destruction of the creative process,” Vanacore says. “This is a downward spiral for all creative people.” And if Discovery follows through with its threat to remove all of the familiar themes from its series based on composers’ refusal to accept the new financial deal, “the shows will be damaged,” he adds. Adds another veteran Discovery composer, who asked for anonymity: “the threat of having our series re-mixed with other music Discovery owns outright is unheard-of and would send a ripple effect throughout the industry.” Several of these composers, and other high-profile composers who share their concerns, have banded together to launch a website, yourmusicyourfuture.com, designed to inform the media composing community of their rights and the possible ramifications of giving up the traditional “writer’s share” of royalties. More than 4,000 music-makers have signed up to support the initiative. “What we have to do,” says Emanuel, “is make sure that new composers understand that this does damage beyond their scope.” Williams adds that the proposed deal, if accepted, “undermines composers as a whole. We don’t understand [Discovery’s] position and we don’t support it.” All of the composers interviewed by Variety said they would be happy to continue working for the networks, and their producers, on these and other shows, but not if it means giving up the majority of their income to do so. In a similar move, Netflix has attempted to convince composers to take the buyout but, sources say, generally back off those demands when composers (especially A-list names) balk. RELATED CONTENT:  Finding a little inner peace might seem all but impossible amidst the current state of quarantines, cancellations and an overwhelming sense of anxiety spurred by coronavirus pandemic. But even on the darkest of days a little bit of art can shine like a beacon of connection, lifting spirits and bringing joy. If you’ve heard the [...] Universal Music Group’s home offices in Santa Monica, Calif., were evacuated Friday after an employee tested positive for the coronavirus. All staffers were directed to leave the buildings “in an orderly manner,” although the discovery was made toward the end of the day and a source tells Variety that most of the staff had either [...] “Revenge Of The Dreamers III,” the latest installment of J.Cole’s Dreamville compilation series, has been certified platinum by the RIAA (Recording Industry  Association of America) marking equivalent sales of over one million units. In July 2019, the star-studded set was certified gold and has since then received visual treatments for the tracks “Under The Sun” [...] The Eastern District of New York on Friday filed a superseding indictment against R. Kelly, who already faces multiple charges in three states, that includes additional charges stemming from two accusers who have been newly added to the suit, according to Rolling Stone. The singer’s New York trial begins on July 7. In the indictment, [...] Despite being known for his collaborative role as Bruce Springsteen’s teenage Jersey Shore pal-turned-guitarist, producer and consigliere, Little Steven Van Zandt has long had an explicit solo vision. And in the Reagan-era climate of the 1980s, Van Zandt’s voice was a scathingly political one, something rare for the era of good-time jingoism, patriotism, MTV and [...] The Dreamville Festival, a one-day multi-artist concert curated by rapper J. Cole, has been postponed. Originally scheduled for April 4 at Dorothea Dix Park in Raleigh, North Carolina, the fest will now take place on Saturday, August 29. The venue remains the same. A lineup has not been revealed — it was due to be [...] The Tupac Estate and Soundgarden have pulled out of a lawsuit against Universal Music Group that was filed by five artists last year over damage the artists’ musical recordings suffered in a 2008 fire that destroyed many assets in the company’s vaults. While Universal’s archives are suffered extensive damage in the fire, the company has [...] 
		© Copyright 2020 Variety Media, LLC, a subsidiary of Penske Business Media, LLC. Variety and the Flying V logos are trademarks of Variety Media, LLC.		Powered by WordPress.com VIP
 Access exclusive content © 2020 Penske Media Corporation",,
