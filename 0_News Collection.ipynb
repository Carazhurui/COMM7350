{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import bs4 as bs \n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yahoo News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = []\n",
    "page_content = []\n",
    "\n",
    "source = requests.get('https://news.search.yahoo.com/search?p=ai+music+composer&fr=uh3_news_vert_gs&fr2=p%3Anews%2Cm%3Asa&guce_referrer=aHR0cHM6Ly9uZXdzLnlhaG9vLmNvbS8&guce_referrer_sig=AQAAAJ8sWO8YODsfi--VCmmI1a3VYjfuKUHL_sGUZGFEBaRDVCGo0azFsVYNtJZSvgj0KLj5E-SypMw7Rv3mB8hNv3SZAiO4XI-uwJPuFk98Ezgil9aQVjx3zs0zkdtgmV5krQNlDFuKfoz8uP8YBLsnDv6GnwPj5BGZPfezu3Qkfcfd&_guc_consent_skip=1584070602')\n",
    "source.encoding = 'utf-8'\n",
    "soup = bs.BeautifulSoup(source.content , features = 'html.parser')\n",
    "news_container = soup.find('div',{'id':'web'})\n",
    "\n",
    "try:\n",
    "    for news in news_container.find_all('div',{'class':'dd NewsArticle'}):# find all (list)\n",
    "        # get links \n",
    "        url_content = news.find('a',{\"class\":\"thmb\"}).get('href')\n",
    "        link.append(url_content)\n",
    "        \n",
    "        # get text of each page \n",
    "        new_source = requests.get(url_content)\n",
    "        new_source.encoding = 'utf-8'\n",
    "        new_soup = bs.BeautifulSoup(new_source.content , features = 'html.parser')\n",
    "        \n",
    "        ps = new_soup.find_all('p')\n",
    "       \n",
    "        page_texts = []\n",
    "        for p in ps:\n",
    "            page_texts.append(p.text)\n",
    "        page_content.append(' '.join(page_texts))\n",
    "              \n",
    "except:\n",
    "    print('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob\n",
    "sentiment = []\n",
    "for content in page_content:\n",
    "    blob = textblob.TextBlob(content)\n",
    "    blob.sentences\n",
    "    sentiment.append(blob.sentiment)\n",
    "    print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YH_URL_dict = {'URL':link,\n",
    "              'Content':page_content,\n",
    "            'Sentiment':sentiment}\n",
    "\n",
    "\n",
    "YH_URL_frame = pd.DataFrame(YH_URL_dict)\n",
    "YH_URL_frame.to_csv('Yahoo sentiment.csv',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YH_URL_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page = requests.get('https://news.google.com/search?q=AI%20composer%20music&hl=en-US&gl=US&ceid=US%3Aen')\n",
    "main_page.encoding = 'utf-8'\n",
    "\n",
    "main_soup = bs.BeautifulSoup ( main_page.content , features='html.parser')\n",
    "print(main_soup.prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty lists to receive values\n",
    "title = []\n",
    "sources = []\n",
    "link = []\n",
    "\n",
    "source = requests.get('https://news.google.com/search?q=AI%20composer%20music&hl=en-US&gl=US&ceid=US%3Aen')\n",
    "source.encoding = 'utf-8'\n",
    "soup = bs.BeautifulSoup(source.content , features = 'html.parser')\n",
    "news_container = soup.find('div',{'class':'lBwEZb BL5WZb xP6mwf'})\n",
    "\n",
    "try:\n",
    "    for news in news_container.find_all('h3',{\"class\":\"ipQwMb ekueJc RD0gLb\"}):# find all (list)\n",
    "        title_content = news.text\n",
    "        title.append(title_content)\n",
    "except:\n",
    "    title.append('None')\n",
    "\n",
    "try:\n",
    "    for news in news_container.find_all('a',{\"class\":\"wEwyrc AVN2gc uQIVzc Sksgp\"}):\n",
    "        sources_content = news.text\n",
    "        sources.append(sources_content)\n",
    "except:\n",
    "    sources.append('None')\n",
    "\n",
    "try:\n",
    "    for news in news_container.find_all('div',{\"class\":\"xrnccd\"}):\n",
    "        link_content = news.find('a').get('href')\n",
    "        link.append(link_content.replace('.','https://news.google.com'))\n",
    "except:\n",
    "    link.append('None')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_dict = {'Title':title,\n",
    "            'Source':sources,\n",
    "            'URL':link}\n",
    "\n",
    "URL_frame = pd.DataFrame(URL_dict)\n",
    "URL_frame.to_csv('Google URLs.csv',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URL_frame['URL']\n",
    "\n",
    "# get text of each page \n",
    "        \n",
    "page_content2 = []\n",
    "for URL in URL_frame['URL']:\n",
    "    print(URL)\n",
    "    \n",
    "    new_source = requests.get(URL)\n",
    "    new_source.encoding = 'utf-8'\n",
    "    new_soup = bs.BeautifulSoup(new_source.content , features = 'html.parser')\n",
    "        \n",
    "    ps = new_soup.find_all('p')\n",
    "       \n",
    "    page_texts = []\n",
    "    for p in ps:\n",
    "        page_texts.append(p.text)\n",
    "    page_content2.append(' '.join(page_texts))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import textblob\n",
    "sentiment2 = []\n",
    "for content in page_content2:\n",
    "    blob = textblob.TextBlob(content)\n",
    "    blob.sentences\n",
    "    sentiment2.append(blob.sentiment)\n",
    "    print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_URL_dict = {'URL':URL_frame['URL'],\n",
    "              'Content':page_content2,\n",
    "            'Sentiment':sentiment2}\n",
    "\n",
    "\n",
    "GO_URL_frame = pd.DataFrame(GO_URL_dict)\n",
    "GO_URL_frame.to_csv('Google sentiment.csv',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_URL_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_sentiment_frame = pd.concat([YH_URL_frame,GO_URL_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_sentiment_frame = URL_sentiment_frame.reset_index(drop=True)\n",
    "URL_sentiment_frame.to_csv('Total Setiment.csv',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
